<!-- prettier-ignore-start -->
**Document Version:** 1.0
**Last Updated:** 2026-02-28
**Status:** ACTIVE
<!-- prettier-ignore-end -->

---

phase: 02-backfill-data-migration plan: 03 type: execute wave: 1 depends_on: []
files_modified:

- scripts/reviews/dedup-debt.ts
- scripts/reviews/**tests**/dedup-debt.test.ts
- docs/technical-debt/MASTER_DEBT.jsonl
- docs/technical-debt/raw/deduped.jsonl autonomous: true

must_haves: truths: - "All existing MASTER_DEBT.jsonl review-sourced entries are
deduplicated (no duplicate debt items for the same finding)" - "content_hash is
the primary dedup key -- entries with identical hashes are collapsed to the one
with lowest DEBT-NNNN ID" - "Non-review-sourced entries are preserved
untouched" - "raw/deduped.jsonl is updated in sync with MASTER_DEBT.jsonl (per
MEMORY.md critical bug rule)" - "Running the script is idempotent -- second run
produces no changes" artifacts: - path: "scripts/reviews/dedup-debt.ts"
provides: "MASTER_DEBT.jsonl deduplication script for review-sourced entries"
min_lines: 60 - path: "scripts/reviews/**tests**/dedup-debt.test.ts" provides:
"Unit tests for dedup logic" min_lines: 50 key_links: - from:
"scripts/reviews/dedup-debt.ts" to: "docs/technical-debt/MASTER_DEBT.jsonl" via:
"reads and rewrites MASTER_DEBT.jsonl" pattern: "MASTER_DEBT\\.jsonl" - from:
"scripts/reviews/dedup-debt.ts" to: "docs/technical-debt/raw/deduped.jsonl" via:
"mirrors changes to deduped.jsonl (MEMORY.md rule)" pattern: "deduped\\.jsonl"

---

<objective>
Deduplicate review-sourced entries in MASTER_DEBT.jsonl, eliminating the 16 known duplicate content_hashes.

Purpose: Clean the debt tracking data so downstream pipeline work (Phase 3+)
starts from accurate counts. The 654 review-sourced entries contain duplicates
from multiple intake pipelines running at different times. Output:
`dedup-debt.ts` script, tests, and cleaned MASTER_DEBT.jsonl + deduped.jsonl.
</objective>

<execution_context> @~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md </execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-backfill-data-migration/02-RESEARCH.md
@docs/technical-debt/MASTER_DEBT.jsonl
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create dedup-debt.ts script and tests</name>
  <files>
    scripts/reviews/dedup-debt.ts
    scripts/reviews/__tests__/dedup-debt.test.ts
  </files>
  <action>
Create `scripts/reviews/dedup-debt.ts`:

1. **Read MASTER_DEBT.jsonl** line by line, parse each as JSON. Handle malformed
   lines gracefully (log warning, skip).

2. **Separate entries by source:** Split into review-sourced (where `source` is
   "review", "pr-review", or similar review-related values) and non-review
   entries. Non-review entries pass through untouched.

3. **Dedup review-sourced entries by content_hash:**
   - Build `Map<string, DebtItem[]>` keyed by content_hash
   - For entries sharing the same content_hash: keep the one with the LOWEST
     DEBT-NNNN numeric ID (it's the original)
   - For entries WITHOUT content_hash: keep all (can't dedup without hash)
   - Log each duplicate found:
     `"Dedup: keeping DEBT-0042, removing DEBT-0187 (same content_hash: abc123)"`

4. **Also check for title-based near-duplicates** (secondary pass):
   - After content_hash dedup, scan remaining review-sourced entries for
     identical `title` + `source` combinations
   - If found, log as potential duplicates but do NOT auto-remove (too risky).
     Just report them for manual review.

5. **Reassemble the full JSONL:**
   - Non-review entries (unchanged) + deduplicated review entries
   - Sort by DEBT-NNNN ID numerically to maintain consistent ordering
   - Write atomically to MASTER_DEBT.jsonl

6. **CRITICAL (per MEMORY.md):** After writing MASTER_DEBT.jsonl, ALSO copy the
   updated file to `docs/technical-debt/raw/deduped.jsonl`. This is mandatory
   because `generate-views.js` reads from deduped.jsonl and overwrites
   MASTER_DEBT.jsonl. If deduped.jsonl is stale, the dedup work gets lost on
   next generate-views run.

7. **Print summary:** Total entries before, duplicates removed, total after,
   potential title-based duplicates flagged.

**File path resolution:** Use `findProjectRoot()` walk-up pattern (per 01-02
decision) to resolve paths to `docs/technical-debt/`.

**Important anti-patterns:**

- Wrap ALL file reads in try/catch (CLAUDE.md rule)
- Use `scripts/lib/sanitize-error.js` pattern for error logging -- never log raw
  error.message
- Do NOT modify the DEBT-NNNN IDs of kept entries -- only remove duplicates
- Make the script idempotent: running on already-deduped data produces identical
  output

Create `scripts/reviews/__tests__/dedup-debt.test.ts`:

1. **Basic dedup test:** 3 entries, 2 sharing content_hash. Verify output has 2
   entries, kept the lower ID.
2. **Non-review preservation test:** Mix of review and non-review entries.
   Verify non-review entries untouched.
3. **No content_hash test:** Entry without content_hash is always kept.
4. **Idempotency test:** Run dedup on already-clean data. Output identical to
   input.
5. **Ordering test:** Output sorted by DEBT-NNNN ID regardless of input order.
6. **Title-based near-duplicate detection test:** Two entries with same
   title+source but different content_hash. Verify they are flagged (logged) but
   not removed.

**Testing approach:** Extract the core dedup logic into a pure function
`dedupReviewSourced(items: DebtItem[]): { kept: DebtItem[], removed: DebtItem[], flagged: DebtItem[] }`.
Tests call this directly with mock data. </action> <verify> Run:
`cd scripts/reviews && npx tsc --noEmit` -- compiles cleanly. Run:
`cd scripts/reviews && npx tsc && node --test dist-tests/__tests__/dedup-debt.test.js`
-- all tests pass. Run:
`cd scripts/reviews && npx tsc && node dist/dedup-debt.js` -- produces summary
showing duplicates removed. Verify:
`wc -l docs/technical-debt/MASTER_DEBT.jsonl` shows fewer lines than before (16+
duplicates removed). Verify:
`diff docs/technical-debt/MASTER_DEBT.jsonl docs/technical-debt/raw/deduped.jsonl`
shows files are identical (sync rule). </verify> <done> MASTER_DEBT.jsonl is
deduplicated with 16+ duplicate content_hash entries removed. raw/deduped.jsonl
is synced. Non-review entries untouched. Script is idempotent. All tests pass.
Requirement BKFL-03 satisfied. </done> </task>

</tasks>

<verification>
1. `cd scripts/reviews && npx tsc --noEmit` passes
2. `node --test dist-tests/__tests__/dedup-debt.test.js` passes all tests
3. `node dist/dedup-debt.js` runs and reports duplicates removed
4. MASTER_DEBT.jsonl and raw/deduped.jsonl are identical after run
5. Running dedup-debt.js a second time reports 0 duplicates removed (idempotent)
6. No non-review entries were modified
</verification>

<success_criteria> All existing MASTER_DEBT.jsonl review-sourced entries are
deduplicated using content_hash as primary key. No duplicate debt items remain
for the same finding. raw/deduped.jsonl is synced per MEMORY.md rule. Script is
idempotent and tested. Requirement BKFL-03 satisfied. </success_criteria>

<output>
After completion, create `.planning/phases/02-backfill-data-migration/02-03-SUMMARY.md`
</output>
