<!-- prettier-ignore-start -->
**Document Version:** 1.0
**Last Updated:** 2026-02-28
**Status:** ACTIVE
<!-- prettier-ignore-end -->

---

phase: 02-backfill-data-migration plan: 02 type: execute wave: 2 depends_on:
["02-01"] files_modified:

- scripts/reviews/backfill-reviews.ts
- scripts/reviews/**tests**/backfill-reviews.test.ts
- data/ecosystem-v2/reviews.jsonl
- data/ecosystem-v2/retros.jsonl autonomous: true

must*haves: truths: - "reviews.jsonl contains entries for all 406 reviews (minus
64 known-skipped), each passing Zod validation" - "The 3 archive overlaps
produce no duplicate records (overlaps resolved by content richness,
KNOWN_DUPLICATE_IDS get disambiguated IDs)" - "Known-skipped IDs generate no
records, not even stubs" - "Running the script twice produces identical output
(idempotent)" - "Retro sections in archives produce RetroRecord entries in
retros.jsonl" - "Migration of v1 .claude/state/reviews.jsonl records into v2
format succeeds with zero validation errors" artifacts: - path:
"scripts/reviews/backfill-reviews.ts" provides: "Idempotent backfill
orchestrator reading all 13 archives + active log" min_lines: 80 - path:
"scripts/reviews/**tests**/backfill-reviews.test.ts" provides: "Integration
tests for backfill orchestrator" min_lines: 60 - path:
"data/ecosystem-v2/reviews.jsonl" provides: "All 406 reviews as validated v2
JSONL records" - path: "data/ecosystem-v2/retros.jsonl" provides: "Retro records
extracted from archives" key_links: - from:
"scripts/reviews/backfill-reviews.ts" to: "scripts/reviews/lib/parse-review.ts"
via: "imports parser functions" pattern:
"import.*parseArchiveFile|parseTableArchive|toV2ReviewRecord.*from.*parse-review" -
from: "scripts/reviews/backfill-reviews.ts" to:
"scripts/reviews/lib/write-jsonl.ts" via: "uses appendRecord for validated JSONL
writes" pattern: "import.*appendRecord.*from.*write-jsonl" - from:
"scripts/reviews/backfill-reviews.ts" to: "docs/archive/REVIEWS*_.md" via:
"reads all 13 archive files as input" pattern: "REVIEWS\_._\\.md"

---

<objective>
Build the idempotent backfill orchestrator that reads all 13 archives + active log, resolves overlaps and gaps, and writes validated v2 JSONL.

Purpose: This is the core data migration -- transforms 2 months of
inconsistently formatted markdown review history into a clean, schema-validated
JSONL dataset that becomes the foundation for all subsequent pipeline work.
Output: `backfill-reviews.ts` script, integration tests, and populated
`data/ecosystem-v2/reviews.jsonl` + `retros.jsonl`. </objective>

<execution_context> @~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md </execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-backfill-data-migration/02-RESEARCH.md
@.planning/phases/02-backfill-data-migration/02-01-SUMMARY.md
@scripts/reviews/lib/schemas/review.ts
@scripts/reviews/lib/schemas/shared.ts
@scripts/reviews/lib/schemas/retro.ts
@scripts/reviews/lib/write-jsonl.ts
@scripts/reviews/lib/read-jsonl.ts
@scripts/sync-reviews-to-jsonl.js
@scripts/check-review-archive.js
@docs/AI_REVIEW_LEARNINGS_LOG.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create backfill-reviews.ts orchestrator with overlap resolution and v1 migration</name>
  <files>scripts/reviews/backfill-reviews.ts</files>
  <action>
Create `scripts/reviews/backfill-reviews.ts` as an idempotent backfill script:

**Input sources (ordered by ID range):**

```
docs/archive/REVIEWS_1-40.md       (heading format)
docs/archive/REVIEWS_42-60.md      (heading format)
docs/archive/REVIEWS_61-100.md     (heading format)
docs/archive/REVIEWS_101-136.md    (TABLE format -- use parseTableArchive)
docs/archive/REVIEWS_137-179.md    (heading format)
docs/archive/REVIEWS_180-201.md    (heading format)
docs/archive/REVIEWS_202-212.md    (heading format)
docs/archive/REVIEWS_213-284.md    (heading format)
docs/archive/REVIEWS_285-346.md    (heading format)
docs/archive/REVIEWS_347-369.md    (heading format)
docs/archive/REVIEWS_354-357.md    (heading format)
docs/archive/REVIEWS_358-388.md    (heading format + retro sections)
docs/archive/REVIEWS_385-393.md    (heading format)
docs/AI_REVIEW_LEARNINGS_LOG.md    (active log, heading format)
```

**Algorithm:**

1. Read all 14 source files, parse each with appropriate parser (heading vs
   table)
2. Collect all ParsedEntry objects into `Map<number, ParsedEntry[]>` keyed by
   reviewNumber
3. **Overlap resolution:**
   - For KNOWN_DUPLICATE_IDS (366-369): keep BOTH copies, assign IDs `rev-{N}-a`
     (from earlier archive) and `rev-{N}-b` (from later archive). Set
     origin.session to identify which archive each came from.
   - For IDs 92-100 (appear in both REVIEWS_61-100 heading format AND
     REVIEWS_101-136 table format): prefer heading format (richer content)
   - For other duplicates: keep the entry with most rawLines (richest content)
4. **Gap handling:**
   - Filter out KNOWN_SKIPPED_IDS -- these produce NO records
   - For IDs that are genuinely missing (not in any archive, not in skip list):
     DO NOT create stub records. Just log them. The research says "7 archive
     gaps are accounted for" but the check-review-archive health check says "0
     missing" after accounting for skips.
5. **Convert each resolved ParsedEntry to v2 ReviewRecord** via
   toV2ReviewRecord()
6. **Retro extraction:** Scan archives for `### PR #N Retrospective` sections.
   Parse into RetroRecord objects. Only create records for explicit retro
   sections, not fabricated ones.
7. **V1 migration:** Read `.claude/state/reviews.jsonl` (50 v1 records with
   numeric IDs). Convert each to v2 format. For any review ID already captured
   from archives, prefer the archive version (more complete). Only add v1
   records that cover review IDs not found in archives (likely recent reviews
   #394+).
8. **Write output:**
   - Create `data/ecosystem-v2/` directory if not exists
   - Write `data/ecosystem-v2/reviews.jsonl` -- one JSON line per record, sorted
     by reviewNumber ascending
   - Write `data/ecosystem-v2/retros.jsonl` -- retro records sorted by date
   - Do NOT use appendRecord for bulk write (that's for single-record appends).
     Write the entire file atomically: build array of records, JSON.stringify
     each line, join with newlines, write file.
   - After writing: validate every record by reading back and parsing through
     Zod schema
9. **Print summary:** Total records, by completeness tier, overlaps resolved,
   retros found, v1 records migrated, validation errors (should be 0)

**BKFL-04 (retro arithmetic tagging):** When creating RetroRecord objects,
compute:

- `metrics.total_findings` from the associated review's total count
- `metrics.fix_rate` from fixed/total
- `metrics.pattern_recurrence` = 0 (set to 0 for backfill; real recurrence
  computed in Phase 3)

**BKFL-05 (consolidation counter fix):** Read `scripts/consolidation.json` (or
wherever consolidation state lives). Verify the "number" field matches actual
review count. If mismatch, log a warning with correct count. Do not auto-fix --
just report.

**BKFL-06 (pattern corrections):** In the field extractors or post-processing,
check for patterns #5 and #13 content errors referenced in requirements. If the
specific errors are identifiable in the markdown, apply corrections during
parsing. If not identifiable without more context, log them as items to
investigate and move on.

**Important:**

- Use `findProjectRoot()` walk-up pattern for reliable path resolution (per
  01-02 decision)
- Wrap ALL file reads in try/catch (per CLAUDE.md anti-pattern: file reads must
  be wrapped)
- The script must be runnable via:
  `cd scripts/reviews && npx tsc && node dist/backfill-reviews.js`
- Make it idempotent: running twice overwrites output file with identical
  content </action> <verify> Run: `cd scripts/reviews && npx tsc --noEmit` --
  compiles cleanly. Run:
  `cd scripts/reviews && npx tsc && node dist/backfill-reviews.js` -- produces
  output with summary. Check: `wc -l data/ecosystem-v2/reviews.jsonl` -- should
  have 340+ lines (406 reviews minus 64 skipped, plus a few disambiguated
  duplicates). Check: each line of reviews.jsonl is valid JSON and passes
  ReviewRecord schema. </verify> <done> backfill-reviews.ts reads all 13
  archives + active log + v1 state, resolves overlaps, writes validated
  reviews.jsonl and retros.jsonl. Running twice produces identical output. Zero
  Zod validation errors on output. </done> </task>

<task type="auto">
  <name>Task 2: Create backfill integration tests</name>
  <files>scripts/reviews/__tests__/backfill-reviews.test.ts</files>
  <action>
Create `scripts/reviews/__tests__/backfill-reviews.test.ts` with integration tests:

1. **Overlap resolution test:** Create mock entries where same reviewNumber
   appears from two different sources. Verify only one record emitted (the
   richer one). For KNOWN_DUPLICATE_IDS, verify both are emitted with `-a` and
   `-b` suffixes.

2. **Known-skipped exclusion test:** Feed entries including IDs from
   KNOWN_SKIPPED_IDS set. Verify they produce no output records.

3. **Completeness tier distribution test:** Parse a mix of heading entries
   (should be full/partial) and table entries (should be stub). Verify tier
   assignment.

4. **V1 migration merge test:** Create mock v1 records (numeric IDs) and mock
   archive entries. Verify archive entries take precedence for overlapping IDs,
   v1-only records (recent reviews) are included.

5. **Retro extraction test:** Create mock archive content with
   `### PR #N Retrospective` section. Verify RetroRecord is created with correct
   PR linkage and metrics computation.

6. **Idempotency test:** Run the resolution logic twice on same input. Verify
   output is identical (compare JSON-serialized arrays).

**Testing approach:** Extract the core logic (overlap resolution, v1 merge,
retro extraction) into testable functions exported from backfill-reviews.ts (or
a separate lib file). Tests call these functions directly with mock data rather
than reading real archive files (keeps tests fast and deterministic).

Follow project patterns: compile with tsc, run with `node --test` from
dist-tests. </action> <verify> Run:
`cd scripts/reviews && npx tsc && node --test dist-tests/__tests__/backfill-reviews.test.js`
-- all tests pass. </verify> <done> Integration tests cover overlap resolution,
skip-list exclusion, completeness tiers, v1 migration merge, retro extraction,
and idempotency. All pass. </done> </task>

</tasks>

<verification>
1. `cd scripts/reviews && npx tsc --noEmit` passes
2. `cd scripts/reviews && npx tsc && node dist/backfill-reviews.js` runs successfully with summary output
3. `data/ecosystem-v2/reviews.jsonl` exists with 340+ records, all passing Zod validation
4. `data/ecosystem-v2/retros.jsonl` exists with retro records
5. No duplicate review IDs in reviews.jsonl (except disambiguated 366-369 with -a/-b suffixes)
6. `node --test dist-tests/__tests__/backfill-reviews.test.js` passes all tests
7. Running backfill twice produces identical output files
</verification>

<success_criteria> reviews.jsonl contains entries for all 406 reviews (minus 64
known-skipped IDs), each passing Zod validation. The 3 archive overlaps produce
no unintended duplicates. KNOWN_DUPLICATE_IDS (366-369) are disambiguated. v1
state records are migrated. retros.jsonl contains explicitly found retrospective
records. Migration script produces zero Zod validation errors on output.
Requirements BKFL-01, BKFL-02, BKFL-04, BKFL-05, BKFL-06, BKFL-07 satisfied.
</success_criteria>

<output>
After completion, create `.planning/phases/02-backfill-data-migration/02-02-SUMMARY.md`
</output>
