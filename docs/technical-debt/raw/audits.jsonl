{"source_id":"audit:hash-U2Vzc2lvbi1z","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-workflow.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/session-start.js:1","line":0,"title":"Session-start timeout risks in low-bandwidth environments","description":"In slow network environments, npm commands can hang indefinitely, blocking the entire session start","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Session-start hook runs npm install, tsc builds, and multiple checks without timeout guards","sources":[],"merged_from":[]}
{"source_id":"audit:hash-RHVwbGljYXRl","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-workflow.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/:multiple","line":0,"title":"Duplicate hook validation for Write+Edit+MultiEdit tools","description":"Reduces hook execution time and prevents inconsistent behavior if one tool's handler is updated but others aren't","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Multiple hooks registered for Write, Edit, and MultiEdit with similar validation logic","sources":[],"merged_from":[]}
{"source_id":"audit:hash-RmlyZWJhc2Ug","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-workflow.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"firebase.json:1","line":0,"title":"Firebase deployment lacks rollback on partial failure","description":"Inconsistent deployment state can cause runtime errors when frontend expects API endpoints that haven't deployed","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"firebase.json configures both hosting and functions targets deployed together","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Q0kgcnVucyBm","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-workflow.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".github/workflows/ci.yml:1","line":0,"title":"CI runs full build twice - once for lint, once for test","description":"Reduces CI pipeline duration by eliminating redundant compilation, faster PR feedback","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"CI workflow has separate build steps for lint and test stages","sources":[],"merged_from":[]}
{"source_id":"audit:hash-U29uYXJDbG91","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-workflow.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".github/workflows/ci.yml:1","line":0,"title":"SonarCloud not enforced in CI gates","description":"Without enforcement, SonarCloud findings accumulate without accountability, reducing the tool's value","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"SonarCloud integration exists but not configured as blocking CI check","sources":[],"merged_from":[]}
{"source_id":"audit:hash-U2VudHJ5IGRp","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-workflow.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"lib/sentry.ts:1","line":0,"title":"Sentry disabled in dev mode by default","description":"Some errors only reproduce in specific conditions; having dev errors in Sentry helps identify patterns before they reach production","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Sentry initialization checks for production environment before enabling","sources":[],"merged_from":[]}
{"source_id":"audit:hash-SG9vayBwZXJm","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-workflow.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/:multiple","line":0,"title":"Hook performance not tracked - no metrics on execution time","description":"Without metrics, slow hooks accumulate unnoticed, gradually degrading developer experience","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"20+ hooks in .claude/hooks/ with no execution time tracking","sources":[],"merged_from":[]}
{"source_id":"audit:hash-UHJlLXB1c2gg","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-workflow.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".husky/pre-push:1","line":0,"title":"Pre-push duplicates pre-commit pattern checks","description":"Saves time per push and prevents graduated enforcement self-escalation (warned in pre-commit, blocked in pre-push)","recommendation":"","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Both .husky/pre-commit and .husky/pre-push run npm run patterns:check","sources":[],"merged_from":[]}
{"source_id":"audit:hash-dW5kZWZpbmVk","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-workflow.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"","line":0,"title":"Untitled finding","description":"Compaction-resilient state persistence with 4-layer approach (commit-tracker, compaction-handoff, pre-compaction-save, compact-restore)","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":["4 independent layers ensure no state loss across compaction","commit-log.jsonl as single source of truth","Successfully recovered from crashes in production use"],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-TWlzc2luZyBB","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-ux.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/navigation/:multiple","line":0,"title":"Missing ARIA labels on critical navigation tabs","description":"Navigation is the primary interaction pattern; missing ARIA labels make the app unusable for screen reader users","recommendation":"","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Tab components found without aria-label attributes in navigation components","sources":[],"merged_from":[]}
{"source_id":"audit:hash-U21hbGwgdG91","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-ux.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/navigation/:multiple","line":0,"title":"Small touch targets on mobile - tabs only 8-10px padding","description":"WCAG 2.5.8 requires minimum 44x44px touch targets; small targets cause frustration and errors on mobile","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Tab components with p-2 (8px) padding observed","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Tm8gZm9jdXMg","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-ux.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/:multiple","line":0,"title":"No focus trap management in modals and dialogs","description":"Focus escaping modals is a WCAG 2.4.3 violation and confusing for keyboard and screen reader users","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Modal components without focus trap implementation or aria-modal attribute","sources":[],"merged_from":[]}
{"source_id":"audit:hash-TWlzc2luZyBl","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-ux.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/:multiple","line":0,"title":"Missing error state illustrations - plain text errors only","description":"Visual error states are more noticeable and help users understand what went wrong faster","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Error handling in components uses plain text without icons or illustrations","sources":[],"merged_from":[]}
{"source_id":"audit:hash-TWlzc2luZyBs","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-ux.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"app/:multiple","line":0,"title":"Missing loading skeletons on data-heavy pages","description":"Consistent loading patterns reduce perceived wait time and create a more polished experience","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Mix of skeleton, spinner, and text loading states across app pages","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Tm8gb3B0aW1p","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-ux.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/journal/:multiple","line":0,"title":"No optimistic updates for form submissions","description":"Optimistic updates make the app feel instant and responsive, improving perceived performance","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Form components use loading state and await server response before UI update","sources":[],"merged_from":[]}
{"source_id":"audit:hash-TWlzc2luZyBr","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-ux.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"app/:multiple","line":0,"title":"Missing keyboard shortcuts for power users","description":"Power users who journal daily benefit significantly from keyboard shortcuts for repetitive actions","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"No keyboard event handlers or shortcut system found in app code","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Rm9ybSB2YWxp","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-ux.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/journal/:multiple","line":0,"title":"Form validation shows errors only on submit, not inline","description":"Inline validation prevents form submission failures and helps users correct mistakes in real-time","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Form components use Zod validation on submit but don't show field-level errors during input","sources":[],"merged_from":[]}
{"source_id":"audit:hash-TWlzc2luZyBD","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-testing.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"tests/:multiple","line":0,"title":"Missing Cloud Functions integration tests - only 1 skipped test exists","description":"Mock-only testing misses real Firestore query behavior, security rules, rate limiting, and function-to-function interactions","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"1 skipped integration test found; all other Cloud Function tests use mocked httpsCallable","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Tm8gdmlzdWFs","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-testing.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"package.json:1","line":0,"title":"No visual regression testing despite Playwright being installed","description":"Catches unintended CSS/layout regressions that functional tests miss; especially important with Tailwind utility classes that can have subtle side effects","recommendation":"","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Playwright in dependencies; 28 .protocol.json files exist but no screenshot comparison tests","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Q292ZXJhZ2Ug","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-testing.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"package.json:10-12","line":0,"title":"Coverage thresholds not enforced despite c8 being available","description":"Without thresholds, coverage can silently decrease as new code is added without tests","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"npm run test:coverage exists but no --check-coverage flags or .c8rc configuration found","sources":[],"merged_from":[]}
{"source_id":"audit:hash-TGltaXRlZCBz","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-testing.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"tests/:multiple","line":0,"title":"Limited script testing - 5 test files for 60+ npm scripts","description":"Script bugs cause cascading failures in the entire development pipeline; the pattern checker and security checker are critical infrastructure","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"85 files in scripts/, 60+ npm scripts, only ~5 test files covering script behavior","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Tm8gcGVyZm9y","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-testing.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/:multiple","line":0,"title":"No performance benchmarks for pagination and data loading","description":"Performance regressions are hard to detect in code review; automated benchmarks catch them before users notice","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Pagination and data loading mentioned in ROADMAP as P0 concerns but no automated performance tests exist","sources":[],"merged_from":[]}
{"source_id":"audit:hash-RXJyb3IgYm91","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-testing.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/:multiple","line":0,"title":"Error boundary and Sentry integration not tested","description":"If error boundaries fail silently, users see white screens instead of graceful fallbacks; if Sentry breaks, errors go unreported","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Sentry integration in lib/sentry.ts, error boundaries in components, no test coverage for either","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Rmxha3kgdGlt","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-testing.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"tests/rate-limiter.test.ts:1","line":0,"title":"Flaky timing in rate limiter tests using real setTimeout","description":"Real timer tests are inherently flaky under CI load and slow down the test suite unnecessarily","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Rate limiter test has explicit setTimeout waits: 30ms, 72ms, 110ms observed in test output","sources":[],"merged_from":[]}
{"source_id":"audit:hash-bXN3IGluc3Rh","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-testing.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"package.json:1","line":0,"title":"msw installed but unused for API mocking","description":"msw provides more realistic API mocking that catches serialization and request format issues","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"msw in package.json dependencies but no msw handlers or setup files found in tests/","sources":[],"merged_from":[]}
{"source_id":"audit:hash-RW5hYmxlIE5l","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-infrastructure.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"next.config.mjs:12-19","line":0,"title":"Enable Next.js experimental optimizations and bundle analysis","description":"Large map/chart libraries (leaflet, recharts) are imported but not tree-shaken optimally","recommendation":"","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"app/layout.tsx imports multiple heavy libraries, no dynamic imports detected, meeting-map.tsx imports entire leaflet","sources":[],"merged_from":[]}
{"source_id":"audit:hash-QWRkIENvbnRl","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-infrastructure.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"firebase.json:6-57","line":0,"title":"Add Content-Security-Policy header to Firebase hosting configuration","description":"CSP provides additional layer against XSS attacks; industry best practice for modern web apps","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"firebase.json has X-Frame-Options, HSTS, X-Content-Type-Options but no CSP","sources":[],"merged_from":[]}
{"source_id":"audit:hash-QWRkIGVuZ2lu","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-infrastructure.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"package.json:1-165","line":0,"title":"Add engines field to root package.json for CI/CD consistency","description":"Ensures consistent Node version across development, CI, and production","recommendation":"","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"functions/package.json:14 specifies node 20, root has no engines field","sources":[],"merged_from":[]}
{"source_id":"audit:hash-RmlyZXN0b3Jl","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-infrastructure.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"firebase.json:72-74","line":0,"title":"Firestore security rules file not found in repository","description":"Security rules are critical infrastructure and should be version controlled for audit trail","recommendation":"","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"firebase.json:73 references firestore.rules, no .rules files found in glob search","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Q29uZmlndXJl","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-infrastructure.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"next.config.mjs:12","line":0,"title":"Configure Next.js build cache and dependency caching for faster CI builds","description":"Faster feedback loops for PRs; reduced CI minutes consumption","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"tsconfig.json:15 has incremental: true, no CI cache actions found","sources":[],"merged_from":[]}
{"source_id":"audit:hash-TGF6eSBsb2Fk","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-infrastructure.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/maps/meeting-map.tsx:1-80","line":0,"title":"Lazy load Leaflet and Recharts with dynamic imports and Suspense","description":"Not all users visit map/chart pages; faster time-to-interactive for initial load","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"meeting-map.tsx imports full leaflet synchronously, no dynamic imports for recharts","sources":[],"merged_from":[]}
{"source_id":"audit:hash-VXBncmFkZSBG","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-infrastructure.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"firebase.json:68","line":0,"title":"Upgrade Firebase Functions to Node.js 24 runtime","description":"Longer LTS support window; aligns with bleeding-edge philosophy of the project","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"firebase.json:68 shows nodejs24 available, functions uses node 20","sources":[],"merged_from":[]}
{"source_id":"audit:hash-QWRkIGNvbXBv","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-infrastructure.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"firestore.indexes.json:18-80","line":0,"title":"Add composite index for userId + timestamp on security_logs collection","description":"Admin panel likely needs per-user security logs; composite index prevents full collection scans","recommendation":"","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"firestore.indexes.json has security_logs indexes but only severity/type/functionName composites","sources":[],"merged_from":[]}
{"source_id":"audit:hash-QUlfV09SS0ZM","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-docs.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"tech-debt","file":"AI_WORKFLOW.md:1","line":0,"title":"AI_WORKFLOW.md size optimization - 872 lines may exceed effective AI context loading","description":"Shorter AI instructions mean less token waste per session and faster context loading","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"AI_WORKFLOW.md is 872 lines. CLAUDE.md was successfully reduced 77% (497â†’118 lines) in v5.0 with positive results","sources":[],"merged_from":[]}
{"source_id":"audit:hash-U3RhbGUgZG9j","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-docs.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"tech-debt","file":"docs/:multiple","line":0,"title":"Stale documentation markers - 96 TODO/TBD occurrences across 34 files","description":"Stale TODOs reduce documentation credibility and create confusion about what is actually planned vs abandoned","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"96 TODO/TBD occurrences found across 34 files via grep","sources":[],"merged_from":[]}
{"source_id":"audit:hash-U2VjdXJpdHkg","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-docs.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"tech-debt","file":"docs/SECURITY.md:1","line":0,"title":"Security guidance consolidation - multiple overlapping security docs","description":"Duplicated security guidance risks contradictions when one doc is updated but others are not, and increases maintenance burden","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"4 separate security-focused documents with overlapping content on path traversal, error sanitization, and input validation","sources":[],"merged_from":[]}
{"source_id":"audit:hash-RG9jdW1lbnRh","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-docs.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"tech-debt","file":"docs/:multiple","line":0,"title":"Documentation effectiveness metrics - no tracking of which docs are useful","description":"Data-driven documentation pruning would reduce maintenance burden and focus effort on docs that actually improve outcomes","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"300+ markdown files in the project, unclear which are actively referenced vs historical","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Q3Jvc3MtcmVm","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-docs.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"tech-debt","file":"CLAUDE.md:1","line":0,"title":"Cross-reference navigation overhead in documentation","description":"Reducing indirection means AI agents and humans can understand guidance without chasing references across files","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Multiple 'See: docs/agent_docs/...' references in CLAUDE.md and AI_WORKFLOW.md","sources":[],"merged_from":[]}
{"source_id":"audit:hash-UHJlLWNvbW1p","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-devx.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".husky/pre-commit:1","line":0,"title":"Pre-commit hook parallelization - sequential checks could run 40% faster","description":"With 361 commits in 2 weeks, even 1-2s savings per commit adds up to 6-12 minutes of developer time saved","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Pre-commit hook is 279 lines with 13 sequential validation gates","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Q0kgbm9uLWJs","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-devx.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".github/workflows/ci.yml:1","line":0,"title":"CI non-blocking checks should block - 5 checks use continue-on-error","description":"Non-blocking checks that always pass provide false confidence. If they fail silently, issues accumulate undetected","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"5 checks with continue-on-error: true in ci.yml","sources":[],"merged_from":[]}
{"source_id":"audit:hash-U2NyaXB0IGNv","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-devx.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"scripts/:multiple","line":0,"title":"Script consolidation - 30+ check/validate/sync scripts with inconsistent CLI patterns","description":"Consistent CLI patterns reduce cognitive load and make scripts easier to discover, document, and maintain","recommendation":"","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"82 npm scripts in package.json, 85 files in scripts/ directory","sources":[],"merged_from":[]}
{"source_id":"audit:hash-R2VuZXJpYyBi","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-content.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/journal/journal-entry-form.tsx:1","line":0,"title":"Generic button labels - Submit used instead of action-specific text","description":"Action-specific button labels reduce cognitive load and confirm what will happen when clicked","recommendation":"","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Generic Submit/Save buttons found in form components","sources":[],"merged_from":[]}
{"source_id":"audit:hash-RXJyb3IgbWVz","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-content.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/:multiple","line":0,"title":"Error messages don't guide user to fix the problem","description":"Actionable error messages reduce support burden and user frustration, especially for daily-use apps","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Error handling shows generic messages like Something went wrong or An error occurred","sources":[],"merged_from":[]}
{"source_id":"audit:hash-SW5jb25zaXN0","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-content.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/meetings/:multiple","line":0,"title":"Inconsistent terminology - meeting vs session vs appointment","description":"Inconsistent terminology confuses users and makes the app feel less professional","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Multiple terms for same concept found across meeting-related components","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Q29uZmlybWF0","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-content.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/:multiple","line":0,"title":"Confirmation dialogs lack specific consequences","description":"Users need to understand consequences before destructive actions, especially for personal recovery data","recommendation":"","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Confirmation dialogs found with generic Are you sure? messaging","sources":[],"merged_from":[]}
{"source_id":"audit:hash-TG9hZGluZyB0","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-content.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"app/:multiple","line":0,"title":"Loading text inconsistency - Loading... vs Fetching vs spinner only","description":"Consistent loading patterns create a more cohesive, polished user experience","recommendation":"","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Multiple loading text variants found across app and component directories","sources":[],"merged_from":[]}
{"source_id":"audit:hash-RGF0ZSBmb3Jt","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-content.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/:multiple","line":0,"title":"Date formats inconsistent across the app","description":"Inconsistent date formats reduce readability and make the app feel unpolished","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Multiple date formatting approaches found across components and lib files","sources":[],"merged_from":[]}
{"source_id":"audit:hash-U3VjY2VzcyBm","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-content.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/:multiple","line":0,"title":"Success feedback messages too brief","description":"Contextual success messages confirm the user's action and build confidence that the right thing was saved","recommendation":"","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Toast notifications with minimal text observed in form submission handlers","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Q29uc29saWRh","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/mood-selector.tsx:1","line":0,"title":"Consolidate duplicate mood selector components - 3 implementations with diverging mood options","description":"Duplicate components diverge over time, creating inconsistent user experience and tripling the maintenance burden for mood-related features","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"3 mood selector implementations found with different emoji sets across components/","sources":[],"merged_from":[]}
{"source_id":"audit:hash-RXh0cmFjdCBt","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/meetings/meeting-form.tsx:1","line":0,"title":"Extract meeting time parsing logic - ~80 lines duplicated between 2 files","description":"Duplicated parsing logic is a bug magnet - fixes applied to one copy but not the other create subtle inconsistencies","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"~80 lines of time parsing duplicated between 2 meeting component files","sources":[],"merged_from":[]}
{"source_id":"audit:hash-TWlncmF0ZSB1","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"hooks/useAuth.ts:1","line":0,"title":"Migrate useAuth consumers to focused hooks - 16 components still using deprecated hook","description":"Focused hooks prevent unnecessary re-renders when only user or only todayLog changes, improving perceived performance especially on mobile","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"16 components import useAuth vs 2 using focused hooks. Auth context is split into AuthProvider (stable) and TodayLogProvider (volatile)","sources":[],"merged_from":[]}
{"source_id":"audit:hash-RXh0cmFjdCBj","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/journal/journal-entry-form.tsx:1","line":0,"title":"Extract common form state logic - ~150 lines of duplicate submission handling","description":"Reduces boilerplate in form components and ensures consistent error handling and user feedback patterns","recommendation":"","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"~150 lines of similar submission handling found across 3+ form components","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Q3JlYXRlIGJh","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/features/index.ts:1","line":0,"title":"Create barrel exports for component directories - only 1 exists","description":"Cleaner imports improve code readability and make refactoring easier since internal file moves don't break external imports","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Only 1 barrel export found. 38 default exports vs 69 named exports showing no consistent convention","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Q29sbG9jYXRl","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/quotes/:1","line":0,"title":"Collocate related quote components - 3 variants scattered across 2 directories","description":"Scattered related components make it harder to understand the full quote feature surface area and increase risk of unintentional divergence","recommendation":"","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"3 quote variants found across 2 directories","sources":[],"merged_from":[]}
{"source_id":"audit:hash-U3RhbmRhcmRp","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"components/:multiple","line":0,"title":"Standardize export style - 38 default vs 69 named exports with no convention","description":"Consistent exports improve IDE support (auto-imports work better with named exports) and prevent the common default-export renaming issue","recommendation":"","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"38 default exports vs 69 named exports found across components/","sources":[],"merged_from":[]}
{"source_id":"audit:hash-dXNlRGFpbHlR","source_file":"docs\\audits\\single-session\\enhancements\\audit-2026-02-11\\stage-1-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":"hooks/useDailyQuote.ts:1","line":0,"title":"useDailyQuote hook has smart module-level caching preventing duplicate fetches","description":"Prevents unnecessary API calls and ensures consistent data across components","recommendation":"","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Module-level cache variable in useDailyQuote.ts shared across 3 consumers","sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/lib::missing-readme","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/lib::missing-readme","category":"process","severity":"S3","type":"process-gap","file":"scripts/lib/","line":1,"title":"Docs: Missing README in scripts/lib/","description":"The lib/ directory contains shared utilities used across multiple scripts. Without a README, developers cannot quickly understand what utilities are available, their purpose, or usage patterns. This increases onboarding time and risk of code duplication.","recommendation":"Create scripts/lib/README.md documenting each utility module: ai-pattern-checks.js (AI pattern detection), sanitize-error.js (error sanitization), security-helpers.js (security utilities), validate-paths.js (path validation).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/config::missing-readme","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/config::missing-readme","category":"process","severity":"S3","type":"process-gap","file":"scripts/config/","line":1,"title":"Docs: Missing README in scripts/config/","description":"The config/ directory contains JSON configuration files used by multiple scripts. Without documentation explaining the schema and purpose of each config file, developers may misuse configs or fail to update them when requirements change.","recommendation":"Create scripts/config/README.md documenting each config file: audit-schema.json (valid audit field values), audit-config.json (category thresholds), ai-patterns.json (AI pattern detection rules), skill-config.json (skill definitions), and load-config.js (config loader utility).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/debt::missing-readme","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/debt::missing-readme","category":"process","severity":"S3","type":"process-gap","file":"scripts/debt/","line":1,"title":"Docs: Missing README in scripts/debt/","description":"The debt/ directory contains 17 technical debt management scripts. Without a README, developers cannot understand the debt workflow, which scripts to run in which order, or how to properly intake and resolve debt items.","recommendation":"Create scripts/debt/README.md documenting the technical debt workflow: intake scripts (intake-audit.js, intake-manual.js, intake-pr-deferred.js), processing scripts (normalize-all.js, consolidate-all.js, dedup-multi-pass.js), and resolution scripts (resolve-item.js, resolve-bulk.js). Include usage examples and workflow diagrams.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/audit::missing-readme","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/audit::missing-readme","category":"process","severity":"S3","type":"process-gap","file":"scripts/audit/","line":1,"title":"Docs: Missing README in scripts/audit/","description":"The audit/ directory contains audit-related scripts without documentation. Developers need to understand what these scripts do and when to use them as part of the audit process.","recommendation":"Create scripts/audit/README.md documenting: transform-jsonl-schema.js (schema transformation) and validate-audit-integration.js (audit validation). Include usage examples and integration points with the main audit workflow.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/enrich-addresses.ts::no-header","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/enrich-addresses.ts::no-header","category":"process","severity":"S3","type":"process-gap","file":"scripts/enrich-addresses.ts","line":1,"title":"Docs: No header comment in enrich-addresses.ts","description":"Script lacks header comment explaining purpose, making it difficult for developers to understand what it does without reading the implementation. Header comments serve as quick documentation.","recommendation":"Add JSDoc header comment explaining: Purpose (enrich meeting addresses with geocoding data from Nominatim/OSM), Usage (npx tsx scripts/enrich-addresses.ts), Prerequisites (Firebase service account, internet connection), and Rate limits (OSM Nominatim 1req/sec).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/test-geocode.ts::no-header","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/test-geocode.ts::no-header","category":"process","severity":"S3","type":"process-gap","file":"scripts/test-geocode.ts","line":1,"title":"Docs: No header comment in test-geocode.ts","description":"Test script lacks header comment explaining purpose and usage. Without documentation, developers don't know this is a test utility or what it validates.","recommendation":"Add JSDoc header comment explaining: Purpose (test Google Maps Geocoding API connectivity and credentials), Usage (npx tsx scripts/test-geocode.ts), Prerequisites (NEXT_PUBLIC_FIREBASE_API_KEY environment variable), and Expected output.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/sync-geocache.ts::no-header","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/sync-geocache.ts::no-header","category":"process","severity":"S3","type":"process-gap","file":"scripts/sync-geocache.ts","line":1,"title":"Docs: No header comment in sync-geocache.ts","description":"Script lacks header comment. Developers cannot quickly understand this script syncs geocoding results from Firestore to a local cache file for performance optimization.","recommendation":"Add JSDoc header comment explaining: Purpose (sync meeting coordinates from Firestore to local geocoding_cache.json), Usage (npx tsx scripts/sync-geocache.ts), Prerequisites (Firebase service account, meetings collection with coordinates), and Output (geocoding_cache.json with sorted address-to-coordinate mappings).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/retry-failures.ts::no-header","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/retry-failures.ts::no-header","category":"process","severity":"S3","type":"process-gap","file":"scripts/retry-failures.ts","line":1,"title":"Docs: No header comment in retry-failures.ts","description":"Script lacks header comment. Developers need to understand this retries failed geocoding operations from enrichment_failures.json.","recommendation":"Add JSDoc header comment explaining: Purpose (retry failed geocoding operations from enrichment_failures.json), Usage (npx tsx scripts/retry-failures.ts), Prerequisites (enrichment_failures.json, Firebase service account, OSM Nominatim access), Dependencies (requires prior run of enrich-addresses.ts), and Rate limits (2.5 second delay between requests).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/migrate-library-content.ts::no-header","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/migrate-library-content.ts::no-header","category":"process","severity":"S3","type":"process-gap","file":"scripts/migrate-library-content.ts","line":1,"title":"Docs: No header comment in migrate-library-content.ts","description":"Migration script lacks header comment. Developers need to know this is a one-time migration from hardcoded library links to Firestore, when to run it, and side effects.","recommendation":"Add JSDoc header comment explaining: Purpose (one-time migration of library quick links from hardcoded array to Firestore), Usage (npx tsx scripts/migrate-library-content.ts), Prerequisites (Firebase service account), Warning (one-time use only, idempotent), and Output (populates library_content collection).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/seed-real-data.ts::no-header","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/seed-real-data.ts::no-header","category":"process","severity":"S3","type":"process-gap","file":"scripts/seed-real-data.ts","line":1,"title":"Docs: No header comment in seed-real-data.ts","description":"Data seeding script lacks header comment. Developers need to understand this imports meeting data from CSV with geocoding, when to use it, and potential performance implications.","recommendation":"Add JSDoc header comment explaining: Purpose (import meeting data from SoNash_Meetings__cleaned.csv with geocoding), Usage (npx tsx scripts/seed-real-data.ts), Prerequisites (CSV file, Firebase service account, Google Maps API key or Nominatim access), Performance notes (rate limited 1.1s per request), and Output (populates meetings collection with geocoded addresses).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/dedupe-quotes.ts::minimal-header","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/dedupe-quotes.ts::minimal-header","category":"process","severity":"S3","type":"process-gap","file":"scripts/dedupe-quotes.ts","line":1,"title":"Docs: Inadequate header in dedupe-quotes.ts","description":"Script has only 2-line header comment that doesn't explain purpose, usage, or expected workflow. Makes it difficult to understand when/how to use this deduplication utility.","recommendation":"Expand header comment to explain: Purpose (deduplication of recovery quotes from recovery_quotes_50.md), Usage, Expected input/output format, and Integration with the quote management system. Current comment 'Deduplication Script for Recovery Quotes' is too terse.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/config/ai-patterns.json::no-schema-docs","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/config/ai-patterns.json::no-schema-docs","category":"process","severity":"S3","type":"process-gap","file":"scripts/config/ai-patterns.json","line":1,"title":"Docs: Config file ai-patterns.json lacks inline documentation","description":"Configuration file defines AI pattern detection rules without schema documentation. Developers modifying patterns need to understand the structure: pattern object format, severity levels, regex descriptor format.","recommendation":"Add JSON comment block at top (or convert to .jsonc) documenting schema: {patterns: {[key]: {name, severity, patterns: [{source, flags}], description}}}. Explain that 'source' is regex pattern string, 'flags' are regex flags, 'severity' is S0-S3.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/config/audit-config.json::no-schema-docs","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/config/audit-config.json::no-schema-docs","category":"process","severity":"S3","type":"process-gap","file":"scripts/config/audit-config.json","line":1,"title":"Docs: Config file audit-config.json lacks inline documentation","description":"Configuration file defines audit category thresholds without documentation. Developers tuning thresholds need to understand units (commit counts vs file counts), regex descriptor format, and threshold semantics.","recommendation":"Add documentation explaining: categoryThresholds structure (commits=count, files=count, filePattern=regex descriptor, excludePattern=regex descriptor), multiAiThresholds (totalCommits threshold, daysSinceAudit threshold), categoryHeaders (regex to find category sections in AUDIT_TRACKER.md). Document that regex descriptors use {source, flags} format.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/config/audit-schema.json::no-schema-docs","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/config/audit-schema.json::no-schema-docs","category":"process","severity":"S3","type":"process-gap","file":"scripts/config/audit-schema.json","line":1,"title":"Docs: Config file audit-schema.json lacks inline documentation","description":"Configuration file defines valid audit field values without explanation. Developers need to understand when to use each category, severity, type, status, and effort level.","recommendation":"Add documentation explaining: validCategories (what each category means: security, performance, code-quality, documentation, process, refactoring, engineering-productivity), validSeverities (S0=critical, S1=high, S2=medium, S3=low), validTypes (bug, code-smell, vulnerability, hotspot, tech-debt, process-gap), validStatuses (lifecycle), validEfforts (E0=days, E1=hours, E2=30min, E3=5min), requiredFields.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/config/skill-config.json::no-schema-docs","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/config/skill-config.json::no-schema-docs","category":"process","severity":"S3","type":"process-gap","file":"scripts/config/skill-config.json","line":1,"title":"Docs: Config file skill-config.json lacks inline documentation","description":"Configuration file defines skill sections and patterns without documentation. Developers adding or modifying skills need to understand the schema structure and pattern format.","recommendation":"Add documentation explaining: requiredSections structure (audit vs session skill types), deprecatedPatterns array format ({pattern: {source, flags}, message}), topicAliases mapping (canonical topic to related terms). Explain regex descriptor format and how aliases improve search accuracy.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/aggregate-audit-findings.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/aggregate-audit-findings.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/aggregate-audit-findings.js","line":1,"title":"Docs: Excessively long file aggregate-audit-findings.js (1934 lines)","description":"File is 1934 lines (nearly 4x recommended 500-line limit). Large files are harder to understand, test, and maintain. Indicates multiple responsibilities that should be separated.","recommendation":"Split into modules: parsers.js (parseCanonItems, parseSingleSessionAudit, parseRoadmapItems, parseTechDebtItems, parseBacklogIssues), deduplication.js (deduplication logic, similarity scoring, shouldMerge), formatters.js (markdown formatting, JSONL output), and main orchestrator (aggregate-audit-findings.js). Move configuration constants to config files.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/analyze-learning-effectiveness.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/analyze-learning-effectiveness.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/analyze-learning-effectiveness.js","line":1,"title":"Docs: Excessively long file analyze-learning-effectiveness.js (1271 lines)","description":"File is 1271 lines (2.5x recommended limit). Contains analysis, reporting, and interactive CLI logic that should be separated for maintainability and testability.","recommendation":"Split into modules: pattern-analyzer.js (pattern detection and recurrence analysis), metrics-calculator.js (effectiveness scoring and statistics), report-generator.js (dashboard and detailed reports), cli-interface.js (readline prompts and interactive mode), and main orchestrator (analyze-learning-effectiveness.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/audit/validate-audit-integration.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/audit/validate-audit-integration.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/audit/validate-audit-integration.js","line":1,"title":"Docs: Excessively long file validate-audit-integration.js (1242 lines)","description":"File is 1242 lines (2.5x recommended limit). Validation script contains schema validation, false positive checking, evidence validation, and reporting that should be modular.","recommendation":"Split into modules: schema-validator.js (JSONL schema validation), false-positive-checker.js (FP database lookup), evidence-validator.js (file:line verification), tool-integrator.js (npm audit, ESLint, patterns:check), report-generator.js (validation reports), and main orchestrator (validate-audit-integration.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-review-needed.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/check-review-needed.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-review-needed.js","line":1,"title":"Docs: Excessively long file check-review-needed.js (1056 lines)","description":"File is 1056 lines (2x recommended limit). Contains git analysis, threshold checking, SonarCloud integration, and reporting that should be modular.","recommendation":"Split into modules: git-analyzer.js (commit counting, file change detection), threshold-checker.js (per-category threshold logic), sonarcloud-client.js (API integration), category-rules.js (category-specific thresholds), report-generator.js (human and JSON output), and main orchestrator (check-review-needed.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/multi-ai/normalize-format.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/multi-ai/normalize-format.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/multi-ai/normalize-format.js","line":1,"title":"Docs: Excessively long file multi-ai/normalize-format.js (1014 lines)","description":"File is 1014 lines (2x recommended limit). Format normalization script handles multiple input formats (JSONL, JSON, markdown, plain text) that should be separate parser modules.","recommendation":"Split into modules: jsonl-parser.js, json-parser.js, markdown-parser.js (tables, lists, headers), text-parser.js, format-detector.js (auto-detection), schema-normalizer.js (output normalization), and main orchestrator (normalize-format.js). Each parser should export parse() function.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/validate-audit.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/validate-audit.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/validate-audit.js","line":1,"title":"Docs: Excessively long file validate-audit.js (980 lines)","description":"File is 980 lines (nearly 2x recommended limit). Post-audit validation contains multiple validation types, external tool integration, and confidence scoring that should be modular.","recommendation":"Split into modules: false-positive-validator.js (FP database checking), evidence-validator.js (file:line verification, code snippet validation), tool-validator.js (npm audit, ESLint, patterns:check cross-reference), confidence-scorer.js (confidence level validation), duplicate-detector.js, and main orchestrator (validate-audit.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/generate-documentation-index.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/generate-documentation-index.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/generate-documentation-index.js","line":1,"title":"Docs: Excessively long file generate-documentation-index.js (980 lines)","description":"File is 980 lines (nearly 2x recommended limit). Documentation indexer combines file traversal, markdown parsing, hierarchy building, and output formatting that should be separated.","recommendation":"Split into modules: file-traverser.js (recursive directory traversal with excludes), markdown-parser.js (frontmatter extraction, header parsing), hierarchy-builder.js (tree structure generation), output-formatter.js (JSON and markdown formatting), and main orchestrator (generate-documentation-index.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-docs-light.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/check-docs-light.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-docs-light.js","line":1,"title":"Docs: Excessively long file check-docs-light.js (866 lines)","description":"File is 866 lines (1.7x recommended limit). Documentation checker combines multiple validation types (frontmatter, links, structure, coverage) that should be separate modules.","recommendation":"Split into modules: frontmatter-validator.js, link-checker.js (internal and external), structure-validator.js (heading hierarchy, required sections), coverage-analyzer.js (undocumented files), report-generator.js, and main orchestrator (check-docs-light.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-pattern-compliance.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/check-pattern-compliance.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-pattern-compliance.js","line":1,"title":"Docs: Excessively long file check-pattern-compliance.js (834 lines)","description":"File is 834 lines (1.7x recommended limit). Pattern compliance checker combines pattern loading, file scanning, pattern matching, and reporting that should be modular.","recommendation":"Split into modules: pattern-loader.js (load from ai-patterns.json), file-scanner.js (recursive scanning with excludes), pattern-matcher.js (regex matching with performance limits), violation-reporter.js (format violations for output), and main orchestrator (check-pattern-compliance.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/debt/sync-sonarcloud.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/debt/sync-sonarcloud.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/debt/sync-sonarcloud.js","line":1,"title":"Docs: Excessively long file debt/sync-sonarcloud.js (770 lines)","description":"File is 770 lines (1.5x recommended limit). SonarCloud sync combines API client logic, data transformation, deduplication, and persistence that should be separated.","recommendation":"Split into modules: sonarcloud-client.js (API requests, authentication, pagination), issue-transformer.js (SonarCloud to internal schema mapping), deduplicator.js (match existing debt items), persistence.js (JSONL writing), and main orchestrator (sync-sonarcloud.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/audit/transform-jsonl-schema.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/audit/transform-jsonl-schema.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/audit/transform-jsonl-schema.js","line":1,"title":"Docs: Excessively long file audit/transform-jsonl-schema.js (761 lines)","description":"File is 761 lines (1.5x recommended limit). Schema transformation combines parsing, validation, field mapping, and output generation that should be modular.","recommendation":"Split into modules: jsonl-parser.js (parse JSONL with error handling), schema-mapper.js (field mapping rules), field-validator.js (validate transformed output), output-generator.js (write transformed JSONL), and main orchestrator (transform-jsonl-schema.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/run-consolidation.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/run-consolidation.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/run-consolidation.js","line":1,"title":"Docs: Excessively long file run-consolidation.js (743 lines)","description":"File is 743 lines (1.5x recommended limit). Consolidation orchestrator combines workflow logic, file I/O, validation, and reporting that should be separated.","recommendation":"Split into modules: consolidation-workflow.js (step orchestration), file-manager.js (read/write JSONL files), validation-runner.js (schema validation), report-generator.js (consolidation reports), and main entry point (run-consolidation.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/multi-ai/unify-findings.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/multi-ai/unify-findings.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/multi-ai/unify-findings.js","line":1,"title":"Docs: Excessively long file multi-ai/unify-findings.js (716 lines)","description":"File is 716 lines (1.4x recommended limit). Finding unification combines parsing, similarity detection, merging, and conflict resolution that should be modular.","recommendation":"Split into modules: finding-parser.js (parse multiple formats), similarity-detector.js (detect duplicate findings), finding-merger.js (merge logic), conflict-resolver.js (handle disagreements), and main orchestrator (unify-findings.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/archive-doc.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/archive-doc.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/archive-doc.js","line":1,"title":"Docs: Excessively long file archive-doc.js (712 lines)","description":"File is 712 lines (1.4x recommended limit). Document archival combines file moving, content transformation, index updating, and git operations that should be separated.","recommendation":"Split into modules: file-archiver.js (move files to archive), content-transformer.js (add archive metadata), index-updater.js (update documentation indexes), git-operations.js (stage and commit), and main orchestrator (archive-doc.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-external-links.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/check-external-links.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-external-links.js","line":1,"title":"Docs: Excessively long file check-external-links.js (701 lines)","description":"File is 701 lines (1.4x recommended limit). Link checker combines link extraction, HTTP requests, caching, retry logic, and reporting that should be modular.","recommendation":"Split into modules: link-extractor.js (parse markdown for links), http-checker.js (validate URLs with retries), cache-manager.js (link validation cache), rate-limiter.js (throttle requests), report-generator.js, and main orchestrator (check-external-links.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/phase-complete-check.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/phase-complete-check.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/phase-complete-check.js","line":1,"title":"Docs: Excessively long file phase-complete-check.js (690 lines)","description":"File is 690 lines (1.4x recommended limit). Phase completion checker combines requirement parsing, status checking, dependency validation, and reporting that should be separated.","recommendation":"Split into modules: requirement-parser.js (parse phase requirements), status-checker.js (check completion criteria), dependency-validator.js (validate phase dependencies), blocking-analyzer.js (identify blockers), report-generator.js, and main orchestrator (phase-complete-check.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-doc-placement.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/check-doc-placement.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-doc-placement.js","line":1,"title":"Docs: Excessively long file check-doc-placement.js (616 lines)","description":"File is 616 lines (1.2x recommended limit). Document placement checker combines rules loading, file scanning, rule evaluation, and reporting that should be modular.","recommendation":"Split into modules: placement-rules.js (load and parse placement rules), file-scanner.js (find documents), rule-evaluator.js (check documents against rules), violation-detector.js (identify misplaced docs), report-generator.js, and main orchestrator (check-doc-placement.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/multi-ai/fix-schema.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/multi-ai/fix-schema.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/multi-ai/fix-schema.js","line":1,"title":"Docs: Excessively long file multi-ai/fix-schema.js (615 lines)","description":"File is 615 lines (1.2x recommended limit). Schema fixing combines validation, field correction, migration, and output that should be separated.","recommendation":"Split into modules: schema-validator.js (detect schema issues), field-fixer.js (correction rules for each field type), migration-rules.js (schema version migrations), output-writer.js (write corrected JSONL), and main orchestrator (fix-schema.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/multi-ai/aggregate-category.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/multi-ai/aggregate-category.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/multi-ai/aggregate-category.js","line":1,"title":"Docs: Excessively long file multi-ai/aggregate-category.js (603 lines)","description":"File is 603 lines (1.2x recommended limit). Category aggregation combines parsing, grouping, statistics, and output formatting that should be modular.","recommendation":"Split into modules: category-parser.js (parse findings by category), grouping-engine.js (group related findings), statistics-calculator.js (compute category metrics), output-formatter.js (generate reports), and main orchestrator (aggregate-category.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/verify-sonar-phase.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/verify-sonar-phase.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/verify-sonar-phase.js","line":1,"title":"Docs: Excessively long file verify-sonar-phase.js (597 lines)","description":"File is 597 lines (1.2x recommended limit). SonarCloud verification combines API calls, phase validation, issue tracking, and reporting that should be separated.","recommendation":"Split into modules: sonarcloud-client.js (API integration), phase-requirements.js (load phase criteria), issue-tracker.js (track verification issues), validation-engine.js (check requirements), report-generator.js, and main orchestrator (verify-sonar-phase.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/update-readme-status.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/update-readme-status.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/update-readme-status.js","line":1,"title":"Docs: Excessively long file update-readme-status.js (597 lines)","description":"File is 597 lines (1.2x recommended limit). README updater combines status collection, badge generation, markdown formatting, and file writing that should be modular.","recommendation":"Split into modules: status-collector.js (gather status from multiple sources), badge-generator.js (create status badges), markdown-formatter.js (format status sections), file-updater.js (in-place README updates), and main orchestrator (update-readme-status.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/debt/intake-audit.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/debt/intake-audit.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/debt/intake-audit.js","line":1,"title":"Docs: Excessively long file debt/intake-audit.js (586 lines)","description":"File is 586 lines (1.2x recommended limit). Audit intake combines parsing, validation, ID generation, deduplication, and persistence that should be separated.","recommendation":"Split into modules: audit-parser.js (parse audit JSONL), item-validator.js (schema validation), id-generator.js (generate debt IDs), deduplicator.js (detect duplicates), persister.js (write to MASTER_DEBT.jsonl), and main orchestrator (intake-audit.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/generate-detailed-sonar-report.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/generate-detailed-sonar-report.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/generate-detailed-sonar-report.js","line":1,"title":"Docs: Excessively long file generate-detailed-sonar-report.js (561 lines)","description":"File is 561 lines (1.1x recommended limit). SonarCloud report generator combines API calls, data aggregation, HTML generation, and file writing that should be modular.","recommendation":"Split into modules: sonarcloud-client.js (API integration), data-aggregator.js (aggregate issues by type/severity), html-generator.js (create HTML report), markdown-generator.js (create MD report), and main orchestrator (generate-detailed-sonar-report.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/lib/ai-pattern-checks.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/lib/ai-pattern-checks.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/lib/ai-pattern-checks.js","line":1,"title":"Docs: Excessively long file lib/ai-pattern-checks.js (554 lines)","description":"File is 554 lines (1.1x recommended limit). AI pattern library combines pattern loading, matching, reporting, and multiple specific pattern checkers that should be separated.","recommendation":"Split into modules: pattern-loader.js (load ai-patterns.json), pattern-matcher.js (core matching engine), pattern-checks/ directory with specific checkers (happy-path.js, trivial-assertions.js, todo-markers.js, etc.), and main exports (ai-pattern-checks.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/migrate-existing-findings.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/migrate-existing-findings.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/migrate-existing-findings.js","line":1,"title":"Docs: Excessively long file migrate-existing-findings.js (541 lines)","description":"File is 541 lines (1.1x recommended limit). Migration script combines multiple source parsing, schema transformation, deduplication, and output that should be separated.","recommendation":"Split into modules: source-parsers.js (parse old formats), schema-transformer.js (old to new schema), deduplicator.js (detect duplicates across sources), output-writer.js (write migrated JSONL), and main orchestrator (migrate-existing-findings.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-content-accuracy.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/check-content-accuracy.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-content-accuracy.js","line":1,"title":"Docs: Excessively long file check-content-accuracy.js (516 lines)","description":"File is 516 lines (1.0x recommended limit). Content accuracy checker combines reference validation, fact checking, staleness detection, and reporting that should be modular.","recommendation":"Split into modules: reference-validator.js (check cross-references), fact-checker.js (validate claims), staleness-detector.js (identify outdated content), accuracy-scorer.js (compute accuracy metrics), report-generator.js, and main orchestrator (check-content-accuracy.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/sync-claude-settings.js::excessive-length","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6c-documentation.jsonl","original_id":"process::scripts/sync-claude-settings.js::excessive-length","category":"process","severity":"S3","type":"process-gap","file":"scripts/sync-claude-settings.js","line":1,"title":"Docs: Excessively long file sync-claude-settings.js (501 lines)","description":"File is 501 lines (exactly at limit). Settings sync combines parsing, validation, merging, and persistence that should be separated for better maintainability.","recommendation":"Split into modules: settings-parser.js (parse .claude/ settings), settings-validator.js (validate schema), settings-merger.js (merge strategies), settings-writer.js (atomic writes), and main orchestrator (sync-claude-settings.js).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::consolidate-flagged-scripts","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::consolidate-flagged-scripts","category":"process","severity":"S3","type":"process-gap","file":"package.json","line":1,"title":"Improve: Consolidate duplicate script patterns into single script with flags","description":"8+ script pairs use pattern 'script:action' + 'script:action-variant' calling same script with different flags -> Single consolidated script reduces maintenance and testing surface","recommendation":"Replace script pairs (learning:analyze/dashboard/detailed, patterns:check/check-all, security:check/check-all, session:gaps/gaps:fix, override:log/list, agents:check/check-strict) with single scripts that accept flags via npm -- syntax (e.g., 'npm run learning -- --dashboard')","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::hook-ci-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::hook-ci-duplication","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-commit","line":1,"title":"Improve: Reduce pre-commit hook check duplication with CI","description":"Pattern compliance runs 3x (pre-commit, pre-push, CI), type checking runs 2x (pre-push, CI), tests run 2x (pre-commit conditional, CI) -> Wastes developer time and CI resources","recommendation":"Move expensive checks (type checking, full test suite) exclusively to CI. Keep only fast checks (<5s each) in pre-commit: ESLint, lint-staged, pattern compliance on staged files only. Add --staged flag to pattern compliance script for faster pre-commit checks.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::hook-output-inefficiency","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::hook-output-inefficiency","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-commit","line":9,"title":"Improve: Eliminate redundant npm run then re-run pattern in hooks","description":"Multiple hooks run 'npm run cmd > /dev/null 2>&1' to check exit code, then re-run 'npm run cmd 2>&1 | tail' to show output on failure -> Doubles execution time for failing checks","recommendation":"Capture output to temp file once: 'npm run cmd > $tmpfile 2>&1; code=$?; if [ $code -ne 0 ]; then tail $tmpfile; exit 1; fi'. Already used correctly for tests (line 56-66).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::ci-cache-test-build","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::ci-cache-test-build","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/ci.yml","line":114,"title":"Improve: Add CI caching for test build artifacts","description":"test:build compiles TypeScript to dist-tests/ on every CI run, taking 20-30s -> No caching means repeated compilation","recommendation":"Add GitHub Actions cache for dist-tests/ directory keyed on hash of src/tests/ and tsconfig.test.json. Skip test:build if cache hit and source unchanged.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::redundant-tsc-invocations","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::redundant-tsc-invocations","category":"process","severity":"S3","type":"process-gap","file":"package.json","line":11,"title":"Improve: Combine test:build and type check into single tsc invocation","description":"CI runs 'npm run test:build' (tsc -p tsconfig.test.json) AND 'tsc --noEmit' separately -> Redundant TypeScript compilation (40-60s total)","recommendation":"Refactor test compilation to use 'tsc --noEmit' for type checking, then use esbuild or tsx for faster test execution. OR ensure test:build also validates non-test types and remove separate type check step.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::ci-parallelization","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::ci-parallelization","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/ci.yml","line":1,"title":"Improve: Parallelize independent CI jobs","description":"lint-typecheck-test job runs 15+ steps sequentially, many are independent (lint, format:check, deps:circular, deps:unused) -> Sequential execution adds 2-3 minutes","recommendation":"Split CI into parallel jobs: 1) Lint & Format (eslint, prettier, markdown) 2) Dependencies (circular, unused) 3) Type Check & Test 4) Build. Use needs: to sequence only what's required.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::scheduled-npm-audit","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::scheduled-npm-audit","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-push","line":92,"title":"Improve: Make npm audit scheduled instead of on every push","description":"npm audit runs on every git push (non-blocking, 3-8s) checking for vulnerabilities -> Slows down push, security issues rarely change between pushes","recommendation":"Remove npm audit from pre-push hook. Add scheduled workflow (daily/weekly) to run npm audit and create GitHub issue if high/critical vulnerabilities found. Faster feedback loop for developers.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::auto-update-doc-index","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::auto-update-doc-index","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-commit","line":134,"title":"Improve: Auto-update DOCUMENTATION_INDEX.md in pre-commit hook","description":"Pre-commit hook BLOCKS if .md files changed but DOCUMENTATION_INDEX.md not updated, requiring manual 'npm run docs:index && git add' -> Friction in commit workflow","recommendation":"Auto-run 'npm run docs:index' and auto-stage DOCUMENTATION_INDEX.md when .md files are in commit. Show diff and ask for confirmation, or make it automatic with override flag SKIP_DOC_INDEX_AUTO=1.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::consolidate-docs-workflow","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::consolidate-docs-workflow","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/docs-lint.yml","line":1,"title":"Improve: Combine docs-lint.yml checks into main CI workflow","description":"docs-lint.yml and ci.yml both trigger on PRs touching docs, both run documentation checks -> Duplicate workflow runs and maintenance burden","recommendation":"Move docs-lint functionality into ci.yml as a separate job that runs conditionally (if: contains(changed-files, '*.md')). Reduces workflows from 10 to 9, single place for doc linting logic.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::task-runner-migration","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::task-runner-migration","category":"process","severity":"S3","type":"process-gap","file":"package.json","line":1,"title":"Improve: Use task runner (turbo/nx) for script orchestration","description":"70+ npm scripts with complex dependencies, no dependency caching, sequential execution in hooks/CI -> Slow execution, hard to maintain, no incremental builds","recommendation":"Introduce turbo or nx for: 1) Dependency graph (test depends on test:build) 2) Caching (hash-based) 3) Parallel execution 4) Incremental rebuilds. Start with test pipeline, expand to lint/build.","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::firebase-deploy-simplification","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::firebase-deploy-simplification","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/deploy-firebase.yml","line":73,"title":"Improve: Simplify Firebase deployment workflow","description":"Deploy workflow has 3 sequential deployment steps (functions, rules, hosting) that could run in parallel -> Adds 2-3 minutes to deployment time","recommendation":"Use 'firebase deploy --only functions,firestore:rules,hosting' single command OR parallelize as separate jobs with proper sequencing. Also remove deprecated function deletion step (line 131-138) which is continue-on-error anyway.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::backlog-stale-reference","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::backlog-stale-reference","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/backlog-enforcement.yml","line":32,"title":"Improve: Backlog enforcement workflow references archived file","description":"Workflow checks AUDIT_FINDINGS_BACKLOG.md which was archived in TDMS Phase 2 -> Workflow always exits early, provides no value, maintenance burden","recommendation":"Either: 1) Update workflow to check docs/technical-debt/MASTER_DEBT.jsonl for backlog health (count open items, S0/S1 thresholds) OR 2) Archive/remove workflow if backlog-health is checked elsewhere.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::hook-timing-visibility","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::hook-timing-visibility","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-commit","line":1,"title":"Improve: Add hook timing instrumentation","description":"Pre-commit has 13 check steps (280 lines), pre-push has 7 steps (155 lines), but no visibility into which steps are slow -> Can't identify optimization opportunities","recommendation":"Add timing instrumentation: 'START=$(date +%s); ... ; echo \"  â±ï¸  Took $(($(date +%s) - START))s\"' for each major step. OR use time command. Log slow steps (>3s) to .git/hooks/timing.log for analysis.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::consolidate-security-checks","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::consolidate-security-checks","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/backlog-enforcement.yml","line":103,"title":"Improve: Consolidate security checking into single workflow","description":"Security patterns checked in pre-push hook (files being pushed) AND backlog-enforcement workflow (all files or PR files) -> Duplication and potential inconsistency","recommendation":"Remove security-patterns job from backlog-enforcement.yml. Keep security check in pre-push hook only. Add scheduled workflow (weekly) for full-repo security audit with GitHub Security tab integration.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::expand-lint-staged","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::expand-lint-staged","category":"process","severity":"S3","type":"process-gap","file":"package.json","line":78,"title":"Improve: Use lint-staged for more than just formatting","description":"lint-staged only runs prettier (formatting) on staged files -> ESLint, pattern compliance, and other checks run on ALL files even if not staged","recommendation":"Expand lint-staged config: '*.{ts,tsx,js,jsx}': ['eslint --fix', 'prettier --write'], '*.md': ['markdownlint --fix', 'prettier --write']. Moves ESLint to lint-staged for automatic fixes and faster execution (staged files only).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::add-commit-msg-hook","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::add-commit-msg-hook","category":"process","severity":"S3","type":"process-gap","file":".husky/_/commit-msg","line":1,"title":"Improve: Add commit message validation hook","description":"No commit message validation enforced -> Inconsistent commit messages make changelog generation and git log navigation harder","recommendation":"Add .husky/commit-msg hook to validate conventional commits format (feat:, fix:, docs:, chore:, etc). Use commitlint or simple regex. Block commits with bad format or provide helpful error message.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::workflow-caching-strategy","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::workflow-caching-strategy","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/ci.yml","line":22,"title":"Improve: Add GitHub Actions workflow caching strategy","description":"All workflows use 'cache: npm' which only caches npm packages, not build artifacts or script outputs -> Miss opportunity for faster CI runs","recommendation":"Add composite caching: 1) npm packages (already cached) 2) Next.js .next/ cache 3) TypeScript build cache 4) test:build dist-tests/ output. Use cache-dependency-path for all package-lock.json files.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::automate-doc-maintenance","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::automate-doc-maintenance","category":"process","severity":"S3","type":"process-gap","file":"package.json","line":14,"title":"Improve: Migrate manual documentation tasks to automated triggers","description":"3 documentation maintenance scripts require manual invocation: docs:update-readme, docs:archive, docs:index -> Leads to stale documentation","recommendation":"1) docs:index - already has pre-commit check, make it auto-run and stage 2) docs:update-readme - add to sync-readme.yml workflow trigger 3) docs:archive - add scheduled workflow (monthly) to identify docs that should be archived (no updates in 6+ months, marked obsolete)","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::stale-branch-cleanup","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::stale-branch-cleanup","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/*","line":1,"title":"Improve: Add workflow for stale branch cleanup","description":"No automated branch cleanup -> Old feature branches accumulate, clutter repository, cause confusion","recommendation":"Add scheduled workflow using actions/stale to: 1) Label branches with no commits in 30 days as 'stale' 2) Delete branches with no commits in 60 days (excluding main, develop, release/*) 3) Post comment on associated PR before deletion","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::automation::simplify-trigger-checks","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6b-improvements.jsonl","original_id":"process::automation::simplify-trigger-checks","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/ci.yml","line":42,"title":"Improve: Replace manual trigger checks with GitHub Actions expressions","description":"Workflows use tj-actions/changed-files then bash scripts to check patterns -> Extra action dependency, slower execution, more complex logic","recommendation":"Use GitHub Actions native path filters and expressions: 'if: contains(github.event.head_commit.modified, '.md')' or combine with paths: filter in workflow trigger. Reduces external dependencies.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::N/A::shell-script-no-lint","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6a-coverage-gaps.jsonl","original_id":"process::N/A::shell-script-no-lint","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/analyze-user-request.sh","line":1,"title":"Gap: Shell scripts not linted or validated","description":"6 shell scripts in .claude/hooks/ (and additional ones in skills/) can be committed with syntax errors, runtime bugs, or security issues. ShellCheck would catch common mistakes like unquoted variables, incorrect conditionals, and unsafe patterns. Shell scripts run at critical points (session start, pre-commit checks) so bugs can break development workflow.","recommendation":"Add ShellCheck validation: 1) Install shellcheck as devDependency, 2) Add npm script 'shellcheck:check' that runs on .claude/hooks/*.sh and .claude/skills/**/*.sh, 3) Add to pre-commit hook before other checks, 4) Add to CI workflow as blocking step, 5) Create .shellcheckrc to configure rules.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::N/A::mjs-config-no-lint","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6a-coverage-gaps.jsonl","original_id":"process::N/A::mjs-config-no-lint","category":"process","severity":"S3","type":"process-gap","file":"eslint.config.mjs","line":25,"title":"Gap: Config .mjs files excluded from linting","description":"eslint.config.mjs explicitly excludes '*.config.mjs' from linting. These are critical configuration files (Next.js, PostCSS, ESLint itself) that affect build and development. Syntax errors or security issues in these files can break builds or introduce vulnerabilities. Currently they can be committed without any validation.","recommendation":"Remove '*.config.mjs' from ignores array in eslint.config.mjs. If specific rules need to be relaxed for config files, create a separate configuration block with adjusted rules (e.g., allow 'export default' without explicit types). Verify all config files pass linting before making change blocking.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::N/A::scripts-no-tests","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6a-coverage-gaps.jsonl","original_id":"process::N/A::scripts-no-tests","category":"process","severity":"S2","type":"process-gap","file":"scripts/","line":1,"title":"Gap: Scripts directory missing test coverage","description":"89 JavaScript files in scripts/ directory but only 5 have test coverage (check-docs-light, phase-complete-check, surface-lessons-learned, update-readme-status, validate-audit-s0s1). These scripts handle critical automation: debt management, audit validation, security checks, hook health, document sync. Bugs in these scripts can corrupt data, break CI, or cause incorrect validation results. Scripts like validate-audit.js, security-check.js, and debt/validate-schema.js are used as blocking gates in pre-commit/pre-push.","recommendation":"Prioritize test coverage for blocking scripts: 1) Start with security-check.js (blocks pre-push), 2) Add tests for debt/validate-schema.js (blocks commits), 3) Cover validate-audit.js (blocks S0/S1), 4) Add tests for check-pattern-compliance.js (blocks commits), 5) Expand to other critical scripts. Create tests/scripts/__helpers__ for common test utilities. Aim for 60%+ coverage of blocking scripts, 30%+ for others.","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::N/A::functions-no-integration-tests","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6a-coverage-gaps.jsonl","original_id":"process::N/A::functions-no-integration-tests","category":"process","severity":"S2","type":"process-gap","file":"functions/src/admin.ts","line":1,"title":"Gap: Firebase functions lack integration tests","description":"8 TypeScript files in functions/src/ contain Cloud Functions (admin operations, recaptcha verification, scheduled jobs, security wrappers) but no integration tests exist in functions/ directory. While unit tests exist for the main app, Firebase functions interact with Firestore, authentication, and external APIs. Integration tests would catch: incorrect Firestore rules interactions, auth token validation issues, rate limiting failures, scheduled job execution problems. Functions are deployed to production and handle sensitive operations.","recommendation":"Set up Firebase Functions integration test framework: 1) Install firebase-functions-test (already in devDeps), 2) Create functions/test/ directory, 3) Add test files for each function module (admin.test.ts, recaptcha-verify.test.ts, jobs.test.ts), 4) Use Firebase emulators for Firestore/Auth, 5) Add 'test' script to functions/package.json, 6) Run function tests in CI after main tests, 7) Document test setup in functions/README.md.","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::N/A::skills-no-usage-docs","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6a-coverage-gaps.jsonl","original_id":"process::N/A::skills-no-usage-docs","category":"process","severity":"S3","type":"process-gap","file":".claude/skills/","line":1,"title":"Gap: Skills missing usage documentation","description":"56 skills exist but 0 have USAGE.md documentation, only 1 has README.md. SKILL_INDEX.md shows skills organized by category (Audit & Code Quality, Session Management, Development Roles, etc.) but individual skills lack: usage examples, parameter documentation, expected outputs, common use cases, troubleshooting tips. This makes skills harder to use correctly and increases likelihood of misuse. New team members or AI agents using these skills lack guidance.","recommendation":"Create standardized skill documentation template: 1) Add USAGE.md template to skill-creator skill, 2) Document top 10 most-used skills first (check MCP logs for frequency), 3) Include sections: Synopsis, Parameters, Examples, Expected Output, Common Issues, Related Skills, 4) Add skills:check-docs npm script to validate USAGE.md exists and has required sections, 5) Add to pre-commit check when skill files are modified, 6) Generate missing USAGE.md files in batch using doc-optimizer skill.","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::N/A::yaml-no-lint","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6a-coverage-gaps.jsonl","original_id":"process::N/A::yaml-no-lint","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/ci.yml","line":1,"title":"Gap: YAML workflow files not linted","description":"10 GitHub workflow YAML files exist (.github/workflows/*.yml) but no YAML linting is configured. Workflow files control CI/CD, deployments, security checks, and automation. YAML syntax errors break CI/CD pipelines. Invalid workflow syntax might not be caught until push, wasting time. Indentation errors, incorrect anchors, or invalid keys can cause silent failures or unexpected behavior.","recommendation":"Add YAML linting: 1) Install yamllint as devDependency, 2) Create .yamllint.yml config (set line-length to 120, indent to 2, allow comments), 3) Add npm script 'yaml:lint' that checks .github/workflows/*.yml and .serena/project.yml, 4) Add to pre-commit hook (non-blocking warning first), 5) Add to CI as blocking step, 6) Fix any existing issues before making blocking.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::N/A::env-no-validation","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6a-coverage-gaps.jsonl","original_id":"process::N/A::env-no-validation","category":"process","severity":"S2","type":"process-gap","file":".env.local.example","line":1,"title":"Gap: Environment files not validated","description":"Multiple .env files exist (.env.local.example, functions/.env.local.example, .env.production) but no validation of required variables or format. Missing required env vars cause runtime errors. Incorrect env var formats (URLs without protocols, invalid API keys) fail late. Example files can become stale and missing new required variables. No check that actual .env files match the example structure.","recommendation":"Create environment validation: 1) Add scripts/validate-env.js that reads .env.local.example and checks actual .env files have required keys, 2) Validate format (URLs, numeric values, required prefixes like NEXT_PUBLIC_), 3) Add npm script 'env:validate', 4) Run in pre-push as non-blocking warning (can't block since .env is gitignored), 5) Add to CI for production builds, 6) Check functions/.env separately with functions-specific requirements, 7) Document required env vars in README.md.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::N/A::functions-no-type-check-pre-push","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6a-coverage-gaps.jsonl","original_id":"process::N/A::functions-no-type-check-pre-push","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-push","line":82,"title":"Gap: Firebase functions TypeScript not type-checked in pre-push","description":"Pre-push hook (line 82-90) runs 'npx tsc --noEmit' for type checking but only for the main project, not for functions/. Firebase functions have their own TypeScript config (functions/tsconfig.json) and can have type errors that slip through. Type errors in functions are only caught during 'npm run build' in functions/ which might not be run before push. CI doesn't explicitly type-check functions directory either (only runs functions build during deploy workflow).","recommendation":"Add functions type check to pre-push: 1) After main type check in .husky/pre-push (line 90), add functions type check, 2) Run 'cd functions && npx tsc --noEmit' (or use absolute path), 3) Show appropriate error message if functions type check fails, 4) Consider adding to CI workflow as explicit step before build, 5) Ensure functions TypeScript errors are visible and block push.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::N/A::ci-no-shell-syntax-check","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6a-coverage-gaps.jsonl","original_id":"process::N/A::ci-no-shell-syntax-check","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/ci.yml","line":1,"title":"Gap: No syntax validation for committed shell scripts in CI","description":"CI workflow checks many things (ESLint, TypeScript, tests, patterns, security) but doesn't validate shell script syntax. Pre-commit hook is itself a shell script (.husky/pre-commit) - if it has syntax errors, commits can become blocked or validation can silently fail. .claude/hooks/ contains 6 critical shell scripts that run during development. A shell syntax error could break session-start.sh or pattern-check.sh, disrupting development flow. While these might work on developer's machine, different shell versions or environments could expose issues.","recommendation":"Add shell script validation to CI: 1) Install ShellCheck in CI environment (add to CI job steps), 2) Add step after 'Checkout code' named 'Validate shell scripts', 3) Run 'shellcheck .husky/pre-commit .husky/pre-push .claude/hooks/*.sh .claude/skills/**/*.sh', 4) Make blocking (don't use continue-on-error), 5) Add corresponding pre-commit check so issues are caught earlier, 6) Document shell script standards in CONTRIBUTING.md.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::N/A::new-files-coverage-check","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-6a-coverage-gaps.jsonl","original_id":"process::N/A::new-files-coverage-check","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-commit","line":1,"title":"Gap: No validation that new files are covered by appropriate checks","description":"When new file types are added to the project (e.g., .proto files, .graphql, .tf for Terraform), there's no check that they're covered by linting, validation, or security checks. Pre-commit checks specific file types (.md for doc index, .jsonl for debt, .ts/.js for ESLint) but doesn't ensure NEW file types have appropriate validation. Could add Terraform files without terraform validate, GraphQL without schema validation, Protocol Buffers without protolint, etc. Gap would only be noticed during PR review or when issues occur.","recommendation":"Create new-file-type detection: 1) Add scripts/check-file-coverage.js that detects file extensions in repo, 2) Maintain config/known-file-types.json mapping extensions to their validators (e.g., .ts->ESLint+TypeScript, .sh->ShellCheck, .yml->yamllint), 3) Check if any committed files have extensions not in known list, 4) Warning in pre-commit (non-blocking), 5) Add 'coverage:files' npm script for manual checking, 6) Run in CI as informational (continue-on-error: true), 7) Prompt to add validation for new file types.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::module-system-mix","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5c-consistency.jsonl","original_id":"process::scripts::module-system-mix","category":"process","severity":"S3","type":"process-gap","file":"scripts/append-hook-warning.js","line":1,"title":"Inconsistent: Mixed CommonJS and ESM module systems","description":"Mixing CommonJS (require) and ESM (import) makes codebase harder to maintain and can cause confusion about module resolution. Standardizing on one approach improves consistency and reduces cognitive load.","recommendation":"Standardize on ESM (import/export) for all new scripts. Create migration plan for remaining CommonJS scripts. ESM is the modern standard and provides better static analysis.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::node-prefix-inconsistency","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5c-consistency.jsonl","original_id":"process::scripts::node-prefix-inconsistency","category":"process","severity":"S3","type":"process-gap","file":"scripts/validate-audit.js","line":18,"title":"Inconsistent: node: prefix usage in imports","description":"Some scripts use 'node:fs' prefix for built-in modules, others use 'fs', and some use aliases like 'node_fs'. This inconsistency makes code reviews harder and creates unnecessary variation in import style.","recommendation":"Standardize on using 'node:' prefix for all Node.js built-in modules (node:fs, node:path, etc.) without aliases. This is the modern Node.js convention and makes built-in modules explicit.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::emoji-consistency","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5c-consistency.jsonl","original_id":"process::scripts::emoji-consistency","category":"process","severity":"S3","type":"process-gap","file":"scripts/seed-commit-log.js","line":105,"title":"Inconsistent: Emoji usage in console output","description":"Most scripts use emojis for visual feedback (âœ…, âŒ, âš ï¸) but some don't, creating inconsistent UX across automation tools. Users expect consistent output formatting.","recommendation":"Establish emoji usage standard: use emojis for all user-facing scripts (âœ… success, âŒ error, âš ï¸ warning, ðŸ“Š info). Create shared constants file for emoji mappings to ensure consistency.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::exit-code-patterns","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5c-consistency.jsonl","original_id":"process::scripts::exit-code-patterns","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-agent-compliance.js","line":192,"title":"Inconsistent: Exit code handling patterns","description":"Scripts use different patterns for setting exit codes: some use process.exit() directly, others use process.exitCode assignment. This inconsistency makes error handling patterns harder to learn and maintain.","recommendation":"Standardize on process.exitCode assignment pattern (not direct process.exit()) to allow cleanup handlers to run. Reserve process.exit() for truly fatal errors only. Document pattern in CONTRIBUTING.md.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::verbose-logging","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5c-consistency.jsonl","original_id":"process::scripts::verbose-logging","category":"process","severity":"S3","type":"process-gap","file":"scripts/archive-doc.js","line":72,"title":"Inconsistent: Verbose/debug logging approaches","description":"Some scripts have verbose() helper functions for debug output, others don't, leading to inconsistent debug capabilities. Makes troubleshooting harder when scripts don't expose internal state consistently.","recommendation":"Create shared logging utility (scripts/lib/logger.js) with consistent verbose/debug/info/warn/error methods. All scripts should import and use this utility for standardized output levels.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::error-message-format","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5c-consistency.jsonl","original_id":"process::scripts::error-message-format","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-agent-compliance.js","line":163,"title":"Inconsistent: Error message formatting","description":"Error messages use different formats: some use boxed separators (===, ---), some use simple prefixes. This creates inconsistent UX and makes it harder to parse errors programmatically.","recommendation":"Standardize error message format: [SCRIPT_NAME] LEVEL: message. Use consistent separator styles (=== for major sections, --- for subsections). Create error formatting utility function.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::arg-parsing","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5c-consistency.jsonl","original_id":"process::scripts::arg-parsing","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-agent-compliance.js","line":29,"title":"Inconsistent: Command line argument parsing","description":"Scripts parse command line arguments differently: some use simple includes() checks, others have custom parseArgs() functions, creating inconsistent CLI interfaces and duplicated parsing logic.","recommendation":"Adopt a standard CLI parsing library (e.g., commander.js or yargs) for all scripts with complex arguments. For simple flags, use consistent pattern. Create shared argument parsing utilities.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::path-validation","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5c-consistency.jsonl","original_id":"process::scripts::path-validation","category":"process","severity":"S2","type":"process-gap","file":"scripts/archive-doc.js","line":83,"title":"Inconsistent: File path validation and sanitization","description":"Path validation and sanitization logic is duplicated across scripts with slightly different implementations, risking security vulnerabilities if one implementation is weaker. Validation should be centralized.","recommendation":"Create shared path validation utilities in scripts/lib/validate-paths.js. Implement validatePathWithinRepo(), sanitizePath(), isSafeFilePath() once and reuse. Add comprehensive tests.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::error-sanitization","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5c-consistency.jsonl","original_id":"process::scripts::error-sanitization","category":"process","severity":"S2","type":"process-gap","file":"scripts/archive-doc.js","line":41,"title":"Inconsistent: Error sanitization approaches","description":"Some scripts import sanitizeError utility, others inline error sanitization, and some don't sanitize at all. This creates security risk of exposing sensitive paths or tokens in error messages.","recommendation":"Ensure all scripts use shared sanitizeError() utility from scripts/lib/sanitize-error.js. Add ESLint rule to catch raw error.message usage. Document sanitization requirement in CONTRIBUTING.md.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/ai-review.js::toctou-existsSync-readFileSync","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::scripts/ai-review.js::toctou-existsSync-readFileSync","category":"process","severity":"S1","type":"process-gap","file":"scripts/ai-review.js","line":139,"title":"Quality: TOCTOU race condition in ai-review.js","description":"Classic TOCTOU (Time-of-Check-Time-of-Use) race condition. File existence is checked with existsSync() then read with readFileSync(). Between these calls, the file could be deleted, moved, or replaced with a symlink, causing crashes or reading wrong files. CODE_PATTERNS.md Rule #3 explicitly requires wrapping ALL file reads in try/catch.","recommendation":"Remove existsSync check and wrap readFileSync in try/catch. Pattern from CODE_PATTERNS.md: try { const content = readFileSync(filePath, 'utf-8'); return { success: true, content }; } catch (error) { if (error.code === 'ENOENT') { return { success: false, error: 'File not found' }; } ... }","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/ai-review.js:139"},{"type":"description","detail":"Classic TOCTOU (Time-of-Check-Time-of-Use) race condition. File existence is checked with existsSync() then read with readFileSync(). Between these calls, the file could be deleted, moved, or replaced with a symlink, causing crashes or reading wrong files. CODE_PATTERNS.md Rule #3 explicitly requires wrapping ALL file reads in try/catch."}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-consolidation-status.js::toctou-existsSync-readFileSync","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::scripts/check-consolidation-status.js::toctou-existsSync-readFileSync","category":"process","severity":"S1","type":"process-gap","file":"scripts/check-consolidation-status.js","line":89,"title":"Quality: TOCTOU race condition in check-consolidation-status.js","description":"Classic TOCTOU race condition. existsSync at line 89 followed by readFileSync at line 96. Violates CODE_PATTERNS.md Critical Pattern #3: 'Wrap ALL file reads in try/catch - existsSync has race conditions'. Between the check and read, file could be deleted causing crash.","recommendation":"Remove existsSync check at line 89. Change error handling to: try { const content = readFileSync(LOG_FILE, 'utf8'); ... } catch (err) { if (err.code === 'ENOENT') { console.error('File not found'); process.exitCode = 2; return; } throw err; }","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/check-consolidation-status.js:89"},{"type":"description","detail":"Classic TOCTOU race condition. existsSync at line 89 followed by readFileSync at line 96. Violates CODE_PATTERNS.md Critical Pattern #3: 'Wrap ALL file reads in try/catch - existsSync has race conditions'. Between the check and read, file could be deleted causing crash."}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/debt/resolve-item.js::toctou-existsSync-readFileSync","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::scripts/debt/resolve-item.js::toctou-existsSync-readFileSync","category":"process","severity":"S1","type":"process-gap","file":"scripts/debt/resolve-item.js","line":53,"title":"Quality: TOCTOU race condition in resolve-item.js","description":"TOCTOU race in loadMasterDebt(). existsSync at line 53 followed by readFileSync at line 56. File could be deleted between check and read. Violates CODE_PATTERNS.md Critical Pattern #3. In a concurrent environment (multiple debt resolution scripts), this can cause crashes.","recommendation":"Replace lines 52-59 with: function loadMasterDebt() { try { const content = fs.readFileSync(MASTER_FILE, 'utf8'); const lines = content.split('\\n').filter(line => line.trim()); return lines.map(line => JSON.parse(line)); } catch (err) { if (err.code === 'ENOENT') return []; throw err; } }","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/debt/resolve-item.js:53"},{"type":"description","detail":"TOCTOU race in loadMasterDebt(). existsSync at line 53 followed by readFileSync at line 56. File could be deleted between check and read. Violates CODE_PATTERNS.md Critical Pattern #3. In a concurrent environment (multiple debt resolution scripts), this can cause crashes."}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/stop-serena-dashboard.js::magic-port-24282","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::.claude/hooks/stop-serena-dashboard.js::magic-port-24282","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/stop-serena-dashboard.js","line":30,"title":"Quality: Magic number - hardcoded port without explanation","description":"Port 24282 is hardcoded without any comment explaining why this specific port. Makes it unclear if this is a well-known port, randomly chosen, or has significance. CODE_PATTERNS.md warns against magic numbers/strings without explanation. If port needs to change, developers won't know the constraints.","recommendation":"Add explanatory comment: // Port 24282: Official Serena MCP server port (assigned in .mcp.json). Or better: load from config file to have single source of truth matching .mcp.json configuration.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/analyze-user-request.js::magic-maxlength-2000","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::.claude/hooks/analyze-user-request.js::magic-maxlength-2000","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/analyze-user-request.js","line":37,"title":"Quality: Magic number - MAX_LENGTH without explanation","description":"MAX_LENGTH=2000 hardcoded without explanation of why 2000 characters. Is this to prevent DoS? Buffer overflow? UI limitation? Without context, future maintainers won't know if this can be safely adjusted.","recommendation":"Add comment explaining rationale: // DoS prevention: Limit input to 2000 chars (typical user prompt is <500 chars). Consider making this configurable if different contexts need different limits.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/large-context-warning.js::magic-line-limit-5000","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::.claude/hooks/large-context-warning.js::magic-line-limit-5000","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/large-context-warning.js","line":20,"title":"Quality: Magic number - SINGLE_FILE_LINE_LIMIT without explanation","description":"SINGLE_FILE_LINE_LIMIT=5000 without explanation. Is this based on Claude token limits? Performance testing? Arbitrary choice? Future Claude model upgrades may allow larger contexts, but without documentation, developers won't know if this is safe to change.","recommendation":"Add comment: // Claude 3 context window = ~200K tokens. 5000 lines â‰ˆ 50K tokens average, leaving headroom for conversation. Based on [reference to testing/decision doc if exists]. Consider loading from config for different model tiers.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-consolidation-status.js::magic-archive-threshold-2500","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::scripts/check-consolidation-status.js::magic-archive-threshold-2500","category":"process","severity":"S2","type":"process-gap","file":"scripts/check-consolidation-status.js","line":26,"title":"Quality: Magic number - ARCHIVE_LINE_THRESHOLD without explanation","description":"ARCHIVE_LINE_THRESHOLD=2500 has no explanation. Why 2500 lines? Performance issue? File size limit? Readability concern? This threshold triggers archival decisions but lacks documentation on how it was determined.","recommendation":"Add comment: // Archive threshold: 2500 lines keeps file manageable for editors (most IDEs struggle >3000 lines). Based on AI_REVIEW_LEARNINGS_LOG.md performance testing [if exists]. Consider making configurable.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/mcp/sonarcloud-server.js::magic-timeout-30000","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::scripts/mcp/sonarcloud-server.js::magic-timeout-30000","category":"process","severity":"S2","type":"process-gap","file":"scripts/mcp/sonarcloud-server.js","line":68,"title":"Quality: Magic number - REQUEST_TIMEOUT_MS without explanation","description":"REQUEST_TIMEOUT_MS=30000 (30 seconds) hardcoded without explanation. Is this based on SonarCloud API SLA? Network timeout? If SonarCloud response times change or users have slow connections, they won't know if adjusting this is safe.","recommendation":"Add comment: // 30s timeout: SonarCloud API p95 response time ~5s, p99 ~15s (as of 2025-01). 30s provides 2x buffer for slow connections. Reference: [SonarCloud API docs]. Consider making configurable for different network conditions.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/state-utils.js::hardcoded-state-dir","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::.claude/hooks/state-utils.js::hardcoded-state-dir","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/state-utils.js","line":22,"title":"Quality: Hardcoded path should be configurable","description":"STATE_DIR = '.claude/state' is hardcoded. In different project structures or CI environments, state might need to be stored elsewhere (temp dirs, mounted volumes). Makes the utility less reusable across different setups.","recommendation":"Make configurable: const STATE_DIR = process.env.CLAUDE_STATE_DIR || '.claude/state'; Document the environment variable in docs/agent_docs/ or CONTRIBUTING.md. This allows override without code changes.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/debt/intake-audit.js::hardcoded-debt-paths","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::scripts/debt/intake-audit.js::hardcoded-debt-paths","category":"process","severity":"S2","type":"process-gap","file":"scripts/debt/intake-audit.js","line":53,"title":"Quality: Multiple hardcoded debt paths","description":"DEBT_DIR, MASTER_FILE, LOG_DIR, LOG_FILE all hardcoded to docs/technical-debt. In CI/CD or different repo structures, technical debt data might need different locations. Testing also becomes harder without configurability.","recommendation":"Load from config: const config = loadConfig('debt-paths'); const DEBT_DIR = config.debtDir || path.join(__dirname, '../../docs/technical-debt'); Or use environment variables with fallbacks. Document configuration in docs/agent_docs/.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/debt/resolve-item.js::execsync-string-interpolation","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::scripts/debt/resolve-item.js::execsync-string-interpolation","category":"process","severity":"S0","type":"process-gap","file":"scripts/debt/resolve-item.js","line":21,"title":"Security: Potential command injection in resolve-item.js execSync","description":"execSync imported from child_process at line 21. While not directly visible in first 100 lines, use of execSync with string concatenation is a critical security issue per CODE_PATTERNS.md. Need to verify later in file that execFileSync is used or execSync uses only array arguments.","recommendation":"If execSync is used with string templates: replace with execFileSync(cmd, [arg1, arg2], options). CODE_PATTERNS.md Security pattern: 'Use execFileSync(cmd, [arg1, arg2]) not execSync(`cmd ${var}`)' eliminates injection vectors even with validated inputs.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/debt/resolve-item.js:21"},{"type":"description","detail":"execSync imported from child_process at line 21. While not directly visible in first 100 lines, use of execSync with string concatenation is a critical security issue per CODE_PATTERNS.md. Need to verify later in file that execFileSync is used or execSync uses only array arguments."}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/debt/intake-audit.js::json-parse-validation","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::scripts/debt/intake-audit.js::json-parse-validation","category":"process","severity":"S1","type":"process-gap","file":"scripts/debt/intake-audit.js","line":119,"title":"Quality: Missing validation on parsed JSON objects","description":"mapDocStandardsToTdms() function processes untrusted JSONL input. While safeCloneObject filters __proto__, the function doesn't validate object shape before accessing properties. Malformed JSONL could cause undefined behavior or crashes when accessing nested properties.","recommendation":"Add input validation: function mapDocStandardsToTdms(item) { if (!item || typeof item !== 'object' || Array.isArray(item)) { return { item: {}, metadata: { format_detected: 'invalid', error: 'Expected object' }}; } ... } Also validate array types before .map(), .length access.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/debt/intake-audit.js:119"},{"type":"description","detail":"mapDocStandardsToTdms() function processes untrusted JSONL input. While safeCloneObject filters __proto__, the function doesn't validate object shape before accessing properties. Malformed JSONL could cause undefined behavior or crashes when accessing nested properties."}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-pattern-compliance.js::redos-risk","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5b-code-quality.jsonl","original_id":"process::scripts/check-pattern-compliance.js::redos-risk","category":"process","severity":"S1","type":"process-gap","file":"scripts/check-pattern-compliance.js","line":106,"title":"Quality: Unsafe regex patterns in pattern checker","description":"ANTI_PATTERNS array contains many complex regex with nested quantifiers and unbounded lookaheads (e.g., line 137: /for\\s+\\w+\\s+in\\s+1\\s+2\\s+3\\s*;\\s*do[\\s\\S]{0,120}?&&\\s*break[\\s\\S]{0,80}?done(?![\\s\\S]{0,80}?...). On maliciously crafted input files, these could cause ReDoS (Regular Expression Denial of Service) hanging the checker.","recommendation":"CODE_PATTERNS.md Security: 'Use {1,64} not + for bounded user input' and 'Add heuristic detection (nested quantifiers, length limits)'. Add input size guards before regex matching: if (content.length > 100000) { console.warn('File too large, skipping pattern checks'); return; }. Review each regex for nested quantifiers and add explicit bounds.","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/check-pattern-compliance.js:106"},{"type":"description","detail":"ANTI_PATTERNS array contains many complex regex with nested quantifiers and unbounded lookaheads (e.g., line 137: /for\\s+\\w+\\s+in\\s+1\\s+2\\s+3\\s*;\\s*do[\\s\\S]{0,120}?&&\\s*break[\\s\\S]{0,80}?done(?![\\s\\S]{0,80}?...). On maliciously crafted input files, these could cause ReDoS (Regular Expression Denial of Service) hanging the checker."}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/global/gsd-check-update.js::empty-catch-blocks","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::.claude/hooks/global/gsd-check-update.js::empty-catch-blocks","category":"process","severity":"S1","type":"process-gap","file":".claude/hooks/global/gsd-check-update.js","line":38,"title":"Error handling: Empty catch blocks swallow errors in gsd-check-update.js","description":"Silent failures in version checking hide network issues and file read errors, preventing users from knowing updates are available","recommendation":"Log errors to stderr or a debug log file. Example: catch (e) { console.error('Failed to check GSD version:', e.message); }","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".claude/hooks/global/gsd-check-update.js:38"},{"type":"description","detail":"Silent failures in version checking hide network issues and file read errors, preventing users from knowing updates are available"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::hooks-scripts::empty-catch-pattern","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::hooks-scripts::empty-catch-pattern","category":"process","severity":"S1","type":"process-gap","file":".claude/hooks/component-size-check.js","line":47,"title":"Error handling: 200+ empty catch blocks across hooks and scripts","description":"Silent failures hide real problems: JSON parse errors, file system issues, permission problems. Debugging becomes impossible when errors are swallowed without any logging","recommendation":"Add minimal error logging: catch (err) { console.error('Operation failed:', err.message); }. For non-critical operations, at least log to debug output","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".claude/hooks/component-size-check.js:47"},{"type":"description","detail":"Silent failures hide real problems: JSON parse errors, file system issues, permission problems. Debugging becomes impossible when errors are swallowed without any logging"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::hooks::exit-code-on-security-fail","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::hooks::exit-code-on-security-fail","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/typescript-strict-check.js","line":28,"title":"Error handling: Hook validation exits with 0 on security failures","description":"Security checks that detect path traversal or invalid input exit with 0 (success), making it appear operations succeeded when they should have been rejected. This could mask security issues","recommendation":"Distinguish between 'not applicable' (exit 0) and 'validation failed' (exit 1). When detecting security issues like path traversal, exit with non-zero code and log the security violation","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::unprotected-readfilesync","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::scripts::unprotected-readfilesync","category":"process","severity":"S1","type":"process-gap","file":"scripts/verify-sonar-phase.js","line":134,"title":"Error handling: readFileSync without try/catch in multiple scripts","description":"Unprotected file reads cause uncaught exceptions that crash scripts. Users see stack traces instead of helpful error messages. ENOENT errors don't explain what file was missing or why","recommendation":"Wrap all readFileSync calls in try/catch with helpful error messages. Example: try { content = fs.readFileSync(file, 'utf8'); } catch (err) { console.error(`Failed to read config file: ${err.message}\\nPlease ensure the file exists and is readable.`); process.exit(1); }","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/verify-sonar-phase.js:134"},{"type":"description","detail":"Unprotected file reads cause uncaught exceptions that crash scripts. Users see stack traces instead of helpful error messages. ENOENT errors don't explain what file was missing or why"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::execsync-no-timeout","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::scripts::execsync-no-timeout","category":"process","severity":"S1","type":"process-gap","file":"scripts/validate-audit.js","line":651,"title":"Error handling: execSync without timeout or error handling","description":"execSync calls without timeout can hang indefinitely if child processes freeze. Missing error handling means command failures crash the script with cryptic errors","recommendation":"Always include timeout option and wrap in try/catch. Example: try { const output = execSync(cmd, { timeout: 10000, encoding: 'utf8' }); } catch (err) { if (err.killed) { console.error('Command timed out after 10s'); } else { console.error('Command failed:', err.message); } process.exit(1); }","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/validate-audit.js:651"},{"type":"description","detail":"execSync calls without timeout can hang indefinitely if child processes freeze. Missing error handling means command failures crash the script with cryptic errors"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::path-exposure-in-errors","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::scripts::path-exposure-in-errors","category":"process","severity":"S2","type":"process-gap","file":"scripts/validate-audit.js","line":110,"title":"Error handling: Error messages expose full system paths","description":"Error messages that include full absolute paths expose system structure (/home/username/, /Users/name/projects/). In logs or error reports, this leaks potentially sensitive information about deployment structure","recommendation":"Use path.basename() or relative paths in error messages. Example: Instead of 'Audit file not found: /home/user/project/file.jsonl', use 'Audit file not found: file.jsonl (expected in docs/audits/)'","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::state-utils::silent-write-failures","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::state-utils::silent-write-failures","category":"process","severity":"S1","type":"process-gap","file":".claude/hooks/state-utils.js","line":75,"title":"Error handling: State file operations fail silently","description":"State persistence failures mean hooks lose track of session context, agent invocations, and compaction data. Silent failures leave the system in an inconsistent state without alerting anyone","recommendation":"writeState() should return false on failure but caller should check the return value. Example: if (!writeState(data)) { console.error('âš ï¸  Failed to save session state - data may be lost after compaction'); }","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".claude/hooks/state-utils.js:75"},{"type":"description","detail":"State persistence failures mean hooks lose track of session context, agent invocations, and compaction data. Silent failures leave the system in an inconsistent state without alerting anyone"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::hooks::json-parse-no-context","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::hooks::json-parse-no-context","category":"process","severity":"S2","type":"process-gap","file":"scripts/verify-skill-usage.js","line":136,"title":"Error handling: JSON.parse failures without validation context","description":"When JSON.parse fails in hooks, empty catch blocks hide what was being parsed and why it failed. Malformed hook arguments or corrupted state files become impossible to debug","recommendation":"Log parse failures with context. Example: catch (err) { console.error('Failed to parse hook arguments:', arg.substring(0, 100), err.message); return null; }","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::write-no-error-msg","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::scripts::write-no-error-msg","category":"process","severity":"S2","type":"process-gap","file":"scripts/update-readme-status.js","line":107,"title":"Error handling: Missing error messages in file write operations","description":"writeFileSync failures (disk full, permission denied, read-only filesystem) crash without explaining what failed to write. Users can't tell if their data was saved or lost","recommendation":"Wrap writes in try/catch with descriptive errors. Example: try { fs.writeFileSync(file, data); } catch (err) { console.error(`Failed to write ${path.basename(file)}: ${err.code === 'ENOSPC' ? 'Disk full' : err.message}`); process.exit(1); }","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/settings.json::continue-on-error-usage","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::.claude/settings.json::continue-on-error-usage","category":"process","severity":"S0","type":"process-gap","file":".claude/settings.json","line":24,"title":"Error handling: continueOnError used appropriately in settings.json","description":"continueOnError is correctly used for non-critical operations: remote branch checks (network may be unavailable), dashboard cleanup (dev-only), and commit tracking (metadata only). These should not block the workflow","recommendation":"No fix needed - usage is appropriate. These hooks enhance the workflow but aren't critical path","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".claude/settings.json:24"},{"type":"description","detail":"continueOnError is correctly used for non-critical operations: remote branch checks (network may be unavailable), dashboard cleanup (dev-only), and commit tracking (metadata only). These should not block the workflow"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/verify-skill-usage.js::exit-zero-with-violations","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::scripts/verify-skill-usage.js::exit-zero-with-violations","category":"process","severity":"S2","type":"process-gap","file":"scripts/verify-skill-usage.js","line":232,"title":"Error handling: Validation scripts exit 0 with violations in non-strict mode","description":"verify-skill-usage.js exits 0 even when violations exist (non-strict mode). This makes CI integration confusing - the script reports issues but signals success. Violations may be ignored","recommendation":"Exit code should reflect presence of violations regardless of mode. Use --quiet to suppress output, not to change exit behavior. Example: if (violations.length > 0) process.exit(1); else process.exit(0);","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/typescript-strict-check.js::multiple-exit-zero","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::.claude/hooks/typescript-strict-check.js::multiple-exit-zero","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/typescript-strict-check.js","line":28,"title":"Error handling: Multiple exit(0) calls suggest unclear control flow","description":"typescript-strict-check.js has 15+ process.exit(0) calls in validation logic. This pattern makes it unclear which exits are 'check passed' vs 'check not applicable' vs 'check skipped due to error'","recommendation":"Refactor to have a single exit point with clear exit code strategy. Use early returns instead of exit(0) in validation functions, then exit once at the end based on accumulated state","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::hooks::error-visibility","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::hooks::error-visibility","category":"process","severity":"S1","type":"process-gap","file":".claude/hooks/component-size-check.js","line":47,"title":"Error handling: Hook execution errors not propagated to user","description":"When PostToolUse hooks fail silently (empty catch blocks), users don't know validation ran or failed. They may proceed thinking code is validated when checks actually crashed","recommendation":"Hooks should output clear status messages: 'ok' on success, error description on failure. Log to stderr for errors while preserving stdout for hook protocol. Example: catch (err) { console.error('Hook failed:', err.message); console.log('error'); process.exit(1); }","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".claude/hooks/component-size-check.js:47"},{"type":"description","detail":"When PostToolUse hooks fail silently (empty catch blocks), users don't know validation ran or failed. They may proceed thinking code is validated when checks actually crashed"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/generate-pending-alerts.js::no-cleanup-on-failure","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::scripts/generate-pending-alerts.js::no-cleanup-on-failure","category":"process","severity":"S1","type":"process-gap","file":"scripts/generate-pending-alerts.js","line":389,"title":"Error handling: generate-pending-alerts.js throws error on write failure but doesn't clean up partial state","description":"Script throws 'Failed to write alerts file' but doesn't rollback partial writes or clean up temp files. This can leave .alerts.json in inconsistent state, causing downstream tools to fail","recommendation":"Use atomic write pattern: write to temp file, validate, then rename. On failure, clean up temp file and preserve existing alerts file. Example: const tmp = file + '.tmp'; try { fs.writeFileSync(tmp, data); fs.renameSync(tmp, file); } catch (err) { fs.rmSync(tmp, {force:true}); throw err; }","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/generate-pending-alerts.js:389"},{"type":"description","detail":"Script throws 'Failed to write alerts file' but doesn't rollback partial writes or clean up temp files. This can leave .alerts.json in inconsistent state, causing downstream tools to fail"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/debt/sync-sonarcloud.js::verbose-api-errors","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::scripts/debt/sync-sonarcloud.js::verbose-api-errors","category":"process","severity":"S2","type":"process-gap","file":"scripts/debt/sync-sonarcloud.js","line":290,"title":"Error handling: sync-sonarcloud.js API errors expose implementation details","description":"Error message includes full HTTP response status and potentially sensitive API error details. In logs, this could expose API keys or internal service details","recommendation":"Sanitize error messages before throwing. Example: throw new Error(`SonarCloud API request failed (${response.status}). Check SONARCLOUD_TOKEN environment variable.`); Don't include response body or headers in error messages","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::execsync-interactive-hang","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-5a-error-handling.jsonl","original_id":"process::scripts::execsync-interactive-hang","category":"process","severity":"S1","type":"process-gap","file":"scripts/validate-audit.js","line":651,"title":"Error handling: execSync in validation scripts can hang on interactive prompts","description":"execSync('npm audit') and execSync('npm run lint') without stdio: 'pipe' can hang if these commands prompt for user input. Hook execution would freeze indefinitely","recommendation":"Already using stdio: ['ignore', 'pipe', 'pipe'] pattern correctly in validate-audit.js. Verify all other execSync calls use similar pattern to prevent stdin interaction","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/validate-audit.js:651"},{"type":"description","detail":"execSync('npm audit') and execSync('npm run lint') without stdio: 'pipe' can hang if these commands prompt for user input. Hook execution would freeze indefinitely"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-pattern-compliance.js::sync-file-reads","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/check-pattern-compliance.js::sync-file-reads","category":"process","severity":"S2","type":"process-gap","file":"scripts/check-pattern-compliance.js","line":799,"title":"Perf: check-pattern-compliance.js - Synchronous file reads in loop","description":"Hook script reads 10-20 files synchronously in pre-commit, blocking for ~200-500ms. Async parallel reads could reduce to ~50-100ms","recommendation":"Convert checkFile() to async, use Promise.all() to read files in parallel batches of 10. Change readFileSync to fs.promises.readFile","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-pattern-compliance.js::unoptimized-pattern-matching","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/check-pattern-compliance.js::unoptimized-pattern-matching","category":"process","severity":"S2","type":"process-gap","file":"scripts/check-pattern-compliance.js","line":733,"title":"Perf: check-pattern-compliance.js - O(n*m) pattern matching without optimization","description":"For each file, iterates ALL 30+ patterns even if file extension doesn't match. Checking 20 JS files = 600+ regex compilations. Pre-filtering could reduce by 60%","recommendation":"Group patterns by fileTypes, only check relevant patterns. Build Map<extension, patterns[]> at startup. Skip patterns where file extension not in fileTypes","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-pattern-compliance.js::regex-recompilation","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/check-pattern-compliance.js::regex-recompilation","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-pattern-compliance.js","line":659,"title":"Perf: check-pattern-compliance.js - Regex recompilation in hot path","description":"Creates new RegExp objects for every file checked (line 659, 662). Checking 20 files with 30 patterns = 1200 regex compilations. Cache compiled regexes","recommendation":"Pre-compile all regexes at module load: const compiledPatterns = ANTI_PATTERNS.map(p => ({...p, regex: new RegExp(...)})); Use compiledPatterns in checkFile","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-docs-light.js::sync-map-file-reads","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/check-docs-light.js::sync-map-file-reads","category":"process","severity":"S1","type":"process-gap","file":"scripts/check-docs-light.js","line":825,"title":"Perf: check-docs-light.js - Synchronous file reads in map()","description":"CI docs-lint job reads 50+ markdown files synchronously with readFileSync (line 495), taking 2-3 seconds. Async parallel reads could cut time to <500ms","recommendation":"Convert lintDocument to async function, use Promise.all() with batch size limit (10 concurrent). Replace readFileSync with fs.promises.readFile in readDocumentContent","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/check-docs-light.js:825"},{"type":"description","detail":"CI docs-lint job reads 50+ markdown files synchronously with readFileSync (line 495), taking 2-3 seconds. Async parallel reads could cut time to <500ms"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-docs-light.js::quadratic-anchor-validation","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/check-docs-light.js::quadratic-anchor-validation","category":"process","severity":"S2","type":"process-gap","file":"scripts/check-docs-light.js","line":411,"title":"Perf: check-docs-light.js - O(n^2) anchor link validation","description":"validateAnchorLinks has nested loop: for each link (50+), iterates all headings (100+) = 5000 comparisons per doc. Large docs like ROADMAP.md take 500ms just for anchor checks","recommendation":"Build Set of valid anchors ONCE (line 400-409), then O(1) Set.has() lookup per link. Remove lines 426-432 partial match fallback (causes the O(n) inner loop)","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-docs-light.js::repeated-fstat-calls","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/check-docs-light.js::repeated-fstat-calls","category":"process","severity":"S2","type":"process-gap","file":"scripts/check-docs-light.js","line":686,"title":"Perf: check-docs-light.js - Repeated realpath/stat calls","description":"resolveFileArgs calls realpathSync 3x per file (lines 686, 707, 714) plus lstatSync. For 50 files = 200 syscalls. Caching realpath(ROOT) saves 100+ calls","recommendation":"Compute rootRealResolved once (done at line 683), cache realpath results in Map<path, realpath>. Skip redundant lstatSync at line 714 (containment already verified)","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/generate-documentation-index.js::sync-doc-processing","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/generate-documentation-index.js::sync-doc-processing","category":"process","severity":"S1","type":"process-gap","file":"scripts/generate-documentation-index.js","line":913,"title":"Perf: generate-documentation-index.js - Synchronous file reads in loop","description":"npm run docs:index reads 80+ markdown files sequentially with readFileSync (line 485), taking 3-4 seconds. CI job blocks during this time. Async could reduce to <1 second","recommendation":"Convert processFile to async, use Promise.all with batch limit (15 concurrent): const batches = chunk(activeFiles, 15); for (batch of batches) await Promise.all(batch.map(processFile))","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/generate-documentation-index.js:913"},{"type":"description","detail":"npm run docs:index reads 80+ markdown files sequentially with readFileSync (line 485), taking 3-4 seconds. CI job blocks during this time. Async could reduce to <1 second"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/generate-documentation-index.js::regex-in-loop","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/generate-documentation-index.js::regex-in-loop","category":"process","severity":"S3","type":"process-gap","file":"scripts/generate-documentation-index.js","line":376,"title":"Perf: generate-documentation-index.js - Regex compilation in extractLinks loop","description":"extractLinks creates new RegExp on line 376 for EVERY document processed (80+ times). This regex is complex with capturing groups. Move to module level","recommendation":"Move linkRegex to module-level constant: const LINK_REGEX = /\\[([^\\]]{1,500})\\]\\(([^)]{1,500})\\)/g; In extractLinks, clone it: const linkRegex = new RegExp(LINK_REGEX.source, LINK_REGEX.flags)","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/generate-documentation-index.js::quadratic-reference-graph","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/generate-documentation-index.js::quadratic-reference-graph","category":"process","severity":"S2","type":"process-gap","file":"scripts/generate-documentation-index.js","line":524,"title":"Perf: generate-documentation-index.js - O(n*m) reference graph building","description":"buildReferenceGraph: for 80 docs with 20 links each = 1600 iterations. Each does Map.get() twice (lines 535-536). With 200+ docs this becomes noticeable (500ms+)","recommendation":"Use Map.get once, assign to variable. Combine lines 534-536 into: const targetNode = graph.get(target); if (targetNode) { node.outbound.push(target); targetNode.inbound.push(doc.path); }","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/aggregate-audit-findings.js::quadratic-dedup","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/aggregate-audit-findings.js::quadratic-dedup","category":"process","severity":"S1","type":"process-gap","file":"scripts/aggregate-audit-findings.js","line":1309,"title":"Perf: aggregate-audit-findings.js - O(n^2) deduplication with large buckets","description":"processBucketPairs has O(k^2) nested loop for each bucket. With 250 item bucket cap, worst case is 31,250 comparisons per bucket. Full aggregation takes 10-15 seconds","recommendation":"Add early termination: if bucket size > threshold AND no merges in last N comparisons, skip rest. Or use LSH (Locality Sensitive Hashing) to reduce comparison space. Lower MAX_FILE_BUCKET from 250 to 100","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/aggregate-audit-findings.js:1309"},{"type":"description","detail":"processBucketPairs has O(k^2) nested loop for each bucket. With 250 item bucket cap, worst case is 31,250 comparisons per bucket. Full aggregation takes 10-15 seconds"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/aggregate-audit-findings.js::sync-jsonl-reads","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/aggregate-audit-findings.js::sync-jsonl-reads","category":"process","severity":"S2","type":"process-gap","file":"scripts/aggregate-audit-findings.js","line":1201,"title":"Perf: aggregate-audit-findings.js - Synchronous JSONL file reads","description":"Reads 7 JSONL files sequentially in parseSingleSessionAudits and parseCanonFiles (lines 1201, 1226). Each readFileSync blocks. Total time ~300-500ms. Parallel reads could reduce to <100ms","recommendation":"Convert parseJsonlFile to async with fs.promises.readFile. Use Promise.all to read all 7 category files in parallel. Await results before proceeding to phase 2","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/aggregate-audit-findings.js::expensive-levenshtein","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/aggregate-audit-findings.js::expensive-levenshtein","category":"process","severity":"S2","type":"process-gap","file":"scripts/aggregate-audit-findings.js","line":1013,"title":"Perf: aggregate-audit-findings.js - Expensive Levenshtein in hot path","description":"levenshteinDistance called in dedup loop with O(m*n) DP algorithm. For 500 findings, potentially 10,000+ calls. Each 500-char comparison = 250,000 operations. Truncate earlier or cache","recommendation":"Reduce MAX_LEVENSHTEIN_LENGTH from 500 to 200 chars (still enough for titles). Add memoization: const cache = new Map(); Check cache before computing. Clear cache between dedup passes","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/aggregate-audit-findings.js::repeated-normalization","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/aggregate-audit-findings.js::repeated-normalization","category":"process","severity":"S3","type":"process-gap","file":"scripts/aggregate-audit-findings.js","line":1044,"title":"Perf: aggregate-audit-findings.js - Repeated string normalization","description":"similarityScore does replaceAll(/[^a-z0-9\\s]/g, '') twice (line 1044) for EVERY comparison. Regex replacement is expensive. Called 10,000+ times during dedup","recommendation":"Cache normalized strings: const normalized = new Map(); function getNormalized(str) { if (!normalized.has(str)) normalized.set(str, str.toLowerCase().replaceAll(...)); return normalized.get(str); }","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-content-accuracy.js::sync-reads-loop","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/check-content-accuracy.js::sync-reads-loop","category":"process","severity":"S2","type":"process-gap","file":"scripts/check-content-accuracy.js","line":458,"title":"Perf: check-content-accuracy.js - Synchronous file reads in loop","description":"Reads all markdown files sequentially with readFileSync (line 412). For 50+ docs, takes 1-2 seconds. CI content validation could be 3-4x faster with async","recommendation":"Convert checkDocument to async function using fs.promises.readFile. Use Promise.all with batch size 10: const results = []; for (const batch of chunk(files, 10)) results.push(...await Promise.all(batch.map(checkDocument)))","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-content-accuracy.js::regex-in-hot-loops","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/check-content-accuracy.js::regex-in-hot-loops","category":"process","severity":"S2","type":"process-gap","file":"scripts/check-content-accuracy.js","line":114,"title":"Perf: check-content-accuracy.js - Regex compilation in hot loops","description":"versionPatterns (line 114), pathPatterns (line 197), npmPatterns (line 286) created fresh for EVERY file. For 50 files = 150+ array allocations with regex literals. Move to module level","recommendation":"Declare patterns as module-level constants outside functions: const VERSION_PATTERNS = [...]; const PATH_PATTERNS = [...]; const NPM_PATTERNS = [...]; Reference these in check functions","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-content-accuracy.js::nested-pattern-loops","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4c-script-performance.jsonl","original_id":"process::scripts/check-content-accuracy.js::nested-pattern-loops","category":"process","severity":"S2","type":"process-gap","file":"scripts/check-content-accuracy.js","line":136,"title":"Perf: check-content-accuracy.js - O(lines * patterns) nested loops","description":"checkVersionAccuracy, checkPathReferences, checkNpmScriptReferences all have nested loops: for each line, iterate all patterns, exec in while loop. Large docs (500+ lines) with 5+ patterns = 2500+ regex execs","recommendation":"Combine all patterns into single alternation regex: /pattern1|pattern2|pattern3/g. Single pass per line with switch on match type. Or use multiline mode to match entire content once","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/ci.yml::build-sequential","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4b-ci-performance.jsonl","original_id":"process::.github/workflows/ci.yml::build-sequential","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/ci.yml","line":136,"title":"CI slow: Build job waits unnecessarily for all lint/test steps","description":"Build could start after type checking completes, saving 2-3 minutes per CI run. Lint/test don't need to block build since both are independent verification steps.","recommendation":"Split into 3 parallel jobs: (1) lint+format+deps checks, (2) typecheck+test, (3) build. Then add a final 'all-checks' job that depends on all three. This reduces critical path from ~8min to ~5min.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/ci.yml::duplicate-npm-ci","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4b-ci-performance.jsonl","original_id":"process::.github/workflows/ci.yml::duplicate-npm-ci","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/ci.yml","line":149,"title":"CI slow: Redundant npm ci in build job","description":"npm ci runs twice (once in lint-typecheck-test, once in build), wasting 30-60s per run. node_modules could be cached as artifact and reused.","recommendation":"Use actions/cache or actions/upload-artifact to cache node_modules after first npm ci, then restore in build job. Or use a shared 'setup' job that both depend on.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/ci.yml::no-nextjs-cache","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4b-ci-performance.jsonl","original_id":"process::.github/workflows/ci.yml::no-nextjs-cache","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/ci.yml","line":152,"title":"CI slow: No Next.js build cache","description":"Next.js builds take 2-4 minutes but .next directory isn't cached between runs. Incremental builds could reduce this to 30-60s for small changes.","recommendation":"Add actions/cache step to cache .next/cache directory using hash of source files as key. Next.js supports incremental builds when cache is present.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/ci.yml::no-path-filters","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4b-ci-performance.jsonl","original_id":"process::.github/workflows/ci.yml::no-path-filters","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/ci.yml","line":3,"title":"CI slow: No path filters on main CI workflow","description":"CI runs full test suite even for docs-only changes (*.md files). Adds 5-8 minutes of unnecessary CI time for documentation PRs.","recommendation":"Add path-ignore filter to skip CI when only docs, markdown, or non-code files change. Keep required checks but mark as skipped for docs PRs.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/deploy-firebase.yml::duplicate-build","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4b-ci-performance.jsonl","original_id":"process::.github/workflows/deploy-firebase.yml::duplicate-build","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/deploy-firebase.yml","line":47,"title":"CI slow: Firebase deploy builds app twice","description":"Both preview-deploy and deploy jobs run 'npm run build' independently (2-4 min each). If CI workflow also builds, that's 3 separate builds of the same code.","recommendation":"Make deploy workflow depend on ci.yml's build job using workflow_run trigger, then download build artifact. Or create a reusable workflow that both can call.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/deploy-firebase.yml::sequential-deploys","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4b-ci-performance.jsonl","original_id":"process::.github/workflows/deploy-firebase.yml::sequential-deploys","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/deploy-firebase.yml","line":141,"title":"CI slow: Firebase deploys run sequentially","description":"Functions, firestore rules, and hosting deploy sequentially (line 141, 144, 147). Each takes 30-90s. Running in parallel could save 1-2 minutes.","recommendation":"Use 3 parallel jobs or firebase deploy with multiple targets in one command (firebase deploy --only functions,firestore:rules,hosting). Verify Firebase CLI supports parallel deploys without conflicts.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/sonarcloud.yml::no-path-filters","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4b-ci-performance.jsonl","original_id":"process::.github/workflows/sonarcloud.yml::no-path-filters","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/sonarcloud.yml","line":7,"title":"CI slow: SonarCloud runs on all changes including docs","description":"SonarCloud analysis runs on every push/PR even for docs-only changes. Takes 1-2 minutes and consumes analysis quota unnecessarily.","recommendation":"Add paths filter to only run when code files change (*.ts, *.tsx, *.js, *.jsx). Skip for docs/markdown-only changes.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/sonarcloud.yml::full-fetch-depth","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4b-ci-performance.jsonl","original_id":"process::.github/workflows/sonarcloud.yml::full-fetch-depth","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/sonarcloud.yml","line":31,"title":"CI slow: Full git history fetched unnecessarily","description":"fetch-depth: 0 downloads entire git history (can be 100MB+ and take 30-60s). Most workflows only need recent commits for diffs.","recommendation":"Change fetch-depth to a reasonable number (e.g., 50) or remove if not needed. Only use fetch-depth: 0 when truly necessary (e.g., for blame analysis).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/backlog-enforcement.yml::duplicate-npm-ci","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4b-ci-performance.jsonl","original_id":"process::.github/workflows/backlog-enforcement.yml::duplicate-npm-ci","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/backlog-enforcement.yml","line":26,"title":"CI slow: Backlog workflow installs deps twice","description":"backlog-health and security-patterns jobs both run npm ci independently (30-60s each). They could share a setup job or reuse cache.","recommendation":"Create a shared 'setup' job that runs npm ci once and uploads node_modules as artifact. Both jobs depend on setup and download artifact instead of running npm ci.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/review-check.yml::no-path-filters","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4b-ci-performance.jsonl","original_id":"process::.github/workflows/review-check.yml::no-path-filters","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/review-check.yml","line":3,"title":"CI slow: Review check runs on all PRs without path filters","description":"Review trigger check runs on all PRs including docs-only changes. Wastes 30-60s for PRs that don't touch code.","recommendation":"Add paths filter to only run when code files change. For docs-only PRs, this check is not relevant.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/auto-label-review-tier.yml::no-npm-cache","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4b-ci-performance.jsonl","original_id":"process::.github/workflows/auto-label-review-tier.yml::no-npm-cache","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/auto-label-review-tier.yml","line":22,"title":"CI slow: Auto-label workflow has no npm cache","description":"setup-node doesn't have cache: 'npm' configured, so npm packages are re-downloaded on every run (adds 10-20s).","recommendation":"Add cache: 'npm' to setup-node action to enable npm caching. This is a one-line change.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/docs-lint.yml::sequential-processing","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4b-ci-performance.jsonl","original_id":"process::.github/workflows/docs-lint.yml::sequential-processing","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/docs-lint.yml","line":58,"title":"CI slow: Docs lint processes files sequentially","description":"Documentation linter processes files one-by-one in bash loop (line 58-113). For PRs with 10+ markdown files, this can take 2-3 minutes. Parallel processing could reduce to 30-60s.","recommendation":"Refactor to run linter in parallel using xargs -P or rewrite the bash logic into a Node.js script that uses Promise.all to check files concurrently.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-commit::slow-eslint-full-scan","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::.husky/pre-commit::slow-eslint-full-scan","category":"process","severity":"S2","type":"process-gap","file":".husky/pre-commit","line":9,"title":"Slow: ESLint full codebase scan in pre-commit","description":"ESLint scans entire codebase (~3-10s) on every commit instead of only staged files. Developers wait unnecessarily even for small changes. Over time this encourages --no-verify bypassing.","recommendation":"Use 'npm run lint -- --cache' for caching, or integrate with lint-staged to check only staged files. lint-staged already runs Prettier (line 23), could also run ESLint on same files.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-commit+pre-push::duplicate-pattern-check","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::.husky/pre-commit+pre-push::duplicate-pattern-check","category":"process","severity":"S2","type":"process-gap","file":".husky/pre-commit","line":35,"title":"Duplicate: Pattern compliance runs in both pre-commit and pre-push","description":"'npm run patterns:check' runs twice - once in pre-commit (line 35) and again in pre-push (line 26). Same files checked twice adds 1-3s per push. Pure waste since files don't change between commit and push.","recommendation":"Remove pattern check from pre-commit OR pre-push. Recommend keeping in pre-commit only (fail fast) and removing from pre-push line 21-35. Pattern violations should be caught at commit time.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-push::slow-tsc-no-incremental","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::.husky/pre-push::slow-tsc-no-incremental","category":"process","severity":"S2","type":"process-gap","file":".husky/pre-push","line":83,"title":"Slow: TypeScript full project type check on every push","description":"'npx tsc --noEmit' does full project type check (5-15s) on every push with no incremental caching. For large projects this becomes painful. Developers may skip with git push --no-verify.","recommendation":"1) Use 'tsc --noEmit --incremental' with tsbuildinfo caching, OR 2) Use 'tsc --noEmit --pretty' with file filtering for changed files only, OR 3) Move to CI and make optional in pre-push with SKIP_TYPE_CHECK=1 override.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-push::slow-madge-full-scan","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::.husky/pre-push::slow-madge-full-scan","category":"process","severity":"S2","type":"process-gap","file":".husky/pre-push","line":12,"title":"Slow: Circular dependency scan on entire codebase every push","description":"'madge --circular' scans lib/, components/, app/ directories (2-5s) on every push regardless of what changed. Circular deps are architectural issues that rarely appear in normal development. Full scan is overkill.","recommendation":"1) Add madge caching or run incrementally on changed files only, OR 2) Move to CI as a scheduled check (daily/weekly), OR 3) Make optional with SKIP_CIRCULAR_CHECK=1 and run only when dependency files change (package.json, imports in changed files).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::package.json::slow-test-build","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::package.json::slow-test-build","category":"process","severity":"S2","type":"process-gap","file":"package.json","line":11,"title":"Slow: Test suite rebuilds TypeScript on every test run","description":"'npm test' runs 'test:build' which compiles ALL TypeScript (tsc + tsc-alias) before tests. Pre-commit runs tests (line 59/80) adding 10-30s for full compilation even for small changes. No incremental compilation.","recommendation":"1) Use 'tsc --incremental' in test:build for caching, OR 2) Use tsx/ts-node for on-the-fly compilation without build step, OR 3) Only run test:build when test files or dependencies change (check git diff).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-push::sequential-security-checks","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::.husky/pre-push::sequential-security-checks","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-push","line":50,"title":"Inefficient: Sequential security checks in pre-push","description":"Pre-push security checks loop through changed files sequentially (lines 50-65), running 'node scripts/security-check.js --file' once per file. For 10+ changed files, this adds 5-15s. No parallelization means cores sit idle.","recommendation":"1) Modify security-check.js to accept multiple files at once, OR 2) Use xargs -P4 for parallel execution, OR 3) Rewrite loop to spawn checks in parallel and wait for all results. Example: 'echo \"$changed_files\" | xargs -P4 -I{} node scripts/security-check.js --file {}'","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/settings.json::many-posttooluse-hooks","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::.claude/settings.json::many-posttooluse-hooks","category":"process","severity":"S2","type":"process-gap","file":".claude/settings.json","line":57,"title":"Performance: 10 Claude hooks run on every Write/Edit operation","description":"Every Write/Edit/MultiEdit triggers 10 separate hooks (check-write-requirements, audit-s0s1-validator, pattern-check, component-size-check, firestore-write-block, test-mocking-validator, app-check-validator, typescript-strict-check, repository-pattern-check, agent-trigger-enforcer). Total: ~5818 lines of hook code. Each hook spawns node process, adds 1-3s latency per file write. Poor DX during active development.","recommendation":"1) Batch-execute hooks in single node process to avoid spawn overhead, OR 2) Make hooks async/parallel where possible, OR 3) Add smart skipping - only run relevant hooks based on file type (e.g., firestore-write-block only for firestore files), OR 4) Move some non-critical checks to pre-commit only.","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/pattern-check.js::per-file-overhead","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::.claude/hooks/pattern-check.js::per-file-overhead","category":"process","severity":"S3","type":"process-gap","file":".claude/hooks/pattern-check.js","line":1,"title":"Inefficient: Pattern check runs on every file write via Claude hook","description":"Pattern-check.js hook (line 73, 123 in settings.json) runs check-pattern-compliance.js on EVERY file write/edit during Claude sessions. For quick doc edits or small changes, running full pattern checker adds 0.5-2s overhead. Pre-commit already runs patterns:check (line 35), making this redundant during development.","recommendation":"1) Make pattern-check.js conditional - skip for .md/.txt/docs files, OR 2) Add debouncing - only check after N writes or M seconds, OR 3) Remove hook and rely solely on pre-commit pattern check (fail fast at commit, not during writing), OR 4) Make hook informational only (warn but don't block).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-commit::multiple-git-scans","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::.husky/pre-commit::multiple-git-scans","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-commit","line":47,"title":"Inefficient: Multiple git status/diff scans in pre-commit","description":"Pre-commit runs 'git diff --cached --name-only' separately at lines 47, 94, 138, 159, 194. Each git call adds 50-200ms overhead. For repos with many files, this compounds to 0.5-1s wasted on duplicate filesystem scans.","recommendation":"Run 'git diff --cached --name-only' ONCE at the top of pre-commit hook and store in STAGED_FILES variable. Reuse this variable throughout. Already done partially (line 47, 94 reuse), but lines 138, 159, 194 run fresh git commands. Consolidate all into single scan.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-commit::no-test-timeout","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::.husky/pre-commit::no-test-timeout","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-commit","line":59,"title":"Risk: No timeout on npm test in pre-commit","description":"'npm test' runs without timeout in pre-commit (lines 59, 80). If test hangs due to async issue, developer waits indefinitely or force-quits, losing context. No escape hatch besides killing terminal. Degraded DX for rare but frustrating hangs.","recommendation":"Add timeout wrapper: 'timeout 120 npm test' (120s = 2 min). If tests hang beyond reasonable time, hook fails fast with clear timeout message. Document with: 'Tests timed out after 120s. Check for hanging async operations or use SKIP_TESTS=1 for emergency commit.'","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-commit::doc-only-detection-complexity","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::.husky/pre-commit::doc-only-detection-complexity","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-commit","line":68,"title":"Optimization: Doc-only commit detection could be smarter","description":"Doc-only commit detection (lines 68-88) uses complex regex grep filtering to skip tests. Logic is hard to maintain and test. False positives (docs with critical info) or false negatives (code masquerading as docs) could occur. Current regex at line 71 is 100+ chars long.","recommendation":"1) Extract doc-only detection to dedicated script 'scripts/is-doc-only-commit.js' with unit tests, OR 2) Use git diff --name-status with explicit allowlist (docs/, *.md, *.png, *.jsonl) instead of complex exclusion regex, OR 3) Make SKIP_TESTS=1 the default for docs and auto-detect risky files (package.json, tsconfig, etc.) to force tests.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/settings.json::slow-session-start","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::.claude/settings.json::slow-session-start","category":"process","severity":"S3","type":"process-gap","file":".claude/settings.json","line":8,"title":"Slow: Session start hooks add 2-5s latency to every session","description":"SessionStart hooks run 4 sequential node processes (session-start.js, check-mcp-servers.js, check-remote-session-context.js, stop-serena-dashboard.js) on EVERY Claude session start. Total latency: 2-5s before developer can begin work. Compounds frustration for quick questions/checks. Remote session check can be slow if network latency high.","recommendation":"1) Parallelize independent hooks (session-start, check-mcp-servers, stop-serena can run concurrently), OR 2) Make check-remote-session-context.js async/non-blocking with background notification, OR 3) Add cache TTL - skip checks if last run was <5min ago, OR 4) Optimize scripts - combine into single process to avoid spawn overhead.","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/settings.json::user-prompt-overhead","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-4a-hook-performance.jsonl","original_id":"process::.claude/settings.json::user-prompt-overhead","category":"process","severity":"S3","type":"process-gap","file":".claude/settings.json","line":265,"title":"Optimization: UserPromptSubmit hooks run before every user message","description":"UserPromptSubmit hooks (lines 265-290) run 4 checks (alerts-reminder, analyze-user-request, session-end-reminder, plan-mode-suggestion) before processing EVERY user prompt. Adds 0.5-2s perceived latency. For rapid back-and-forth conversations, this degrades conversational flow.","recommendation":"1) Make hooks async - run in background and surface results after response, OR 2) Add smart throttling - only run every Nth prompt or when certain keywords detected, OR 3) Batch hooks into single process to reduce spawn overhead, OR 4) Cache results - skip redundant checks if recent prompt was similar.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude::missing-skill-registry","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3d-skill-functionality.jsonl","original_id":"process::.claude::missing-skill-registry","category":"process","severity":"S3","type":"process-gap","file":".claude","line":1,"title":"Skill issue: skill-registry.json does not exist","description":"Audit checklist mentions verifying skill-registry.json matches actual skills, but this file doesn't exist. This may be outdated documentation or an incomplete implementation of skill tracking.","recommendation":"Either create .claude/skill-registry.json with current skill metadata, or update audit documentation to remove references to this file if it's no longer used.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/skills/SKILL_INDEX.md::incorrect-count","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3d-skill-functionality.jsonl","original_id":"process::.claude/skills/SKILL_INDEX.md::incorrect-count","category":"process","severity":"S3","type":"process-gap","file":".claude/skills/SKILL_INDEX.md","line":3,"title":"Skill issue: SKILL_INDEX.md has incorrect skill count","description":"SKILL_INDEX.md claims 50 total skills but directory contains 57 skills (ls -1 .claude/skills/ | wc -l). This misleads users about available capabilities.","recommendation":"Update SKILL_INDEX.md line 3 to show correct count: 'Total Skills: 57'. Also review category counts for accuracy.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude::episodic-memory-not-configured","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3d-skill-functionality.jsonl","original_id":"process::.claude::episodic-memory-not-configured","category":"process","severity":"S2","type":"process-gap","file":".claude/skills/audit-process/SKILL.md","line":1,"title":"Skill issue: episodic memory MCP not configured but referenced by 11 skills","description":"11 skills reference mcp__plugin_episodic_memory for context retrieval from past sessions, but no MCP configuration file (mcp.json*) contains episodic memory setup. Skills will fail when trying to use this feature, leading to confusing errors for Claude.","recommendation":"Either: (1) Add episodic-memory MCP server to mcp.json configuration with proper credentials, OR (2) Remove episodic memory search sections from all 11 skills if this feature is not available. Option 2 is faster but loses valuable context-retrieval capability.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::docs/audits::missing-false-positives","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3d-skill-functionality.jsonl","original_id":"process::docs/audits::missing-false-positives","category":"process","severity":"S2","type":"process-gap","file":".claude/skills/audit-process/SKILL.md","line":172,"title":"Skill issue: audit skills reference non-existent FALSE_POSITIVES.jsonl","description":"Multiple audit skills instruct Claude to read docs/audits/FALSE_POSITIVES.jsonl to exclude known false positives, but this file doesn't exist. This will cause file read errors during audits and prevent false positive filtering from working.","recommendation":"Create docs/audits/FALSE_POSITIVES.jsonl as an empty array [] or with initial structure: {\"category\":\"security\",\"pattern\":\"...\",\"reason\":\"...\",\"expires\":\"YYYY-MM-DD\"}. Update audit skill documentation to note the file should be created during first audit if it doesn't exist.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/skills/gh-fix-ci::missing-script","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3d-skill-functionality.jsonl","original_id":"process::.claude/skills/gh-fix-ci::missing-script","category":"process","severity":"S1","type":"process-gap","file":".claude/skills/gh-fix-ci/SKILL.md","line":36,"title":"Skill issue: gh-fix-ci references non-existent inspect_pr_checks.py","description":"gh-fix-ci skill's primary workflow relies on scripts/inspect_pr_checks.py to fetch PR check failures, but this script doesn't exist anywhere in the repository. The skill is completely non-functional without it, and will fail immediately when invoked.","recommendation":"Either: (1) Create .claude/skills/gh-fix-ci/scripts/inspect_pr_checks.py implementing the documented API (--repo, --pr, --json flags), OR (2) Update gh-fix-ci skill to use gh CLI commands directly without the wrapper script. Option 2 is simpler but loses abstraction benefits.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".claude/skills/gh-fix-ci/SKILL.md:36"},{"type":"description","detail":"gh-fix-ci skill's primary workflow relies on scripts/inspect_pr_checks.py to fetch PR check failures, but this script doesn't exist anywhere in the repository. The skill is completely non-functional without it, and will fail immediately when invoked."}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/skills/systematic-debugging::missing-superpowers-refs","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3d-skill-functionality.jsonl","original_id":"process::.claude/skills/systematic-debugging::missing-superpowers-refs","category":"process","severity":"S2","type":"process-gap","file":".claude/skills/systematic-debugging/SKILL.md","line":229,"title":"Skill issue: systematic-debugging references non-existent superpowers skills","description":"systematic-debugging references 'superpowers:test-driven-development' and 'superpowers:verification-before-completion' as related skills, but no skills with these names exist. This creates broken references and confuses users trying to follow the skill workflow.","recommendation":"Either: (1) Remove the 'superpowers:' prefix and references since these skills don't exist, OR (2) Create these skills if test-driven-development and verification-before-completion are intended features, OR (3) Update references to point to existing equivalent skills if they exist under different names.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/skills/code-reviewer/scripts::placeholder-code","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3d-skill-functionality.jsonl","original_id":"process::.claude/skills/code-reviewer/scripts::placeholder-code","category":"process","severity":"S3","type":"process-gap","file":".claude/skills/code-reviewer/scripts/pr_analyzer.py","line":1,"title":"Skill issue: code-reviewer scripts may be non-functional templates","description":"code-reviewer skill heavily emphasizes using three Python scripts (pr_analyzer.py, code_quality_checker.py, review_report_generator.py), but these scripts appear to be placeholder/template code based on skill description patterns. If they're not functional implementations, the skill guidance is misleading.","recommendation":"Test the three scripts with sample inputs to verify functionality. If they're templates: (1) Add clear 'TEMPLATE - NOT IMPLEMENTED' warnings to skill documentation, (2) Update skill to focus on manual review patterns instead of script automation, OR (3) Implement the scripts properly if automation is the intended workflow.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/skills/audit-process::complex-orchestration","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3d-skill-functionality.jsonl","original_id":"process::.claude/skills/audit-process::complex-orchestration","category":"process","severity":"S2","type":"process-gap","file":".claude/skills/audit-process/SKILL.md","line":1,"title":"Skill issue: audit-process has complex 7-stage orchestration that may be brittle","description":"audit-process orchestrates 22 parallel agents across 7 stages with extensive verification checkpoints and context recovery logic. This complexity increases the risk of agent coordination failures, variable loss during context compaction, and difficult debugging when stages fail. The skill itself acknowledges this with extensive recovery procedures.","recommendation":"Consider simplifying to fewer stages (e.g., 3-4 instead of 7) or providing a 'lite' mode that runs sequentially with simpler orchestration for smaller codebases. Add automated testing for the orchestration logic itself.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/skills::duplicate-pre-audit","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3d-skill-functionality.jsonl","original_id":"process::.claude/skills::duplicate-pre-audit","category":"process","severity":"S3","type":"process-gap","file":".claude/skills/audit-process/SKILL.md","line":100,"title":"Skill issue: multiple audit skills have duplicate functionality in pre-audit steps","description":"All audit skills (process, security, code, comprehensive) have nearly identical Step 0 (episodic memory search), baseline gathering, and false positives loading. This duplication means updates must be applied to 4+ files, increasing maintenance burden and risk of inconsistency.","recommendation":"Extract common pre-audit steps into a shared skill or reference document (.claude/skills/_audit-common/pre-audit-steps.md) that all audit skills import or reference. This creates a single source of truth for audit initialization.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/skills/session-begin::outdated-filename-reference","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3d-skill-functionality.jsonl","original_id":"process::.claude/skills/session-begin::outdated-filename-reference","category":"process","severity":"S3","type":"process-gap","file":".claude/skills/session-begin/SKILL.md","line":301,"title":"Skill issue: session-begin references deprecated TECHNICAL_DEBT_MASTER.md filename","description":"session-begin skill refers to 'TECHNICAL_DEBT_MASTER.md' but the actual file is 'MASTER_DEBT.jsonl' (JSONL format, not Markdown). This will cause file-not-found errors when following the skill checklist.","recommendation":"Update session-begin/SKILL.md line 301 and any other references to use correct filename: 'docs/technical-debt/MASTER_DEBT.jsonl' instead of 'TECHNICAL_DEBT_MASTER.md'.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-review-needed.js::getnextday-silent-failure","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3c-script-functionality.jsonl","original_id":"process::scripts/check-review-needed.js::getnextday-silent-failure","category":"process","severity":"S2","type":"process-gap","file":"scripts/check-review-needed.js","line":170,"title":"Bug: check-review-needed.js - getNextDay() fails silently on invalid dates","description":"getNextDay() returns empty string on invalid date (line 176), but callers at lines 534 and 550 don't validate the result. This passes empty string to git commands as --since=\"\" which may not fail but could produce unexpected results, potentially causing incorrect review trigger calculations.","recommendation":"Add validation in getNextDay() callers to check for empty string return and handle gracefully: const afterDate = getNextDay(sinceDate); if (!afterDate) { return 0; } // or appropriate fallback","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-cross-doc-deps.js::checkdiffpattern-silent-skip","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3c-script-functionality.jsonl","original_id":"process::scripts/check-cross-doc-deps.js::checkdiffpattern-silent-skip","category":"process","severity":"S2","type":"process-gap","file":"scripts/check-cross-doc-deps.js","line":100,"title":"Bug: check-cross-doc-deps.js - checkDiffPattern() silently skips rules on git errors","description":"checkDiffPattern() catches all errors and returns false without warning (lines 110-118). If git diff fails (e.g., binary file, permission issue, corrupted file), the dependency check rule is silently skipped. This could miss required dependent document updates, defeating the purpose of cross-document enforcement.","recommendation":"Log errors when git diff fails in non-verbose mode, or collect failed checks and report them in summary. Consider: if (verbose) logVerbose(`Failed to check diff...`) should be if (verbose || diffCheckFailed) log(`Warning: Failed to check...`)","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-cross-doc-deps.js::empty-rules-inconsistent","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3c-script-functionality.jsonl","original_id":"process::scripts/check-cross-doc-deps.js::empty-rules-inconsistent","category":"process","severity":"S2","type":"process-gap","file":"scripts/check-cross-doc-deps.js","line":69,"title":"Bug: check-cross-doc-deps.js - inconsistent behavior with empty dependency rules","description":"When config loads with empty rules (line 69), the script exits with error code 2 in normal mode (line 75) but continues in dry-run mode (line 73) and reports success. This inconsistency means --dry-run doesn't accurately simulate normal behavior, and empty config might go unnoticed in testing.","recommendation":"Remove the dry-run exemption: if (dependencyRules.length === 0) { log('Error: cross-doc dependency enforcement disabled due to empty rules.', colors.red); process.exit(2); } // No dry-run check","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/security-check.js::getstagedfiles-silent-failure","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3c-script-functionality.jsonl","original_id":"process::scripts/security-check.js::getstagedfiles-silent-failure","category":"process","severity":"S2","type":"process-gap","file":"scripts/security-check.js","line":310,"title":"Bug: security-check.js - getStagedFiles() returns empty array on git failure without error","description":"getStagedFiles() catch block (line 321-323) returns empty array when git fails (not a git repo, git not installed, permission issues) without logging the error. Main function then prints 'No files to check' which is misleading - the issue is git failure, not absence of files. Users may think security check passed when it silently failed.","recommendation":"Log git errors before returning empty array: try { ... } catch (err) { if (!isQuiet) console.error(`Warning: Could not get staged files from git: ${err.message}`); return []; }","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/validate-audit.js::wildcard-no-existence-check","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3c-script-functionality.jsonl","original_id":"process::scripts/validate-audit.js::wildcard-no-existence-check","category":"process","severity":"S3","type":"process-gap","file":"scripts/validate-audit.js","line":370,"title":"Bug: validate-audit.js - wildcard file patterns not validated for existence","description":"validateFilePath() checks wildcard patterns (line 370-383) by validating only the prefix path for containment, but doesn't verify if the pattern matches any actual files. A finding with file='src/*.js' passes validation even if no such files exist. This allows ineffective or typo'd patterns to pass validation, reducing audit quality.","recommendation":"After containment check for wildcards, optionally use glob library to check if pattern matches at least one file: const matches = glob.sync(finding.file, {cwd: repoRoot}); if (matches.length === 0) { issues.push({type: 'WILDCARD_NO_MATCH', ...}) }","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/generate-documentation-index.js::skipped-links-invisible","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3c-script-functionality.jsonl","original_id":"process::scripts/generate-documentation-index.js::skipped-links-invisible","category":"process","severity":"S3","type":"process-gap","file":"scripts/generate-documentation-index.js","line":417,"title":"Enhancement: generate-documentation-index.js - no visibility into skipped links","description":"extractLinks() silently skips links that fail path containment (line 422-424) or URL decoding (line 401-404). In verbose mode, there's no count or warning about how many links were skipped. Users can't tell if their documentation has broken links attempting path traversal or malformed URLs, reducing the index quality.","recommendation":"Add counter for skipped links and log in verbose mode: let skipped = 0; ... if (resolvedPath === null) { skipped++; if (verbose) logVerbose(`Skipped link in ${currentFile}: ${href} (path traversal)`); continue; } ... if (verbose && skipped > 0) log(`Skipped ${skipped} invalid links in ${currentFile}`)","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/check-pattern-compliance.js::silent-file-filtering","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3c-script-functionality.jsonl","original_id":"process::scripts/check-pattern-compliance.js::silent-file-filtering","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-pattern-compliance.js","line":494,"title":"Enhancement: check-pattern-compliance.js - silent filtering of user-provided files","description":"getFilesToCheck() with explicit FILES argument (line 494-508) applies filtering (path validation, existence checks, global excludes) that can reduce the list to empty. If user provides 'node scripts/check-pattern-compliance.js file1.js file2.js' but both are excluded, the script exits with 'No files to check' without explaining why their specified files were ignored. This is confusing user experience.","recommendation":"Track filtering reasons and report them: const filtered = FILES.filter(...).map((f) => ({file: f, reason: null})); // Track reasons during filtering, then: if (filtered.length === 0 && FILES.length > 0) { console.log(`Warning: All ${FILES.length} specified files were excluded`); }","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/aggregate-audit-findings.js::no-progress-feedback","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3c-script-functionality.jsonl","original_id":"process::scripts/aggregate-audit-findings.js::no-progress-feedback","category":"process","severity":"S3","type":"process-gap","file":"scripts/aggregate-audit-findings.js","line":1446,"title":"Bug: aggregate-audit-findings.js - potential long-running operation with no progress indication","description":"aggregate() performs complex multi-pass deduplication (lines 1344-1402), cross-referencing (lines 1518-1544), and markdown generation on potentially thousands of findings. In non-verbose mode, there's no progress indication. If processing takes minutes or hangs, users can't tell if the script is working or frozen, leading to premature cancellation or confusion.","recommendation":"Add progress indicators for long operations: console.log('Phase 3: Deduplicating findings...'); let passCount = 0; while (didMerge && passCount < MAX_PASSES) { passCount++; if (passCount % 2 === 0) process.stdout.write('.'); ... } console.log(` (${passCount} passes)`)","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/deploy-firebase.yml::pull-request-target-risk","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/deploy-firebase.yml::pull-request-target-risk","category":"process","severity":"S0","type":"process-gap","file":".github/workflows/deploy-firebase.yml","line":7,"title":"CI gap: pull_request_target security vulnerability allows untrusted code execution","description":"Using pull_request_target with code checkout from PR head (line 32) allows malicious PRs to execute arbitrary code with repository secrets access. This is a well-documented GitHub Actions security anti-pattern that could lead to credential theft or repository compromise.","recommendation":"Replace pull_request_target with pull_request and use a separate workflow for preview deploys that runs after CI passes. Alternatively, use pull_request_target but only checkout base branch code, then merge PR changes in a sandboxed environment. See GitHub's security hardening guide.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".github/workflows/deploy-firebase.yml:7"},{"type":"description","detail":"Using pull_request_target with code checkout from PR head (line 32) allows malicious PRs to execute arbitrary code with repository secrets access. This is a well-documented GitHub Actions security anti-pattern that could lead to credential theft or repository compromise."}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/ci.yml::no-coverage-thresholds","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/ci.yml::no-coverage-thresholds","category":"process","severity":"S1","type":"process-gap","file":".github/workflows/ci.yml","line":115,"title":"CI gap: No test coverage thresholds enforced","description":"Tests run with coverage reporting but there are no minimum thresholds configured. Code with 0% test coverage can pass CI, allowing untested code to merge. This defeats the purpose of coverage tracking.","recommendation":"Add c8 configuration with minimum thresholds (e.g., 70% lines, 60% branches). Update ci.yml line 115 to fail if thresholds not met: npm run test:coverage -- --check-coverage --lines 70 --branches 60","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".github/workflows/ci.yml:115"},{"type":"description","detail":"Tests run with coverage reporting but there are no minimum thresholds configured. Code with 0% test coverage can pass CI, allowing untested code to merge. This defeats the purpose of coverage tracking."}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/ci.yml::continue-on-error-abuse","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/ci.yml::continue-on-error-abuse","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/ci.yml","line":74,"title":"CI gap: continue-on-error bypasses critical validations","description":"Four validation steps use continue-on-error: pattern compliance on main (74), documentation check (79), audit validation (89), and technical debt views (106). These failures never block merges, allowing broken patterns, bad docs, invalid audits, and stale debt tracking to accumulate unnoticed.","recommendation":"Remove continue-on-error from all steps except known unstable checks. Make pattern compliance blocking for changed files. Add separate non-blocking 'advisory' job for experimental checks. Use required status checks in GitHub branch protection.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/ci.yml::missing-secrets-silent","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/ci.yml::missing-secrets-silent","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/ci.yml","line":154,"title":"CI gap: Missing secrets cause silent build success","description":"Build step uses secrets for Firebase config (lines 154-159). If secrets are missing or misconfigured, environment variables are set to empty strings and build succeeds. This hides configuration issues until runtime in production.","recommendation":"Add validation step before build: for secret in NEXT_PUBLIC_FIREBASE_API_KEY ...; do [[ -z $secret ]] && exit 1; done. Or use GitHub's required secrets feature and fail explicitly if not set.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/ci.yml::pattern-check-incomplete","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/ci.yml::pattern-check-incomplete","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/ci.yml","line":58,"title":"CI gap: Pattern compliance only checks changed files in PRs","description":"Line 64 only checks changed files in PRs. If pattern rules are updated, existing violations in unchanged files are never caught. Pattern debt accumulates invisibly until someone touches those files. Also, if someone bypasses the check once, the violation persists forever.","recommendation":"Run full pattern check on a schedule (weekly) and post issues for violations. Or run full check on PRs that modify pattern rules themselves. Consider making pattern check blocking only for new violations but report existing ones.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/ci.yml::no-dependency-caching","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/ci.yml::no-dependency-caching","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/ci.yml","line":136,"title":"CI gap: Build job re-installs dependencies wastefully","description":"Build job (lines 136-149) runs after lint-typecheck-test but re-installs all dependencies and rebuilds from scratch. This wastes 2-5 minutes per CI run and increases risk of dependency resolution differences between jobs.","recommendation":"Cache node_modules as artifact in first job and restore in build job. Or combine jobs into single job with multiple steps. Or use Docker layer caching. Document reason if separate jobs are required.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/auto-label-review-tier.yml::label-race-condition","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/auto-label-review-tier.yml::label-race-condition","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/auto-label-review-tier.yml","line":77,"title":"CI gap: Race condition in tier label assignment","description":"Label removal (lines 77-100) and addition (101-150) are separate steps. If workflow runs twice concurrently on rapid PR updates, labels can get into inconsistent state (e.g., both tier-2 and tier-3 present, or no tier label at all).","recommendation":"Use GitHub API's replaceLabels operation which is atomic. Or add concurrency group keyed on PR number with cancel-in-progress: true. Or use a single API call that removes and adds in one transaction.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/auto-label-review-tier.yml::duplicate-tier-logic","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/auto-label-review-tier.yml::duplicate-tier-logic","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/auto-label-review-tier.yml","line":60,"title":"CI gap: Inline tier assignment logic creates maintenance drift","description":"Lines 60-75 contain inline bash logic that duplicates scripts/assign-review-tier.js. Comment says 'TODO: Uncomment when script is ready' but script exists and is referenced. This technical debt causes maintenance burden and drift between workflow and script logic.","recommendation":"Remove TODO placeholder logic and uncomment line 56 to use the actual script. Or if script needs updates, fix it first then switch. Add test that workflow and script produce same tier for sample file sets.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/sonarcloud.yml::skip-fork-prs","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/sonarcloud.yml::skip-fork-prs","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/sonarcloud.yml","line":22,"title":"CI gap: Fork PRs completely skip SonarCloud analysis","description":"Lines 22-24 skip SonarCloud analysis for all fork PRs because secrets aren't available. External contributions get zero static analysis, allowing security issues, code smells, and bugs to merge without detection. This is especially risky since forks are more likely to introduce novel bugs.","recommendation":"Run SonarCloud on a schedule against main branch to catch issues after merge. Or use pull_request_target carefully to analyze fork code (but see security implications). Or require maintainers to manually trigger analysis before merging fork PRs.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/deploy-firebase.yml::no-deployment-validation","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/deploy-firebase.yml::no-deployment-validation","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/deploy-firebase.yml","line":140,"title":"CI gap: Firebase deployment has no success validation","description":"Lines 140-147 deploy functions, rules, and hosting but don't verify deployment actually succeeded. Firebase CLI can exit 0 even if deployment partially failed. Broken deployments may go unnoticed until users report issues.","recommendation":"Add validation steps after each deploy: query Firebase to confirm functions are deployed and callable, test firestore rules with sample operations, curl hosting URL to verify it returns 200. Fail workflow if validation fails.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/deploy-firebase.yml::no-rollback","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/deploy-firebase.yml::no-rollback","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/deploy-firebase.yml","line":140,"title":"CI gap: Deployment has no rollback mechanism","description":"Deployment steps run sequentially (lines 140-147) but if one fails, there's no rollback. A partial deployment could leave production in inconsistent state (e.g., new functions deployed but old hosting, or new rules but old functions).","recommendation":"Add rollback step that runs on failure: capture previous deployment SHA before deploy, on failure run firebase deploy with previous version. Or use Firebase's rollback API. Or deploy to staging first, validate, then promote.","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/deploy-firebase.yml::credentials-filesystem-risk","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/deploy-firebase.yml::credentials-filesystem-risk","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/deploy-firebase.yml","line":120,"title":"CI gap: Service account credentials written to filesystem","description":"Lines 120-124 write service account JSON to $HOME/gcloud-key.json. If subsequent steps fail or are compromised, credentials could be leaked in logs, artifacts, or through file disclosure. File is only cleaned up in always() block which might not run if workflow is cancelled.","recommendation":"Use Google's official auth action which handles credentials securely without writing to disk. Or use base64 encode/pipe: echo $SECRET | base64 -d | gcloud auth activate-service-account --key-file=-. Ensure cleanup happens even on workflow cancellation.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/deploy-firebase.yml::function-delete-masked","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/deploy-firebase.yml::function-delete-masked","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/deploy-firebase.yml","line":131,"title":"CI gap: Deleting functions uses continue-on-error hiding real failures","description":"Line 138 uses continue-on-error for function deletion. This is intended to handle 'function doesn't exist' but also hides real errors like authentication failures, permission issues, or API outages. These failures should block deployment.","recommendation":"Check if function exists before deleting: firebase functions:list | grep -q functionName && firebase functions:delete functionName. Or capture error message and only ignore specific 404-style errors. Fail on auth/permission/API errors.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/deploy-firebase.yml::env-var-inconsistency","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/deploy-firebase.yml::env-var-inconsistency","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/deploy-firebase.yml","line":51,"title":"CI gap: Preview and production use different env var sources","description":"Preview deploy (line 51) uses vars.* (GitHub environment variables) while production (line 104) uses secrets.*. This inconsistency means preview and production builds could have different configurations, making preview testing unreliable and potentially hiding config issues.","recommendation":"Use same source for both (either both vars or both secrets). Document why they're different if intentional. Add validation that all required env vars are present in both sources. Consider using a config file instead.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/backlog-enforcement.yml::missing-replacement-check","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/backlog-enforcement.yml::missing-replacement-check","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/backlog-enforcement.yml","line":35,"title":"CI gap: Backlog check gracefully skips with no replacement validation","description":"Lines 35-42 gracefully skip if AUDIT_FINDINGS_BACKLOG.md doesn't exist (archived per comment), but there's no check that replacement MASTER_DEBT.jsonl exists and is valid. Backlog tracking could silently break if neither file exists.","recommendation":"Add elif check: if [ -f docs/technical-debt/MASTER_DEBT.jsonl ]; then run new validation; else fail with error. Or integrate with existing TDMS validation in ci.yml. Ensure one source of truth is always validated.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/backlog-enforcement.yml::inefficient-security-loop","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/backlog-enforcement.yml::inefficient-security-loop","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/backlog-enforcement.yml","line":147,"title":"CI gap: Security pattern check runs file-by-file inefficiently","description":"Lines 147-150 run security-check.js in a loop for each changed file. This is inefficient (spawns N processes), doesn't aggregate results, and could hide failures in earlier iterations. Also prevents batch optimizations in the script.","recommendation":"Modify security-check.js to accept multiple files: node scripts/security-check.js --files file1 file2 file3. Or pass all files via stdin. Aggregate results and report summary. Run once instead of N times.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/docs-lint.yml::archives-never-checked","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/docs-lint.yml::archives-never-checked","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/docs-lint.yml","line":78,"title":"CI gap: Documentation linting skips archive files entirely","description":"Lines 78-81 skip archive files completely. While archives are historical, broken links and formatting issues make them harder to reference. If someone needs to consult archived docs, broken content creates confusion and wastes time.","recommendation":"Create separate non-blocking job for archive linting. Report issues but don't block PRs. Or run archive lint on a schedule. Or document that archives are explicitly not maintained and add warning banner to archive docs.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/resolve-debt.yml::closed-pr-skip","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/resolve-debt.yml::closed-pr-skip","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/resolve-debt.yml","line":11,"title":"CI gap: Resolve debt workflow only runs on merged PRs","description":"Line 11 only triggers on merged PRs. If a PR mentions DEBT-123 but is closed without merging, the debt item is never updated to reflect the cancelled work. This causes debt tracking to become stale and inaccurate.","recommendation":"Add separate step for closed-without-merge PRs: update debt items to add comment 'PR #123 closed without merging' and revert status if it was changed. Or don't auto-update debt items at all, require manual resolution.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/resolve-debt.yml::skip-ci-debt","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/resolve-debt.yml::skip-ci-debt","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/resolve-debt.yml","line":88,"title":"CI gap: Debt resolution skips CI with [skip ci]","description":"Line 88 includes [skip ci] in commit message. This means debt resolution commits bypass all CI checks including validation of the debt file structure, pattern compliance, and any other checks. Malformed debt commits could merge without detection.","recommendation":"Remove [skip ci] and let normal CI run. Debt resolution should be validated like any other commit. If CI is too slow, optimize CI rather than skipping it. Or use more specific skip flag that only skips expensive tests.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/resolve-debt.yml::rebase-race","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/resolve-debt.yml::rebase-race","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/resolve-debt.yml","line":92,"title":"CI gap: Debt resolution has race condition on rebase","description":"Line 92 does git pull --rebase to handle case where main moved after checkout. But if another workflow pushes between pull and push, this fails. Multiple merged PRs in quick succession can cause workflow failures and failed debt resolution.","recommendation":"Add retry loop like sync-readme.yml (lines 64-78). Or use GitHub's REST API to create commits instead of git push. Or add concurrency group to serialize debt resolution commits.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/review-check.yml::complex-json-parse","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/review-check.yml::complex-json-parse","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/review-check.yml","line":46,"title":"CI gap: Review trigger check has fragile JSON validation","description":"Lines 46-50 have complex JSON validation using piped node one-liner. If this fails, output is replaced with error JSON but parsing errors are silently swallowed. Malformed JSON could cause workflow to incorrectly mark PRs as needing review.","recommendation":"Simplify validation: use jq or node -p 'JSON.parse(process.argv[1])' $OUTPUT. If validation fails, fail the workflow explicitly rather than substituting error JSON. Log original output for debugging.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/review-check.yml::continue-hides-crashes","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/review-check.yml::continue-hides-crashes","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/review-check.yml","line":33,"title":"CI gap: Review check uses continue-on-error hiding crashes","description":"Line 33 has continue-on-error which means if check-review-needed.js crashes (OOM, unhandled exception, etc), workflow continues and treats it as 'review needed'. This creates false positives and alert fatigue. Real errors should be fixed, not hidden.","recommendation":"Remove continue-on-error. Let script failures fail the workflow. Fix the script to handle errors gracefully and return proper exit codes. Use workflow retry for transient failures.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/validate-plan.yml::dead-workflow","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/validate-plan.yml::dead-workflow","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/validate-plan.yml","line":7,"title":"CI gap: Phase validation workflow is likely dead code","description":"Line 7 only triggers for changes to docs/archive/completed-plans/INTEGRATED_IMPROVEMENT_PLAN.md which is an archived document. This workflow never runs on modern changes. Dead code creates maintenance burden and confusion.","recommendation":"Delete the workflow and document in commit message that phase validation is now handled elsewhere. Or update path to current planning documents. Or disable workflow explicitly with if: false.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/sync-readme.yml::fragile-retry","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/sync-readme.yml::fragile-retry","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/sync-readme.yml","line":64,"title":"CI gap: Sync README has fragile retry logic","description":"Lines 64-78 retry push 3 times with 5-second sleep. This is fragile: assumes conflicts resolve within 5s, doesn't backoff exponentially, and 3 retries may not be enough if multiple workflows queue up. Also mixes pull --rebase with retry which could compound conflicts.","recommendation":"Use exponential backoff: sleep $((i * i * 5)). Increase retries to 5. Use GitHub API to create commits instead of git push to avoid git-level races. Or use concurrency group to serialize commits.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/sync-readme.yml::no-verify-bypass","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/sync-readme.yml::no-verify-bypass","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/sync-readme.yml","line":57,"title":"CI gap: Sync README uses --no-verify bypassing hooks","description":"Line 57 uses --no-verify which bypasses pre-commit hooks. If hooks check for commit message format, trailing whitespace, or other issues, README sync commits could violate standards. This creates inconsistency in commit history.","recommendation":"Remove --no-verify unless there's specific reason (document reason if so). Let hooks run to ensure consistency. If hooks are too slow for automation, optimize hooks rather than skipping them.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/*::inconsistent-pinning","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/*::inconsistent-pinning","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/ci.yml","line":45,"title":"CI gap: Inconsistent GitHub Action version pinning","description":"Some workflows use SHA pinning for security (ci.yml line 45, backlog-enforcement.yml line 18) while others use semantic versions (auto-label-review-tier.yml line 29 uses @v46). Inconsistent pinning creates security gaps and makes supply chain attacks easier.","recommendation":"Adopt consistent pinning strategy: either pin all actions to SHAs with comments showing version, or use Dependabot to keep semantic versions updated. Document strategy in CONTRIBUTING.md.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/ci.yml::no-node-version-check","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/ci.yml::no-node-version-check","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/ci.yml","line":21,"title":"CI gap: No validation of Node.js version consistency","description":"Line 21 hardcodes Node 22 but there's no check that this matches package.json engines field or .nvmrc. Developers could use different Node version locally, causing 'works on my machine' issues. CI should enforce version consistency.","recommendation":"Add .nvmrc or package.json engines field with required Node version. Change workflow to read version from file: node-version-file: '.nvmrc'. Add pre-commit hook to check local Node version matches.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/docs-lint.yml::template-regex-brittle","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/docs-lint.yml::template-regex-brittle","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/docs-lint.yml","line":72,"title":"CI gap: Template file exclusion is brittle regex","description":"Lines 72-75 use regex patterns to skip template files: (TEMPLATE|_TEMPLATE)\\.md$. A file named TEMPLATE-proposal.md or my-TEMPLATE.md wouldn't match and would be incorrectly linted. Brittle pattern matching causes false positives.","recommendation":"Use more specific paths: if [[ $file =~ ^docs/templates/ ]]; then skip. Or maintain list of template files in config. Or add header to templates that linter detects. Make pattern matching more robust.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/docs-lint.yml::markdown-injection-incomplete","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/docs-lint.yml::markdown-injection-incomplete","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/docs-lint.yml","line":91,"title":"CI gap: Markdown injection sanitization incomplete","description":"Line 91 sanitizes ``` to prevent markdown injection but doesn't handle other vectors like [clickjacking](javascript:alert(1)) or HTML tags. Malicious or buggy docs could inject content into PR comments.","recommendation":"Use comprehensive sanitization library or render lint output as code block (which GitHub auto-escapes). Or limit output length and use GitHub's built-in comment rendering which sanitizes HTML.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/backlog-enforcement.yml::git-diff-merge-commits","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/backlog-enforcement.yml::git-diff-merge-commits","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/backlog-enforcement.yml","line":127,"title":"CI gap: Changed files detection could miss merge commits","description":"Line 127 uses git diff origin/$BASE...HEAD which could miss files in merge commits depending on git configuration. Three-dot diff shows changes since common ancestor, but merge commits might introduce changes not in either parent.","recommendation":"Use two-dot diff: git diff origin/$BASE..HEAD. Or use GitHub's changed files API which is authoritative. Or use tj-actions/changed-files action like other workflows for consistency.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/auto-label-review-tier.yml::comment-on-synchronize","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3b-ci-effectiveness.jsonl","original_id":"process::.github/workflows/auto-label-review-tier.yml::comment-on-synchronize","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/auto-label-review-tier.yml","line":186,"title":"CI gap: Tier comment spam on every synchronize event","description":"Line 186 only posts comment on opened/reopened, not synchronize. This is good for avoiding spam, but if tier changes due to new files added, PR author doesn't get notified. They might miss important tier escalation (e.g., tier 2 -> tier 4).","recommendation":"Post comment on synchronize only if tier label changed. Track previous tier in workflow state or by reading PR labels. Notify author when tier increases (escalation) but not when it stays same.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/audit-s0s1-validator.js::always-warn","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.claude/hooks/audit-s0s1-validator.js::always-warn","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/audit-s0s1-validator.js","line":21,"title":"Ineffective: audit-s0s1-validator defaults to WARN mode","description":"S0/S1 audit findings require strict validation but hook defaults to non-blocking WARN mode, allowing invalid findings to be written. The hook only blocks if AUDIT_S0S1_MODE=BLOCK is explicitly set, which is not the default.","recommendation":"Change default mode to BLOCK. Move to WARN mode as opt-out (AUDIT_S0S1_MODE=WARN) rather than opt-in blocking. Rationale: S0/S1 findings represent critical/high severity issues that must have proper verification_steps - this should be enforced by default.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/pattern-check.js::always-succeeds","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.claude/hooks/pattern-check.js::always-succeeds","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/pattern-check.js","line":221,"title":"Ineffective: pattern-check hook never blocks violations","description":"Pattern compliance hook always exits with 'ok' (line 221) even when violations are detected. Warnings are shown but violations don't prevent writes. This makes the hook informational noise rather than an enforcement mechanism.","recommendation":"Implement severity-based blocking: Block on critical (ðŸ”´) pattern violations, warn on non-critical. Add PATTERN_CHECK_MODE env var for gradual rollout (WARN/BLOCK), similar to audit-s0s1-validator but default to BLOCK for critical patterns. Update error message to indicate operation was blocked.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/pattern-check.js::small-file-bypass","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.claude/hooks/pattern-check.js::small-file-bypass","category":"process","severity":"S3","type":"process-gap","file":".claude/hooks/pattern-check.js","line":138,"title":"Ineffective: Small file bypass in pattern-check","description":"Pattern check skips files under 100 lines (line 138-165). This creates perverse incentive to keep files artificially small to avoid pattern checks. Critical patterns like hardcoded secrets or auth bypasses can exist in small files.","recommendation":"Remove small file bypass or make it configurable. Alternative: Run lightweight pattern checks on all files, reserve expensive checks for large files. Critical security patterns (secrets, auth) should always run regardless of file size. Document rationale if bypass is intentional performance optimization.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-commit::excessive-bypasses","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.husky/pre-commit::excessive-bypasses","category":"process","severity":"S2","type":"process-gap","file":".husky/pre-commit","line":46,"title":"Ineffective: Pre-commit bypass conditions too easy","description":"Pre-commit hook has 6 easy bypass conditions via environment variables: SKIP_TESTS, SKIP_CROSS_DOC_CHECK, SKIP_DOC_HEADER_CHECK, SKIP_AUDIT_VALIDATION, SKIP_DEBT_VALIDATION, SKIP_DOC_INDEX_CHECK. No audit trail or rate limiting. Developers can habitually bypass checks without visibility.","recommendation":"1. Log all bypasses to .claude/hooks/bypass-audit.jsonl with timestamp, user, check type, reason. 2. Require SKIP_REASON environment variable for all bypasses. 3. Add bypass budget: warn if same user bypasses >3 times in 7 days. 4. Make some checks non-bypassable (e.g., SKIP_AUDIT_VALIDATION for S0/S1). 5. Pre-push hook should fail if too many pre-commit bypasses detected.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-push::network-failure-success","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.husky/pre-push::network-failure-success","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-push","line":107,"title":"Ineffective: Network failures treated as success in pre-push","description":"npm audit check (lines 92-118) treats network errors as success - if npm registry is unreachable, check is skipped silently. Attacker with network control could bypass vulnerability detection. Same pattern could exist in other network-dependent checks.","recommendation":"Distinguish between 'no vulnerabilities' and 'cannot verify'. For network failures: 1. Show clear warning that check was skipped. 2. Log to bypass audit trail. 3. Consider making network checks blocking by default (fail closed), with ALLOW_NETWORK_SKIP=1 opt-out. 4. Cache last successful audit result and warn if stale (>7 days).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/check-write-requirements.js::post-task-not-enforced","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.claude/hooks/check-write-requirements.js::post-task-not-enforced","category":"process","severity":"S3","type":"process-gap","file":".claude/hooks/check-write-requirements.js","line":72,"title":"Ineffective: check-write-requirements POST-TASK not enforced","description":"Hook outputs 'POST-TASK: MUST run code-reviewer' and 'POST-TASK: SHOULD run test-engineer' but these are suggestions only. No enforcement mechanism. No tracking whether suggested agents actually ran. Messages become background noise that users ignore.","recommendation":"1. Track suggested agents in .claude/state/required-agents.json. 2. Pre-commit hook checks this file and warns/blocks if required agents not invoked. 3. Agent invocation hooks (track-agent-invocation.js) mark agents as completed. 4. MUST requirements block commit, SHOULD requirements warn. 5. Clear with AGENT_REVIEW_COMPLETE=1 override with reason logging.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/component-size-check.js::always-ok","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.claude/hooks/component-size-check.js::always-ok","category":"process","severity":"S3","type":"process-gap","file":".claude/hooks/component-size-check.js","line":142,"title":"Ineffective: component-size-check always succeeds","description":"Hook warns about oversized components (>300 lines) but always exits with 'ok' (line 142). Warnings are easily ignored. No escalation for egregiously large files (e.g., 1000+ lines). Size limits become suggestions rather than architectural constraints.","recommendation":"Implement tiered enforcement: 1. >300 lines: WARN (current behavior). 2. >500 lines: WARN with stronger message, add to tech debt tracker. 3. >750 lines: BLOCK unless ALLOW_LARGE_COMPONENT=1 with required explanation. 4. Track component sizes over time - warn if file growing rapidly (>50 lines/week). Form components keep higher limits but require Form suffix in filename.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/agent-trigger-enforcer.js::phase-not-implemented","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.claude/hooks/agent-trigger-enforcer.js::phase-not-implemented","category":"process","severity":"S3","type":"process-gap","file":".claude/hooks/agent-trigger-enforcer.js","line":295,"title":"Ineffective: agent-trigger-enforcer Phase 2/3 not implemented","description":"Hook shows phase transition notifications (lines 219-234) recommending upgrade to Phase 2 WARN or Phase 3 BLOCK modes, but these phases are not implemented. Hook always succeeds regardless of phase setting (line 295). Phase transitions are notification theater without actual behavior change.","recommendation":"Implement phase enforcement logic: Phase 1 (current): Suggest agents. Phase 2: Warn prominently if required agents not invoked, track to pending-reviews.json. Phase 3: Block Write/Edit if required agent not invoked this session, require SKIP_AGENT_CHECK=1 override with reason. Use state.phase to control behavior. Document phase upgrade process and rollback plan.","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/large-context-warning.js::warning-once","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.claude/hooks/large-context-warning.js::warning-once","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/large-context-warning.js","line":146,"title":"Ineffective: large-context-warning warningShown flag prevents repeated warnings","description":"Hook warns when >15 files read in session (line 146-153) but sets warningShown flag to prevent repeated warnings. After first warning, can read 50+ more files without additional feedback. Warning effectiveness degrades over long sessions. State resets after 30 minutes, allowing warning suppression.","recommendation":"Change warning strategy: 1. Warn at thresholds: 15, 30, 50, 100 files (escalating urgency). 2. Show file count in status bar if available. 3. After 30 files, suggest /save-context every 5 files. 4. After 50 files, warn that compaction likely soon. 5. Track context pressure score (files + total lines) not just file count. Don't use warningShown flag for suppression.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/check-remote-session-context.js::informational-only","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.claude/hooks/check-remote-session-context.js::informational-only","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/check-remote-session-context.js","line":32,"title":"Ineffective: check-remote-session-context always succeeds","description":"Hook detects when remote branches have newer session context (higher session counter) but always exits with 'ok' even when mismatch found. Warning shown (lines 163-176) but no enforcement. Developers can ignore and work on stale context, leading to lost work or duplicate sessions.","recommendation":"Make blocking when session counter difference >5 (likely working on very stale branch): 1. Show warning and suggest merge for small differences (1-2 sessions). 2. Block session start for large differences (>5 sessions) unless ALLOW_STALE_CONTEXT=1. 3. Offer auto-merge or checkout options. 4. Track if user repeatedly ignores warnings (>3 times in 14 days) and escalate to block. 5. continueOnError: true in settings keeps non-blocking for fetch failures.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/audit-s0s1-validator.js::parse-error-evasion","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.claude/hooks/audit-s0s1-validator.js::parse-error-evasion","category":"process","severity":"S3","type":"process-gap","file":".claude/hooks/audit-s0s1-validator.js","line":84,"title":"Ineffective: audit-s0s1-validator allows parse errors","description":"When JSONL parsing fails (lines 79-86), malformed lines are marked with _parseError but validation continues. If S0/S1 finding is in a malformed line, it escapes validation. Attacker or lazy developer could intentionally malform S0/S1 entries to bypass strict verification requirements.","recommendation":"Fail validation if any parse errors found in audit file: 1. Count parse errors during JSONL parsing. 2. If parseErrorCount > 0, block with message: 'JSONL file has malformed entries - fix syntax before committing'. 3. Show first 3 malformed lines with line numbers. 4. No override - proper JSON is non-negotiable for audit data. 5. Validate that all S0/S1 findings were successfully parsed (cross-check line count of S0/S1 severity strings vs parsed S0/S1 objects).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks/commit-tracker.js::continue-on-error","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.claude/hooks/commit-tracker.js::continue-on-error","category":"process","severity":"S2","type":"process-gap","file":".claude/settings.json","line":251,"title":"Ineffective: commit-tracker continueOnError makes failures silent","description":"commit-tracker.js has continueOnError: true (settings.json line 251) so failures are silent. If state files become corrupted or filesystem has issues, commit tracking silently breaks. Compaction resilience (Session #138) depends on this tracking but failures are invisible.","recommendation":"Keep continueOnError: true (appropriate for non-critical tracking) but add failure detection and alerting: 1. If commit-tracker fails 3+ times in session, show warning. 2. Log failures to .claude/hooks/hook-failures.jsonl with timestamp, hook name, error. 3. Session-start hook checks failure log and warns if recent failures (last 7 days). 4. Health check command to verify state files readable/writable. Document that continueOnError is intentional but monitored.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-push::skip-triggers","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-3a-hook-effectiveness.jsonl","original_id":"process::.husky/pre-push::skip-triggers","category":"process","severity":"S3","type":"process-gap","file":".husky/pre-push","line":123,"title":"Ineffective: Pre-push SKIP_TRIGGERS bypass has no budget","description":"SKIP_TRIGGERS=1 bypasses event-based trigger checks (lines 123-152) including security audits. Override logged to scripts/log-override.js but no enforcement of bypass budget. Developer could use SKIP_TRIGGERS=1 on every push to avoid all trigger-based checks including security scans.","recommendation":"Implement bypass budget for SKIP_TRIGGERS: 1. Track bypass frequency in .claude/state/bypass-budget.json (user, timestamp, reason). 2. Allow 3 bypasses per 7 days without warning. 3. Warn at 4-5 bypasses in 7 days. 4. Block at 6+ bypasses in 7 days unless BYPASS_BUDGET_OVERRIDE=1 (requires escalation approval or explicit reason). 5. Reset budget weekly. 6. Distinguish security trigger bypasses (stricter budget: 1 per 7 days) from other triggers.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/backlog-enforcement.yml::never-executes-backlog-check","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2c-unused.jsonl","original_id":"process::.github/workflows/backlog-enforcement.yml::never-executes-backlog-check","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/backlog-enforcement.yml","line":32,"title":"Never executes: AUDIT_FINDINGS_BACKLOG.md backlog check","description":"Workflow checks for docs/AUDIT_FINDINGS_BACKLOG.md which doesn't exist (archived in TDMS Phase 2). The check always exits early with 0 items, making the entire job pointless. Script check-backlog-health.js also references this non-existent file.","recommendation":"Remove backlog-health job from workflow or update to check docs/technical-debt/MASTER_DEBT.jsonl instead","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::package.json::never-executes-test-coverage-report","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2c-unused.jsonl","original_id":"process::package.json::never-executes-test-coverage-report","category":"process","severity":"S3","type":"process-gap","file":"package.json","line":13,"title":"Never executes: npm script test:coverage:report","description":"Script defined but never called in any workflow, hook, or automation. Only appears in audit documentation. Creates impression of functionality that doesn't execute.","recommendation":"Remove script from package.json or add to CI workflow if coverage reporting is needed","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::package.json::never-executes-learning-category","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2c-unused.jsonl","original_id":"process::package.json::never-executes-learning-category","category":"process","severity":"S3","type":"process-gap","file":"package.json","line":38,"title":"Never executes: npm script learning:category","description":"Script defined but never called in any workflow, hook, documentation, or automation. Likely vestigial from development.","recommendation":"Remove script from package.json","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::package.json::never-executes-learning-since","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2c-unused.jsonl","original_id":"process::package.json::never-executes-learning-since","category":"process","severity":"S3","type":"process-gap","file":"package.json","line":39,"title":"Never executes: npm script learning:since","description":"Script defined but only referenced in archived plan. Not called in any active workflow or automation.","recommendation":"Remove script from package.json","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::package.json::never-executes-config-validate","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2c-unused.jsonl","original_id":"process::package.json::never-executes-config-validate","category":"process","severity":"S3","type":"process-gap","file":"package.json","line":52,"title":"Never executes: npm script config:validate","description":"Inline script to validate JSON config files. Mentioned in DEVELOPMENT.md but never called in CI, pre-commit, or pre-push hooks. Manual-only scripts reduce reliability.","recommendation":"Add to pre-commit hook when scripts/config/*.json files are modified, or remove if configs are stable","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::package.json::never-executes-session-summary","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2c-unused.jsonl","original_id":"process::package.json::never-executes-session-summary","category":"process","severity":"S3","type":"process-gap","file":"package.json","line":41,"title":"Never executes: npm script session:summary","description":"Script passes --summary flag to log-session-activity.js but is only documented, never automated. Manual-only logging reduces effectiveness.","recommendation":"Remove script or add to session-end skill/workflow","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::package.json::never-executes-override-list","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2c-unused.jsonl","original_id":"process::package.json::never-executes-override-list","category":"process","severity":"S3","type":"process-gap","file":"package.json","line":43,"title":"Never executes: npm script override:list","description":"Lists overrides logged by log-override.js but never called in automation. Only manual execution means oversight data is not reviewed systematically.","recommendation":"Remove script or add to weekly/monthly automated review process","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/backlog-enforcement.yml::never-executes-master-branch","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2c-unused.jsonl","original_id":"process::.github/workflows/backlog-enforcement.yml::never-executes-master-branch","category":"process","severity":"S3","type":"process-gap","file":".github/workflows/backlog-enforcement.yml","line":5,"title":"Never executes: GitHub workflow master branch trigger","description":"Workflow configured to trigger on [main, master] but repo only has 'main' branch. The 'master' condition never fires, cluttering the trigger configuration.","recommendation":"Remove 'master' from branches array, keep only 'main'","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/validate-plan.yml::never-executes-validate-plan","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2c-unused.jsonl","original_id":"process::.github/workflows/validate-plan.yml::never-executes-validate-plan","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/validate-plan.yml","line":7,"title":"Never executes: validate-plan workflow for archived file","description":"Workflow triggers only when docs/archive/completed-plans/INTEGRATED_IMPROVEMENT_PLAN.md changes. This file is archived and unlikely to be modified, making the entire workflow dormant.","recommendation":"Archive/disable workflow or expand paths to include active plan documents","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-commit::eslint-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.husky/pre-commit::eslint-duplication","category":"process","severity":"S2","type":"process-gap","file":".husky/pre-commit","line":9,"title":"Duplicated: ESLint validation in pre-commit AND CI","description":"Running ESLint in both pre-commit and CI is redundant - pre-commit already blocks commits with errors, so CI check adds no value but doubles execution time","recommendation":"Keep ESLint blocking in pre-commit, make CI check non-blocking or remove it entirely. CI should only catch cases where pre-commit was bypassed","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-commit::pattern-check-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.husky/pre-commit::pattern-check-duplication","category":"process","severity":"S2","type":"process-gap","file":".husky/pre-commit","line":35,"title":"Duplicated: Pattern compliance check in pre-commit AND CI","description":"Pattern compliance runs in pre-commit (blocking all files) and CI (PR changed files only). This creates inconsistent behavior and wastes CI resources","recommendation":"Decide single source: either pre-commit blocks all or CI blocks changed files. Remove the other. Current setup means pre-commit catches issues CI might miss","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-commit::debt-validation-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.husky/pre-commit::debt-validation-duplication","category":"process","severity":"S2","type":"process-gap","file":".husky/pre-commit","line":248,"title":"Duplicated: Technical debt schema validation in pre-commit AND CI","description":"Tech debt schema validation runs in both pre-commit (blocking when MASTER_DEBT.jsonl staged) and CI. Double validation on same file wastes resources","recommendation":"Keep pre-commit validation (faster feedback). Make CI check non-blocking or remove it","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-commit::canon-validation-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.husky/pre-commit::canon-validation-duplication","category":"process","severity":"S2","type":"process-gap","file":".husky/pre-commit","line":97,"title":"Duplicated: CANON schema validation in pre-commit AND CI","description":"CANON validation runs in pre-commit (non-blocking warning) and CI (blocking). Inconsistent - same validation with different severity in two places","recommendation":"Unify validation behavior. Either pre-commit blocks or CI blocks, not both. Current state: pre-commit warns, CI blocks - confusing","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.husky/pre-commit::test-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.husky/pre-commit::test-duplication","category":"process","severity":"S1","type":"process-gap","file":".husky/pre-commit","line":59,"title":"Duplicated: Test execution in pre-commit AND CI","description":"Tests run in pre-commit (conditionally blocking) AND CI (always blocking). For config changes, tests run twice. Adds 30-60s to commit+push workflow","recommendation":"Pre-commit: quick smoke tests only for high-risk changes. CI: full test suite. Or skip pre-commit tests entirely, rely on CI","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".husky/pre-commit:59"},{"type":"description","detail":"Tests run in pre-commit (conditionally blocking) AND CI (always blocking). For config changes, tests run twice. Adds 30-60s to commit+push workflow"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks::path-validation-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.claude/hooks::path-validation-duplication","category":"process","severity":"S1","type":"process-gap","file":".claude/hooks/check-edit-requirements.js","line":18,"title":"Duplicated: Path validation logic across 16+ hooks","description":"16+ hooks duplicate identical path validation (security checks, traversal prevention, normalization). Total ~800 lines. Shared utility exists but unused. Bug fixes must be applied 16+ times","recommendation":"All hooks import and use validateFilePath() from scripts/lib/validate-paths.js. Delete inline duplicates","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".claude/hooks/check-edit-requirements.js:18"},{"type":"description","detail":"16+ hooks duplicate identical path validation (security checks, traversal prevention, normalization). Total ~800 lines. Shared utility exists but unused. Bug fixes must be applied 16+ times"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks::file-reading-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.claude/hooks::file-reading-duplication","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/firestore-write-block.js","line":114,"title":"Duplicated: File reading logic across 5 hooks","description":"5 hooks duplicate identical file reading logic: check if content provided, resolve path, readFileSync with error handling. ~50 lines duplicated","recommendation":"Create shared utility readFileForHook(filePath, projectDir) in scripts/lib/validate-paths.js. All hooks use it","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::error-handling-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::scripts::error-handling-duplication","category":"process","severity":"S1","type":"process-gap","file":"scripts/check-pattern-compliance.js","line":40,"title":"Duplicated: Error message extraction pattern across 42+ files","description":"Pattern 'err instanceof Error ? err.message : String(err)' appears in 42+ files. Shared sanitizeError() utility exists but unused. Inconsistent error handling","recommendation":"All scripts import and use sanitizeError() from scripts/lib/sanitize-error.js. Replace inline pattern","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":"scripts/check-pattern-compliance.js:40"},{"type":"description","detail":"Pattern 'err instanceof Error ? err.message : String(err)' appears in 42+ files. Shared sanitizeError() utility exists but unused. Inconsistent error handling"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::tty-color-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::scripts::tty-color-duplication","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-doc-headers.js","line":48,"title":"Duplicated: TTY-aware color code across 3+ scripts","description":"TTY color detection and color object creation duplicated across scripts. ~15 lines each. Inconsistent color usage","recommendation":"Create shared getColors() utility in scripts/lib/ that returns color object. All scripts import it","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks::security-validation-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.claude/hooks::security-validation-duplication","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/check-edit-requirements.js","line":40,"title":"Duplicated: Security validations across all hooks","description":"All hooks duplicate identical security validations: startsWith('-') check, multiline rejection, backslash normalization, absolute path blocking, traversal detection. Already covered by validateFilePath() but duplicated inline","recommendation":"Use validateFilePath() from validate-paths.js which includes all security checks. Delete inline duplicates","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::git-staged-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::scripts::git-staged-duplication","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-cross-doc-deps.js","line":83,"title":"Duplicated: Git staged files retrieval across scripts","description":"Multiple scripts duplicate git diff --cached --name-only logic with error handling. ~15 lines each","recommendation":"Create shared getStagedFiles() utility in scripts/lib/. All scripts import it","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts::config-loading-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::scripts::config-loading-duplication","category":"process","severity":"S3","type":"process-gap","file":"scripts/check-pattern-compliance.js","line":34,"title":"Duplicated: Config loading pattern across 17+ files","description":"17+ files duplicate loadConfig/loadConfigWithRegex pattern with try-catch and error message extraction. ~10 lines each. Shared utility exists but usage is inconsistent","recommendation":"Create loadConfigSafe() wrapper that handles try-catch internally. All scripts use one-liner: const config = loadConfigSafe('name')","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks::basedir-resolution-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.claude/hooks::basedir-resolution-duplication","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/check-edit-requirements.js","line":18,"title":"Duplicated: Base directory resolution across hooks","description":"All hooks duplicate baseDir resolution: path.resolve(process.env.CLAUDE_PROJECT_DIR || process.cwd()) plus security validation. ~10 lines each across 16+ hooks","recommendation":"Create getProjectDir() utility in validate-paths.js that returns validated project directory. All hooks use it","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks::json-parsing-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.claude/hooks::json-parsing-duplication","category":"process","severity":"S3","type":"process-gap","file":".claude/hooks/check-edit-requirements.js","line":20,"title":"Duplicated: JSON argument parsing across hooks","description":"All hooks duplicate JSON argument parsing: try parse, extract file_path/content, handle errors. ~15 lines each across 16+ hooks","recommendation":"Create parseHookArgs(arg) utility that returns {filePath, content, error}. All hooks use it","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks::extension-check-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.claude/hooks::extension-check-duplication","category":"process","severity":"S3","type":"process-gap","file":".claude/hooks/firestore-write-block.js","line":102,"title":"Duplicated: File extension checks across hooks","description":"Multiple hooks check file extensions with regex patterns. While patterns differ, the approach is duplicated","recommendation":"Create utility hasExtension(filePath, extensions) that accepts array of extensions. Standardize extension checking","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks::allowed-paths-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.claude/hooks::allowed-paths-duplication","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/firestore-write-block.js","line":30,"title":"Duplicated: ALLOWED_PATHS pattern matching across hooks","description":"Multiple hooks define ALLOWED_PATHS arrays and test with .some(pattern.test). Pattern is identical but lists differ. Hard to maintain consistency","recommendation":"Centralize path exemption rules in config file. Create isPathExempt(filePath, ruleSet) utility that loads from config","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.claude/hooks::edit-write-requirements-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::.claude/hooks::edit-write-requirements-duplication","category":"process","severity":"S1","type":"process-gap","file":".claude/hooks/check-edit-requirements.js","line":1,"title":"Duplicated: check-edit-requirements and check-write-requirements logic","description":"These two hooks are 95% identical. Same path validation (lines 18-63), same security checks, same file type detection. Only difference: priority order and specific messages. ~200 lines duplicated","recommendation":"Merge into single check-file-requirements.js that handles both Edit and Write. Pass operation type as parameter","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".claude/hooks/check-edit-requirements.js:1"},{"type":"description","detail":"These two hooks are 95% identical. Same path validation (lines 18-63), same security checks, same file type detection. Only difference: priority order and specific messages. ~200 lines duplicated"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::patterns::check-implementation-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::patterns::check-implementation-duplication","category":"process","severity":"S2","type":"process-gap","file":".claude/hooks/pattern-check.js","line":1,"title":"Duplicated: Pattern check implementation between hook and script","description":"Hook invokes script but also duplicates path validation, file size checks, and output formatting. Hook should be thin wrapper, not duplicate logic","recommendation":"Hook should only validate input and delegate to script. Remove duplicated validation logic from hook","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::validation::audit-schema-duplication","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2b-duplications.jsonl","original_id":"process::validation::audit-schema-duplication","category":"process","severity":"S2","type":"process-gap","file":"scripts/validate-audit.js","line":1,"title":"Duplicated: Validation between validate-audit.js and validate-schema.js","description":"Both validators load schema from config, validate JSONL structure, check required fields. Core validation logic is similar but implemented twice","recommendation":"Extract shared validateJsonlSchema(file, schemaConfig) utility. Both validators use it with different schema configs","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/seed-commit-log.js::orphaned","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::scripts/seed-commit-log.js::orphaned","category":"process","severity":"S2","type":"process-gap","file":"scripts/seed-commit-log.js","line":1,"title":"Orphaned: seed-commit-log.js","description":"Orphaned development/testing script increases maintenance burden without providing value","recommendation":"Remove if no longer needed, or document intended testing workflow","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/sync-claude-settings.js::orphaned","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::scripts/sync-claude-settings.js::orphaned","category":"process","severity":"S2","type":"process-gap","file":"scripts/sync-claude-settings.js","line":1,"title":"Orphaned: sync-claude-settings.js","description":"Orphaned setup/config script with no references in automation or documentation","recommendation":"Remove if obsolete, or document setup workflow that requires it","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/update-legacy-lines.js::orphaned","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::scripts/update-legacy-lines.js::orphaned","category":"process","severity":"S2","type":"process-gap","file":"scripts/update-legacy-lines.js","line":1,"title":"Orphaned: update-legacy-lines.js","description":"Code modernization script with no references - likely completed its purpose","recommendation":"Remove if migration is complete, or schedule remaining work","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/create-canonical-findings.js::orphaned","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::scripts/create-canonical-findings.js::orphaned","category":"process","severity":"S2","type":"process-gap","file":"scripts/create-canonical-findings.js","line":1,"title":"Orphaned: create-canonical-findings.js","description":"Listed as called by 'audit consolidation workflows' but no workflow actually calls it","recommendation":"Remove if obsolete, or integrate into actual workflow if needed","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/generate-detailed-sonar-report.js::orphaned","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::scripts/generate-detailed-sonar-report.js::orphaned","category":"process","severity":"S2","type":"process-gap","file":"scripts/generate-detailed-sonar-report.js","line":1,"title":"Orphaned: generate-detailed-sonar-report.js","description":"SonarCloud workflow exists but doesn't call this script - orphaned reporting tool","recommendation":"Remove if unused, or integrate into sonarcloud.yml if reports are needed","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/generate-placement-report.js::orphaned","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::scripts/generate-placement-report.js::orphaned","category":"process","severity":"S2","type":"process-gap","file":"scripts/generate-placement-report.js","line":1,"title":"Orphaned: generate-placement-report.js","description":"Listed as called by 'documentation workflows' but no workflow uses it","recommendation":"Remove if check-doc-placement.js supersedes it, or integrate if needed","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/migrate-existing-findings.js::orphaned","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::scripts/migrate-existing-findings.js::orphaned","category":"process","severity":"S2","type":"process-gap","file":"scripts/migrate-existing-findings.js","line":1,"title":"Orphaned: migrate-existing-findings.js","description":"One-time migration script no longer needed - technical debt","recommendation":"Remove after confirming migration is complete and stable","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/regenerate-findings-index.js::orphaned","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::scripts/regenerate-findings-index.js::orphaned","category":"process","severity":"S2","type":"process-gap","file":"scripts/regenerate-findings-index.js","line":1,"title":"Orphaned: regenerate-findings-index.js","description":"No references found in any workflow, skill, or npm script","recommendation":"Remove if aggregate-audit-findings.js handles this, or integrate if needed","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/verify-sonar-phase.js::orphaned","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::scripts/verify-sonar-phase.js::orphaned","category":"process","severity":"S2","type":"process-gap","file":"scripts/verify-sonar-phase.js","line":1,"title":"Orphaned: verify-sonar-phase.js","description":"Listed for SonarCloud workflows but sonarcloud.yml doesn't call it","recommendation":"Remove if SonarCloud integration doesn't need phase verification, or integrate if needed","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/auto-label-review-tier.yml::orphaned-script","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::.github/workflows/auto-label-review-tier.yml::orphaned-script","category":"process","severity":"S1","type":"process-gap","file":".github/workflows/auto-label-review-tier.yml","line":56,"title":"Orphaned: assign-review-tier.js disabled in workflow","description":"Script is commented out in auto-label-review-tier.yml with duplicate logic inline - creates confusion and maintenance burden","recommendation":"Either integrate assign-review-tier.js or remove it and keep inline logic","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".github/workflows/auto-label-review-tier.yml:56"},{"type":"description","detail":"Script is commented out in auto-label-review-tier.yml with duplicate logic inline - creates confusion and maintenance burden"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/*.ts::orphaned-migrations","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::scripts/*.ts::orphaned-migrations","category":"process","severity":"S2","type":"process-gap","file":"scripts/dedupe-quotes.ts","line":1,"title":"Orphaned: TypeScript migration scripts (14 files)","description":"14 TypeScript database seeding/migration scripts are not called by any automation - likely one-time use completed","recommendation":"Move to scripts/archive/ or scripts/migrations-archive/ to preserve history without clutter","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::package.json::npm-script-learning-category","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::package.json::npm-script-learning-category","category":"process","severity":"S2","type":"process-gap","file":"package.json","line":38,"title":"Orphaned npm script: learning:category","description":"npm script exists but is not documented, called by any workflow, or referenced in skills","recommendation":"Remove if experimental, or document purpose and integrate into learning workflow","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::package.json::npm-script-learning-since","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::package.json::npm-script-learning-since","category":"process","severity":"S2","type":"process-gap","file":"package.json","line":39,"title":"Orphaned npm script: learning:since","description":"npm script exists but is not documented, called by any workflow, or referenced in skills","recommendation":"Remove if experimental, or document purpose and integrate into learning workflow","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::package.json::npm-script-phase-complete-auto","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::package.json::npm-script-phase-complete-auto","category":"process","severity":"S2","type":"process-gap","file":"package.json","line":23,"title":"Orphaned npm script: phase:complete:auto","description":"npm script with --auto flag never called by workflows or skills - unclear purpose","recommendation":"Remove if not needed, or integrate into phase completion workflow","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/backlog-enforcement.yml::obsolete-file-check","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::.github/workflows/backlog-enforcement.yml::obsolete-file-check","category":"process","severity":"S1","type":"process-gap","file":".github/workflows/backlog-enforcement.yml","line":32,"title":"Obsolete: backlog-enforcement.yml checks deleted file","description":"Workflow checks AUDIT_FINDINGS_BACKLOG.md which was archived in TDMS Phase 2 (2026-01-31) - workflow gracefully skips but wastes CI resources","recommendation":"Remove backlog-health job or update to check docs/technical-debt/MASTER_DEBT.jsonl instead","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[{"type":"code_reference","detail":".github/workflows/backlog-enforcement.yml:32"},{"type":"description","detail":"Workflow checks AUDIT_FINDINGS_BACKLOG.md which was archived in TDMS Phase 2 (2026-01-31) - workflow gracefully skips but wastes CI resources"}],"sources":[],"merged_from":[]}
{"source_id":"audit:process::.github/workflows/validate-plan.yml::narrow-trigger","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::.github/workflows/validate-plan.yml::narrow-trigger","category":"process","severity":"S2","type":"process-gap","file":".github/workflows/validate-plan.yml","line":6,"title":"Narrow trigger: validate-plan.yml for specific archived file","description":"Workflow only triggers on PR changes to docs/archive/completed-plans/INTEGRATED_IMPROVEMENT_PLAN.md - extremely specific path unlikely to be modified","recommendation":"Broaden to any completed-plans/*.md or remove if INTEGRATED_IMPROVEMENT_PLAN is fully archived","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:process::scripts/manual-only::documentation-gap","source_file":"docs\\audits\\single-session\\process\\audit-2026-02-09\\stage-2a-orphans.jsonl","original_id":"process::scripts/manual-only::documentation-gap","category":"process","severity":"S3","type":"process-gap","file":"scripts/ai-review.js","line":1,"title":"Manual-only scripts not in automation","description":"ai-review.js and add-false-positive.js are manual command-line tools not in automation - need documentation of when to use them","recommendation":"Add to DEVELOPMENT.md under 'Manual Scripts' section with usage examples","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F001","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F001","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/ROADMAP.md","line":0,"title":"ROADMAP.md milestone tracking tables","description":"ROADMAP.md contains 270 lines of markdown tables tracking milestones, status, progress %, phases, and priorities. Large structure with frequent updates during sprint work.","recommendation":"Extract milestone data to milestones.jsonl with schema: {id, name, status, progress, phase, priority, items, relatedDocs}. Keep ROADMAP.md as narrative + embedded reference. Add npm script to sync ROADMAP.md from milestones.jsonl.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Sprint-focused: SESSION_CONTEXT.md reads ROADMAP.md to extract current priorities; scripts/velocity/track-session.js uses readFileSync on markdown; updated almost every session (Session #156 is latest); 270 table rows parsing fragile to formatting changes","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F002","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F002","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/docs/AUDIT_TRACKER.md","line":0,"title":"AUDIT_TRACKER.md audit log tables","description":"AUDIT_TRACKER.md contains 96 table rows tracking audit completion dates, commits covered, findings, and threshold reset status across 7 audit categories. Updated after each audit cycle (Session #143 shows 258 findings).","recommendation":"Extract to audits.jsonl with schema: {auditType, date, session, commitsCount, filesCount, findingsRaw, findingsUnique, findingsBySeverity, resetThreshold, relatedFile}. Create views by category and date for faster querying.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Directly queried by audit workflow; Expansion Evaluation Tracker cross-references audit status; Manual table maintenance error-prone (Session #116 showed date misalignment that had to be fixed); thresholds reset frequently","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F003","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F003","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/docs/EXPANSION_EVALUATION_TRACKER.md","line":0,"title":"EXPANSION_EVALUATION_TRACKER.md decision log with 280 ideas","description":"Tracks evaluation of ~280 expansion ideas across 21 modules (F1-F12, T1-T9) with decision logs, placement metadata, milestone assignments, insertion points, and relationships. Complex multi-session tracking document.","recommendation":"Extract decisions to decisions.jsonl with schema: {moduleId, ideaNum, title, decision, rationale, milestone, insertAfter, relationship, stagedDate, decidedInSession, tags}. Keep EXPANSION_EVALUATION_TRACKER.md as UI summary with aggregates. AI processes decisions.jsonl directly.","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"AI-intensive: /expansion-evaluation skill reads/writes this document; 280 items across 21 modules creates parsing overhead; Session #152 shows merging of IMS into TDMS â€” cross-document sync required; placement metadata mandatory for all items but stored inline as prose","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F004","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F004","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/docs/AUDIT_TRACKER.md","line":0,"title":"AUDIT_TRACKER.md threshold matrix and version history","description":"Contains 2 tables tracking threshold configuration (46 rows) and version history (12 rows). Thresholds reset after each audit; version history appended frequently.","recommendation":"Extract to thresholds.jsonl and versions.jsonl. Thresholds schema: {category, lastAudit, commitsSince, filesSince, triggerAt, resetScript}. Allows automation of threshold checks via npm scripts. Version history is append-only log.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"scripts/reset-audit-triggers.js manipulates thresholds but reads from markdown; Manual table updates (Session #143 added new category; Session #152 merged IMS); Version history now 260+ lines","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F005","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F005","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/docs/DOCUMENT_DEPENDENCIES.md","line":0,"title":"DOCUMENT_DEPENDENCIES.md sync status tracking","description":"Tracks 43 rows of template-instance relationships, sync status, and last-synced dates. Manual sync protocol requires regex parsing to detect drift (placeholder detection patterns).","recommendation":"Extract to doc-sync-status.jsonl with schema: {templatePath, instancePath, lastSynced, syncStatus, driftDetected, issuesFound}. Enhance scripts/check-document-sync.js to read/write this file. Enables automated validation with clear history.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"scripts/check-document-sync.js (Session #35) validates sync status using regex patterns; >90 day staleness checks require parsing markdown dates; Session #140 removed 6 archived instances â€” requires manual table cleanup; Session #144 shows drift detection complexity","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F006","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F006","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/SESSION_CONTEXT.md","line":0,"title":"SESSION_CONTEXT.md quick status table","description":"Contains 8-row status tracking table (lines 118-128) for track status (Operational Visibility, Track A/B/C, GRAND PLAN, milestones). Updated frequently at session start/end.","recommendation":"Extract to session-status.jsonl with schema: {item, status, progress, percent, lastUpdated, session}. Keep SESSION_CONTEXT.md <300 lines (current instruction). Allow automated session startup to read/write status atomically.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"SESSION_CONTEXT.md reads during every session (Session #156 latest); scripts/velocity/track-session.js explicitly reads this file via readFileSync; frequent updates (Session-to-session); 8 rows Ã— 156 sessions = massive token overhead","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F007","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F007","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/docs/DOCUMENT_DEPENDENCIES.md","line":0,"title":"DOCUMENT_DEPENDENCIES.md cross-document update triggers matrix","description":"48-row trigger matrix (lines 309-342) mapping document changes to dependent documents. Used to coordinate cross-document sync but stored as markdown table.","recommendation":"Extract to doc-triggers.jsonl with schema: {sourceDoc, triggerCondition, targetDocs[], reason, enforced, blockingLevel}. Integrate into pre-commit hook to auto-warn on dependent document changes (currently 'Manual' enforcement in 35/48 rows).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Session #152 shows IMS merged into TDMS requiring widespread trigger updates; only 2/48 triggers are âœ… BLOCK enforced; 35 are Manual enforcement = high error rate; Pre-commit hook currently doesn't use this matrix","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F008","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F008","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/ROADMAP.md","line":0,"title":"ROADMAP.md detailed milestone specifications embedded","description":"ROADMAP.md contains narrative milestone details mixed with data (status, progress %, priority, items count). Detailed specifications for M1.5-M10 hardcoded inline with dependency references.","recommendation":"Create milestones-detail.jsonl with schema: {milestoneId, phase, priority, description, itemCount, dependencies[], risks[], successCriteria[], dependencies}. ROADMAP.md becomes curated narrative referencing milestones.jsonl data via embedded tables regenerated from data.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Session #151 shows ROADMAP_FUTURE.md manually split from ROADMAP.md; ROADMAP.md is 3164 lines, heavily cross-referenced; Expansion Evaluation Tracker references placement in ROADMAP but can't parse it reliably; dependencies[].blockedBy structure suggests structured data buried in prose","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F009","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F009","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/docs/PR_WORKFLOW_CHECKLIST.md","line":0,"title":"PR_WORKFLOW_CHECKLIST.md version history table","description":"Version history table (lines 443-447) tracking 3 versions with dates and changes. Append-only log stored as markdown.","recommendation":"Extract to doc-versions.jsonl (append-only) with schema: {doc, version, date, changes, author}. All documentation version history consolidates to single source. Enables automated change tracking per document.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Every document has version history table (PR_WORKFLOW_CHECKLIST.md, DOCUMENT_DEPENDENCIES.md, SESSION_CONTEXT.md, PLAN_MAP.md show pattern); Manual maintenance; Lines 240-260+ in many docs; Could be centralized","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F010","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F010","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/docs/EXPANSION_EVALUATION_TRACKER.md","line":0,"title":"EXPANSION_EVALUATION_TRACKER.md command reference table","description":"Command reference table (lines 81-99) documenting 11 commands with descriptions. Static reference data embedded as markdown.","recommendation":"Extract to .claude/skills/expansion-evaluation/commands.jsonl with schema: {command, subcommands[], parameters, description, example}. Centralize skill command documentation to single source; .claude/COMMAND_REFERENCE.md references it.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Duplicated in COMMAND_REFERENCE.md (Session #140 update adds expansion-evaluation skill); Single source of truth in skills/ directory reduces cross-sync burden; Complex sub-command taxonomy (accept/defer/reject/merge/discuss)","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F011","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F011","category":"code-quality","severity":"S3","type":"code-smell","file":"/home/user/sonash-v0/SESSION_CONTEXT.md","line":0,"title":"SESSION_CONTEXT.md recent session summaries","description":"3 session summaries (lines 82-112) capturing major work completed. Append-only archive with manual rotation to SESSION_HISTORY.md every session.","recommendation":"Extract to session-summaries.jsonl with schema: {sessionNum, date, title, workItems[], impact, nextSteps}. SESSION_CONTEXT.md reads latest 3 automatically. /session-end skill handles archive rotation. Centralize all session history.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Manual rotation (Session #149 shows archival from SESSION_CONTEXT to SESSION_HISTORY); 3-session limit = rotating window; /session-end explicitly handles this archival step; Could be automated","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F012","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F012","category":"code-quality","severity":"S3","type":"code-smell","file":"/home/user/sonash-v0/docs/PLAN_MAP.md","line":0,"title":"PLAN_MAP.md version history table","description":"Version history table (lines 228-242) tracking 14 versions. Append-only log with dates, descriptions, authors.","recommendation":"Extract to doc-versions.jsonl (see OPT-F009 for consolidation). PLAN_MAP.md can remove version history table entirely.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"All 30+ markdown files have version history tables; Centralized version tracking reduces duplication; Could be auto-generated from git history","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F013","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F013","category":"code-quality","severity":"S3","type":"code-smell","file":"/home/user/sonash-v0/docs/AI_REVIEW_LEARNINGS_LOG.md","line":0,"title":"AI_REVIEW_LEARNINGS_LOG.md large append-only learning journal (317KB)","description":"Large document (1587 lines, 317KB) containing review-level learning entries. Used by /pr-review skill but stored as markdown prose without structured extraction.","recommendation":"Parallel reviews.jsonl with schema: {reviewNum, prNum, date, category, finding, rationale, pattern, linkedDocs[], status}. Keep AI_REVIEW_LEARNINGS_LOG.md as human-readable summary; skill reads from structured reviews.jsonl for automation.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Scripts/add-false-positive.js references AI_REVIEW_LEARNINGS_LOG.md#review-NNN format in comments; Session #142 shows learnings being added; /pr-review skill processes learnings; 317KB markdown = massive token cost when read into context; Narrative prose harder to aggregate/cross-reference than structured data","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-F014","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-F014","category":"code-quality","severity":"S3","type":"code-smell","file":"/home/user/sonash-v0/ROADMAP_LOG.md","line":0,"title":"ROADMAP_LOG.md completed items history (31KB, 1,129 lines)","description":"Archive of completed milestones and features in markdown. Append-only log with dates, completion summaries, version history.","recommendation":"Create roadmap-history.jsonl (append-only) with schema: {completedItem, completedDate, milestone, summary, session, commits, impact}. ROADMAP_LOG.md becomes human-readable narrative referencing jsonl data. Allows automated timeline generation.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"1129 lines of append-only history; Used as reference for context but rarely updated (append-only); Could be auto-generated from milestone completion dates + commit history","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-S001","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-S001","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/scripts/ai-review.js","line":0,"title":"ai-review.js - Unused AI review prompt applicator","description":"Script applies specialized AI review prompts to different artifact types. Despite having security features (sensitive file detection), it is never invoked from package.json, workflows, hooks, skills, or documentation. Only historical archive references exist in REVIEWS_42-60.md.","recommendation":"Either add npm script entry or remove. If planned for future AI-driven reviews, add to DEVELOPMENT.md with clear invocation pattern.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Not found in package.json, .claude/settings.json, .github/workflows/*.yml, .husky/*, or skill references. Only archive mentions of past security reviews.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-S002","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-S002","category":"code-quality","severity":"S3","type":"code-smell","file":"/home/user/sonash-v0/scripts/check-review-triggers.sh","line":0,"title":"check-review-triggers.sh - Dead shell script for multi-AI triggers","description":"Bash script checks git commit/file counts to determine if code review triggers are active. Appears to duplicate functionality of check-triggers.js (which IS referenced in package.json). Script prints colored output to console but has no output targets.","recommendation":"Remove in favor of the active check-triggers.js npm script. Consolidate shell logic into Node.js for consistency.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"check-triggers.js is referenced in package.json as 'npm run triggers:check'. This .sh file is never called from any configuration or workflow.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-S003","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-S003","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/scripts/create-canonical-findings.js","line":0,"title":"create-canonical-findings.js - Unused canonical findings generator","description":"Script converts net-new findings from docs/aggregation/net-new-findings.jsonl into canonical format with ROADMAP placement mapping. References deprecated ROADMAP_INTEGRATION logic. 340 lines with clear purpose but never executed.","recommendation":"Add to package.json scripts if part of audit pipeline (e.g., 'npm run canon:create'), or consolidate into aggregate-audit-findings.js workflow. Currently blocks on file that depends on aggregate-audit-findings.js output.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Referenced in no npm scripts, workflows, or hooks. audit/validate-audit-integration.js mentions 'aggregate-audit-findings.js' but not create-canonical-findings.js.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-S004","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-S004","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/scripts/generate-pending-alerts.js","line":0,"title":"generate-pending-alerts.js - Unused session-start alert generator","description":"Script scans AI_REVIEW_LEARNINGS_LOG.md and AUDIT_FINDINGS_BACKLOG.md for DEFERRED/S1+ items to write pending-alerts.json for Claude session start. Functional but never called. Depends on legacy backlog files (now archived to MASTER_DEBT.jsonl).","recommendation":"Either add to session-start hook or remove. If needed for alerts, update file paths to use MASTER_DEBT.jsonl instead of archived AUDIT_FINDINGS_BACKLOG.md.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Not referenced in package.json, workflows, or .claude/settings.json hooks. Mentioned in skill markdown but not as actual command invocation.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-S005","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-S005","category":"code-quality","severity":"S3","type":"code-smell","file":"/home/user/sonash-v0/scripts/generate-placement-report.js","line":0,"title":"generate-placement-report.js - Unused roadmap placement suggester","description":"Reads net-new findings from docs/aggregation/net-new-findings.jsonl and generates roadmap placement suggestions in NET_NEW_ROADMAP_PLACEMENT.md. Depends on external file that may or may not exist. Complements create-canonical-findings.js.","recommendation":"Remove or consolidate into create-canonical-findings.js. If placement suggestions are valuable, add explicit npm script and document workflow.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"No references in any configuration, npm scripts, workflows, hooks, or documentation. Depends on aggregation pipeline output but has no consumer.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-S006","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-S006","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/scripts/migrate-existing-findings.js","line":0,"title":"migrate-existing-findings.js - Unused legacy findings migration tool","description":"One-time migration script to move ROADMAP findings to canonical location (docs/audits/canonical/MASTER_FINDINGS.jsonl). References obsolete file structures (REFACTOR_BACKLOG.md). Clear one-off purpose but permanently left in codebase.","recommendation":"Move to docs/archive/scripts/ or remove entirely. One-time migration utilities should be archived after successful migration to prevent accidental re-runs.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Never called from npm scripts, workflows, or hooks. Clear 'Session #116' marker indicates one-time use.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-S007","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-S007","category":"code-quality","severity":"S3","type":"code-smell","file":"/home/user/sonash-v0/scripts/redeploy-admin-dashboard.sh","line":0,"title":"redeploy-admin-dashboard.sh - Firebase deployment helper for admin functions","description":"Shell script for deleting and redeploying admin dashboard Cloud Functions (adminHealthCheck, adminGetDashboardStats) to ensure clean App Check configuration. Hardcoded to 'sonash-app' Firebase project. 20 lines.","recommendation":"Either parameterize Firebase project and add to npm scripts, or document as manual troubleshooting tool with clear prerequisites.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Not in package.json, workflows, .husky/, or any configuration. Appears to be manual maintenance script left behind after deployment debugging.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-S008","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-S008","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/scripts/regenerate-findings-index.js","line":0,"title":"regenerate-findings-index.js - Unused canonical findings index rebuilder","description":"Reads MASTER_FINDINGS.jsonl and regenerates MASTER_FINDINGS_INDEX.md with severity/category grouping. ~80 lines with clear, single-purpose functionality. Related to Session #116 canonicalization but never invoked in workflows.","recommendation":"Add to npm scripts (e.g., 'npm run canon:index') or consolidate into debt management workflow. If MASTER_FINDINGS.jsonl is auto-updated, add regenerate-findings-index to post-intake hooks.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"No references in package.json npm scripts, workflows, or hooks. Appears to be utility for canonical audit system that was never wired into automation.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-S009","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-S009","category":"code-quality","severity":"S3","type":"code-smell","file":"/home/user/sonash-v0/scripts/seed-commit-log.js","line":0,"title":"seed-commit-log.js - One-time commit log backfill utility","description":"One-time script to initialize .claude/state/commit-log.jsonl with recent git commits for commit tracking system. Self-documenting with clear 'only run once' semantics. Part of Session #138 state persistence setup.","recommendation":"Document as one-time setup script in DEVELOPMENT.md or move to docs/setup/. No need to run again unless --force is used. Safe to leave but consider archiving.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Not in npm scripts, workflows, or hooks. Self-identifies as 'one-time backfill utility' with guard against double-execution.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-S010","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-S010","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/scripts/sync-claude-settings.js","line":0,"title":"sync-claude-settings.js - Unused Claude Code settings synchronization utility","description":"Syncs Claude Code settings between local ~/.claude/ and repository .claude/ for cross-platform portability. 508 lines with --export, --import, --diff flags. REFERENCED IN DOCUMENTATION but never called from automation. Listed in DEVELOPMENT.md table but not in package.json scripts. Users expected to run manually.","recommendation":"Add npm script entry 'npm run settings:sync' with sensible default (--diff or --import). Or remove from codebase if manual invocation via 'node scripts/...' is sufficient. Currently confusing: documented but not in package.json scripts.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Referenced 3 times in DEVELOPMENT.md (lines mentioning '--import', '--export', '--diff') but NOT in package.json npm scripts section or any workflow/hook. Creates user confusion about how to invoke.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-S011","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-S011","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/scripts/update-legacy-lines.js","line":0,"title":"update-legacy-lines.js - Unused legacy findings line number updater","description":"One-time script to backfill line numbers for legacy findings (DEDUP-XXXX, EFF-XXX, PERF-XXX, M-series IDs). Hardcoded mapping of 50+ finding IDs to file locations. Session #116 artifact, 100+ lines.","recommendation":"Move to docs/archive/scripts/ or remove. One-time canonicalization utility with no ongoing use. If legacy findings are still updated, consolidate logic into update script.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Not in any npm scripts, workflows, hooks. Hardcoded data suggests one-time execution completed in Session #116.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H001","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H001","category":"code-quality","severity":"S1","type":"code-smell","file":".claude/settings.json","line":0,"title":"Write/Edit/MultiEdit share 8 redundant hooks (~530ms overhead)","description":"Write, Edit, and MultiEdit tools execute nearly identical hook chains with 8 shared hooks (pattern-check, component-size-check, firestore-write-block, test-mocking-validator, app-check-validator, typescript-strict-check, repository-pattern-check, agent-trigger-enforcer). Only difference: Write uses check-write-requirements (test-first) vs Edit/MultiEdit use check-edit-requirements (security-first). This causes ~530ms latency on every file write operation.","recommendation":"Consolidate to single validation pipeline with tool-aware conditional logic. Create unified hook that checks file type once, then runs all applicable validators in sequence. Reuse file content across validators instead of re-reading.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Analyzed hook array definitions in settings.json. Each of Write/Edit/MultiEdit has identical command chains except first hook. Total cumulative time: pattern-check(100ms) + typescript-strict(100ms) + agent-trigger(100ms) + repository-pattern(80ms) + app-check(60ms) + firestore-block(50ms) + test-mocking(30ms) + component-size(10ms) = ~530ms per operation","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H002","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H002","category":"code-quality","severity":"S0","type":"code-smell","file":".claude/hooks/pattern-check.js","line":0,"title":"pattern-check.js spawns subprocess every Write/Edit/MultiEdit (~100ms latency)","description":"pattern-check.js runs spawnSync to execute scripts/check-pattern-compliance.js on EVERY Write/Edit/MultiEdit operation, adding 100ms latency. File has pre-filters (only runs on .js/.ts/.sh files >100 lines) but subprocess spawn is expensive. Non-blocking warning only.","recommendation":"Cache pattern checker state or use native regex validation for common patterns. Add session-level cache of recently-checked files. Skip re-validation on already-validated files this session. Consider moving to async hook if possible.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"pattern-check.js lines 181-186 show spawnSync call. Combined with agent-trigger-enforcer (100ms) and typescript-strict-check (100ms), every Write/Edit now incurs 300ms just from these three hooks","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H003","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H003","category":"code-quality","severity":"S1","type":"code-smell","file":".claude/hooks/check-write-requirements.js, .claude/hooks/check-edit-requirements.js","line":0,"title":"check-write-requirements vs check-edit-requirements duplication","description":"Two nearly identical hooks with different priority orders. check-write-requirements (Write only) checks tests first, then security. check-edit-requirements (Edit/MultiEdit) checks security first. Both perform identical path validation and file classification but in different order. Adds 10ms overhead per tool call.","recommendation":"Merge into single check-requirements.js hook that accepts tool parameter and applies correct priority. Use conditional branches for tool-specific behavior rather than duplicate files.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Both files implement identical security validation (lines 40-68 in both), identical file parsing (lines 20-32 in both), but differ only in priority order (security-first vs test-first)","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H004","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H004","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/agent-trigger-enforcer.js","line":0,"title":"agent-trigger-enforcer.js runs on every code file edit with double-write overhead","description":"Runs on every Write/Edit/MultiEdit (100ms latency) to track file modifications and suggest agents. Loads config from disk every invocation. Uses double-write pattern: writes .claude/hooks/.agent-trigger-state.json AND potentially .claude/state/pending-reviews.json. Only matters for code files matching AGENT_TRIGGERS patterns, but runs unconditionally then does pattern matching.","recommendation":"Add early check for applicable agent patterns BEFORE reading state file. Cache AGENT_TRIGGERS config at hook initialization. Batch state writes: consolidate review queue write into single atomic operation instead of separate writeJson call.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Loads config at lines 34-43, reads state at line 189, writes at line 214, then reads/writes review queue at lines 258-321. For non-code files, could skip entire state read/write cycle.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H005","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H005","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/large-context-warning.js, .claude/hooks/auto-save-context.js, .claude/hooks/compaction-handoff.js","line":0,"title":"Three Read hooks contend for .context-tracking-state.json state file","description":"large-context-warning.js, auto-save-context.js, and compaction-handoff.js all read/write .claude/hooks/.context-tracking-state.json on every Read operation. Multiple hooks doing atomic writes (temp file + rename) creates contention and potential race conditions. State resets if >30 minutes old (large-context-warning line 98).","recommendation":"Consolidate context tracking into single hook that: 1) reads state once, 2) updates all needed metrics, 3) writes once. Or create state-utils-based shared handler. Have large-context-warning handle reset logic for all three hooks.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"large-context-warning: lines 92-131 (read/write cycle), auto-save-context: lines 107-113 (read filesRead), compaction-handoff: lines 242-248 (read filesRead). All three do their own atomic write operations","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H006","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H006","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/state/, .claude/hooks/","line":0,"title":"State file sprawl across .claude/hooks/ and .claude/state/","description":"10 separate state files created by hooks: 6 in .claude/hooks/ (.session-state, .context-tracking-state, .auto-save-state, .handoff-state, .commit-tracker-state, .agent-trigger-state) and 4 in .claude/state/ (handoff.json, pending-reviews.json, commit-log.jsonl, agent-invocations.jsonl). Some files only read/written by single hook (redundant). No schema documentation.","recommendation":"Consolidate single-use state files. Establish clear naming convention: .claude/state/ for session-surviving data (compaction-safe), .claude/hooks/ for ephemeral session state. Create state-schema.md documenting all state files, their consumers, and retention policy.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"agent-trigger-enforcer uses 2 state files, compaction-handoff reads 4 files, commit-tracker uses 2. Many files created but single-purposed (e.g., .auto-save-state.json only used by auto-save-context.js)","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H007","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H007","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/component-size-check.js, firestore-write-block.js, test-mocking-validator.js, app-check-validator.js, typescript-strict-check.js, repository-pattern-check.js","line":0,"title":"validation hooks could share file read to reduce I/O","description":"Multiple validation hooks on Write/Edit/MultiEdit independently read file content: component-size-check, firestore-write-block, test-mocking-validator, app-check-validator, typescript-strict-check, repository-pattern-check all do fs.readFileSync on same file. Pattern-check also reads file. No caching between hooks.","recommendation":"Implement hook file cache: pass file content through hook environment or temp file. Or create composite validator hook that reads file once and runs all applicable validators. At minimum, share path validation results.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"6 hooks Ã— fs.readFileSync per Write/Edit = significant I/O overhead. Each does similar path containment checks, file existence checks, and security validation before actual logic.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H008","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H008","category":"code-quality","severity":"S1","type":"code-smell","file":".claude/hooks/audit-s0s1-validator.js","line":0,"title":"audit-s0s1-validator.js only triggers on audit files but runs full validation on Write","description":"audit-s0s1-validator.js (lines 216-218) only processes docs/audits/*.jsonl files but is registered on PostToolUse Write hook, meaning it runs on every file write operation. Does JSON parsing of file argument and path matching checks (~10-15ms wasted) on ~99% of writes that don't match audit file pattern.","recommendation":"Create separate matcher condition in settings.json for audit files (similar to how Read, Bash, Task have matchers). Only register this hook when file path matches audit pattern. Or use very fast path-only check at top of hook before any other processing.","effort":"E3","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Hook does quick exit at line 218 for non-audit files, but all overhead (path normalization, argument parsing, linting checks) still happens. Runs on ~99% of writes unnecessarily.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H009","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H009","category":"code-quality","severity":"S1","type":"code-smell","file":".claude/hooks/large-context-warning.js, .claude/hooks/auto-save-context.js, .claude/hooks/compaction-handoff.js","line":0,"title":"Read hooks have no execution order guarantee - context tracking race condition potential","description":"large-context-warning, auto-save-context, and compaction-handoff run in arbitrary order on every Read. They all touch .context-tracking-state.json. If one fails/timeout, state could be corrupt or get reset by another hook. Large-context-warning resets state if >30min old (line 98), which could wipe data another hook just wrote.","recommendation":"Establish clear execution order (orchestrate in single hook or specify in settings). Have one hook be state authority that handles reset logic. Others read-only or use state-utils shared module. Document dependency order.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"large-context-warning lines 96-103 show potential reset. If large-context-warning runs after auto-save-context in some scenarios, it could reset context that was just saved. Atomic writes don't prevent logical race conditions.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H010","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H010","category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/commit-tracker.js","line":0,"title":"Bash hook (commit-tracker) does fast-path regex on every command but mostly bails (~1ms overhead)","description":"commit-tracker.js runs on every Bash command but uses COMMIT_COMMAND_REGEX (line 50) to bail out fast for non-commit commands. Estimated ~1ms overhead per Bash call for non-commit operations (regex + argument parsing). Non-critical since bail-out is very fast.","recommendation":"Move regex check to settings matcher instead of hook code. Create specific 'git commit' matcher rather than catching all Bash and bailing out. Would eliminate overhead entirely.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"commit-tracker.js line 157: 'if (!COMMIT_COMMAND_REGEX.test(command))' - regex test happens on every bash call even though mostly bails. Matcher in settings could avoid hook invocation entirely.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H011","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H011","category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/check-write-requirements.js, .claude/hooks/check-edit-requirements.js","line":0,"title":"check-write-requirements and check-edit-requirements could be unified with tool parameter","description":"Both hooks (106 lines each) contain ~95% identical code. Only difference is keyword priority order (test-first vs security-first). Could merge into single hook that accepts tool name parameter and applies correct priority matrix.","recommendation":"Create single unified hook: check-file-requirements.js. Accept tool parameter. Build priority array based on tool. Use loop through priority array instead of separate if-blocks for each type. Saves 1 file and reduces maintenance burden.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Lines 70-103 in both files show priority order is only real difference. Everything else (argument parsing, path validation, filename extraction) is identical.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H012","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H012","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/session-start.js","line":0,"title":"session-start.js does heavy work at SessionStart (builds, installs, checks) - could be async","description":"session-start.js (561 lines) runs synchronously at SessionStart and does heavy I/O: npm install, npm ci, build commands (up to 120s timeout each), pattern checks, consolidation checks, TDMS metrics. Blocks session start if any step hangs. SessionStart is synchronous hook context.","recommendation":"Split session-start into critical-path (check secrets, load alerts) and background tasks (npm install, build, pattern check). Document which steps are blocking vs can be skipped if time-constrained. Consider if builds should happen at all during hook vs on-demand.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"session-start.js total execution: npm ci (potentially 30-60s) + functions npm ci (30-60s) + build (60s) + pattern check (5-10s) + consolidation (5-10s) + TDMS check (5s) = potential 150+ seconds blocking session start","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H013","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H013","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/auto-save-context.js","line":0,"title":"Auto-save-context hook reads 4 files per Read operation to find recent decisions","description":"auto-save-context.js reads SESSION_DECISIONS.md and processes it with regex on every Read operation to extract recent decisions (line 119-138). File could be large. Regex does full content scan. Only saves if thresholds exceeded (~15min interval), but reads every time.","recommendation":"Cache recent decisions with timestamp. Only re-read SESSION_DECISIONS.md if modification time changed. Store cache in .auto-save-state.json. Reduces I/O from every Read to periodic (file-change-based).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"getRecentDecisions() is called line 253 on every Read hook invocation, but auto-save only happens per SAVE_INTERVAL_MINUTES (line 24 = 15min). Doing full file read/parse on every invocation is wasteful.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H014","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H014","category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/typescript-strict-check.js","line":0,"title":"TypeScript strict check runs on every Write/Edit/MultiEdit but skips for small files","description":"typescript-strict-check.js (100ms cost) runs on every Write/Edit/MultiEdit but skips test files (.test.ts), .d.ts files, and scripts/ directory. Still does full file read even for pre-filtered file types.","recommendation":"Move file read behind secondary filter check. Check file extension AND skip patterns before fs.readFileSync. Add cache for recently-checked files so repeated edits don't re-scan.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Hook reads file for every .ts/.tsx file (line 108 fs.readFileSync) even though some will be skipped. Could add extension-based cache key to avoid re-reading same file multiple times in one session.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-H015","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-H015","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/session-start.js","line":0,"title":"SessionStart hook chain is sequential with no parallelization","description":"session-start.js runs npm install, npm ci, builds, and checks sequentially (lines 312-410). Each command waits for previous. Node installations could run in parallel. Build after install requires wait, but pattern check and consolidation check could run in parallel.","recommendation":"Use Promise.all() for parallelizable steps: npm install root + functions can run in parallel. Pattern check + consolidation check can run parallel. Build must wait for install. Requires async/await refactor but could cut session startup time by 40-50%.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Current sequential: npm (30-60s) + functions npm (30-60s) + build (60s) = 150s+ total. Parallel: Math.max(npm, functions npm) + build = ~100-120s potential savings.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K001","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K001","category":"code-quality","severity":"S1","type":"code-smell","file":".claude/skills/audit-*/SKILL.md","line":0,"title":"Multiple audit skills with overlapping domain coverage","description":"audit-code, audit-security, audit-performance, audit-refactoring, audit-documentation, audit-process, and audit-engineering-productivity are individual audits that are also orchestrated together via audit-comprehensive. audit-enhancements is an enhancement-specific audit that partially overlaps functionality.","recommendation":"Clarify the relationship: are individual audits meant to be standalone or only called from comprehensive? Consider consolidating orchestration logic or creating a more explicit hierarchy to prevent confusion about when to use which.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"audit-comprehensive explicitly calls audit-code, audit-security, audit-performance, audit-refactoring, audit-documentation, audit-process. Each individual audit also stands alone with full execution instructions. audit-enhancements does similar discovery but for enhancements vs fixes.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K002","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K002","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/docs-sync/SKILL.md, docs-update/SKILL.md, doc-optimizer/SKILL.md","line":0,"title":"Skill overlap: docs-sync vs docs-update vs doc-optimizer","description":"Three skills appear to address documentation updates/synchronization. Without reading full content (files are empty/minimal in search), unclear if there's genuine overlap or specialization.","recommendation":"Verify the specialization of each: if docs-sync is for keeping docs in sync, docs-update is for content changes, and doc-optimizer is for quality, that's clear. If not, consolidate.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Names suggest overlapping responsibility: sync, update, optimize all deal with documentation. Audit-documentation skill also modifies docs.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K003","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K003","category":"code-quality","severity":"S1","type":"code-smell","file":".claude/skills/senior-*/SKILL.md","line":0,"title":"Senior specialist skills vs audit-comprehensive coverage","description":"6 senior-* skills (architect, backend, devops, frontend, fullstack, qa) may duplicate functionality already covered by audit-comprehensive and its 7 domain audits.","recommendation":"Clarify: Are senior-* skills meant for code review/consultation vs automated audits? If they're for architectural review, they should have distinct scope from audits. If they're redundant, consider consolidating review logic into audits.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Audit skills find issues; senior-* skills presumably review code. Possible that senior-* are deprecated or should be renamed to 'review-*' instead.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K004","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K004","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/[multiple]/SKILL.md","line":0,"title":"Empty or minimal SKILL.md descriptions","description":"Many skills have description fields with empty or placeholder values: developer-growth-analysis, excel-analysis, find-skills, frontend-design, etc.","recommendation":"Complete all description fields. Even a one-sentence summary helps users understand when to use each skill. This is a quick fix with high clarity benefit.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Grep output shows 'description:' with nothing after it for many skills. Users cannot understand purpose from skill index.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K005","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K005","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/add-deferred-debt/SKILL.md, add-manual-debt/SKILL.md, verify-technical-debt/SKILL.md, sync-sonarcloud-debt/SKILL.md","line":0,"title":"Skill overlap: debt tracking skills (add-deferred-debt, add-manual-debt, verify-technical-debt, sync-sonarcloud-debt)","description":"Four skills handle technical debt in different ways. add-deferred-debt and add-manual-debt both add debt but from different sources. verify-technical-debt verifies. sync-sonarcloud-debt imports from SonarCloud.","recommendation":"These may be appropriately specialized by source (deferred from PR, manual discovery, SonarCloud import, verification workflow). Verify each has distinct trigger criteria. If overlapping, consolidate intake logic.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"All route to MASTER_DEBT.jsonl via intake-audit.js. Overlap is in the 'add' step; deferred vs manual is a valid distinction by source.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K006","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K006","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/pr-review/SKILL.md, pr-retro/SKILL.md, code-reviewer/SKILL.md, requesting-code-review/SKILL.md","line":0,"title":"Skill overlap: PR and review skills (pr-review, pr-retro, code-reviewer, requesting-code-review)","description":"Four skills handle pull requests and code review. pr-review is a comprehensive review skill. pr-retro does retrospective analysis. code-reviewer is a general code review tool. requesting-code-review initiates code review.","recommendation":"Clarify workflow: Does requesting-code-review trigger pr-review? Is pr-retro a post-PR analysis? Is code-reviewer the general tool used by pr-review? Map the relationships clearly.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"All four deal with pull requests, code quality, and review processes. Names suggest a workflow but hierarchy is unclear.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K007","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K007","category":"code-quality","severity":"S1","type":"code-smell","file":".claude/skills/audit-validation-wrapper/SKILL.md","line":0,"title":"Unclear relationship: audit-validation-wrapper vs audit-comprehensive","description":"audit-validation-wrapper 'wraps' audit-comprehensive but also serves as a standalone alternative. Uncertain if it should always run with comprehensive or be independent.","recommendation":"Clarify: Is audit-validation-wrapper the authoritative entry point (call audit-comprehensive from it) or should they be independent? Current doc creates ambiguity about which to invoke.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"SKILL.md shows both 'Manual Integration' and 'Automated Wrapper (Recommended)' workflows, suggesting uncertainty about the primary flow.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K008","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K008","category":"code-quality","severity":"S3","type":"code-smell","file":".claude/skills/using-superpowers/SKILL.md, task-next/SKILL.md, validate-claude-folder/SKILL.md","line":0,"title":"Placeholder skills with minimal utility (using-superpowers, task-next, validate-claude-folder)","description":"Several skills appear to be utilities or helpers with limited scope. using-superpowers presumably activates advanced features. task-next shows next task. validate-claude-folder validates .claude folder.","recommendation":"These are fine as utilities, but ensure they're documented clearly. Consider a separate 'utilities' section in skill index to avoid confusion with major skills.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"These skills are helper functions, not major workflows. They're useful but shouldn't be at the same priority level as audit-comprehensive.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K009","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K009","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/skill-creator/SKILL.md, find-skills/SKILL.md","line":0,"title":"Potential deprecation: skill-creator, skill-related skills in favor of ai-native workflow","description":"skill-creator and find-skills manage skills as artifacts. But skills may be better managed through evolving the codebase or agent instructions rather than as files.","recommendation":"Evaluate whether skill-creator and find-skills can be automated or replaced by better skill discovery mechanisms. If skill management is becoming a bottleneck, consider refactoring.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"skill-creator and find-skills exist to manage skills manually. If this is frequently needed, it suggests the skill system itself needs improvement.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K010","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K010","category":"code-quality","severity":"S3","type":"code-smell","file":".claude/skills/session-begin/SKILL.md, session-end/SKILL.md, checkpoint/SKILL.md","line":0,"title":"Session lifecycle skills (session-begin, session-end, checkpoint) could be consolidated","description":"Three skills handle session management: session-begin starts sessions, session-end ends sessions, checkpoint saves state. These are tightly coupled.","recommendation":"Consider consolidating into a single 'session-management' skill with subcommands for begin/end/checkpoint. This reduces skill index clutter.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"These three skills are procedurally coupled (always used in session-begin â†’ work â†’ checkpoint â†’ session-end order). One unified skill would reduce invocation overhead.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K011","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K011","category":"code-quality","severity":"S1","type":"code-smell","file":".claude/skills/audit-*/SKILL.md","line":0,"title":"Audit skill explosion: 9 audit-related skills with inconsistent parameterization","description":"audit-code, audit-security, audit-performance, audit-refactoring, audit-documentation, audit-process, audit-engineering-productivity, audit-comprehensive, audit-enhancements + audit-validation-wrapper and audit-aggregator = 11 audit-related skills","recommendation":"Evaluate consolidation: Could these be one 'audit' skill with domain parameters (e.g., /audit --domain code,security,performance)? Or are the individual skills appropriately specialized for standalone use? Current approach creates high cognitive load.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"57 total skills, 11 are audits (19% of skill portfolio). This is a significant portion devoted to one pattern. Consolidation could free up skill slots for new functionality.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K012","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K012","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/audit-enhancements/SKILL.md, audit-process/SKILL.md","line":0,"title":"Skill naming inconsistency: enhancement vs audit-enhancements, process vs audit-process","description":"Most audits are prefixed 'audit-' (audit-code, audit-security) but audit-enhancements uses 'enhancement' alone. audit-process is standalone but part of audit-comprehensive.","recommendation":"Standardize: Either all audit skills are audit-* (audit-enhancements, audit-process are correct) or domain names are standalone (code, security, enhancements are correct). Pick one pattern.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"audit-code, audit-security, audit-performance, audit-refactoring, audit-documentation, audit-process, audit-engineering-productivity use 'audit-' prefix. audit-enhancements doesn't. Inconsistent.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K013","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K013","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/content-research-writer/SKILL.md, market-research-reports/SKILL.md, ux-researcher-designer/SKILL.md","line":0,"title":"Marketing-focused skills in technical codebase (content-research-writer, market-research-reports, ux-researcher-designer)","description":"Three skills are marketing/content/design focused and may be out of scope for a technical codebase audit toolkit.","recommendation":"Verify these belong in the codebase. If the project is web-based with UX concerns, keep them. If purely backend/technical, consider moving to separate skill repository or marking as optional.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"content-research-writer helps with articles/newsletters. market-research-reports does market research. ux-researcher-designer does UX research. These are orthogonal to code audits, debt tracking, and technical skills.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K014","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K014","category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/*/SKILL.md","line":0,"title":"Undefined skill descriptions create discovery problem (50% of skills missing meaningful descriptions)","description":"At least 25+ skills have empty description fields, preventing skill discovery via /find-skills or skill index searches.","recommendation":"Populate all descriptions with 1-2 sentence summaries. This is foundational metadata for a 57-skill portfolio.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Grep output shows 'description:' with no value for ~50% of skills. Users cannot discover these skills via automated tools.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-K015","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-K015","category":"code-quality","severity":"S1","type":"code-smell","file":".claude/skills/artifacts-builder/SKILL.md, markitdown/SKILL.md, mcp-builder/SKILL.md, expansion-evaluation/SKILL.md, systematic-debugging/SKILL.md","line":0,"title":"Deprecated or unclear purpose: artifacts-builder, markitdown, mcp-builder, expansion-evaluation, systematic-debugging","description":"Five skills have unclear or potentially deprecated purposes: artifacts-builder (build HTML artifacts), markitdown (convert markdown), mcp-builder (build MCP servers), expansion-evaluation (evaluate expansions), systematic-debugging (debug systematically).","recommendation":"Audit usage: Are these actually used? If artifacts-builder is for creating Claude artifacts, verify it's still needed post-api-changes. If MCP-builder is for custom MCP servers, verify it's maintained. Mark deprecated if not actively used.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"markitdown description mentions 'convert to markdown' but unclear from what. expansion-evaluation is unexplained. systematic-debugging has no description. These need clarity or deprecation notices.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-A001","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-A001","category":"code-quality","severity":"S1","type":"code-smell","file":"docs/**/*.md, SESSION_CONTEXT.md, AUDIT_TRACKER.md, PLAN_MAP.md (and 54 others)","line":0,"title":"57 documents with duplicate/inconsistent AI Instructions sections","description":"Audit found 57 markdown files containing 'AI Instructions' sections. Many are duplicated across docs with similar guidance (e.g., APPCHECK_SETUP.md, RECAPTCHA_REMOVAL_GUIDE.md both have App Check-specific instructions that could be consolidated). Each section ranges from ~100-800 chars (25-200 tokens). Total estimated token waste: ~4,500+ tokens per session load.","recommendation":"Consolidate AI Instructions into 3-4 canonical sections in claude.md: (1) Universal Meta-Instructions (placement, format, scope), (2) Per-Document-Type Instructions (planning docs vs process docs vs setup guides), (3) Safety/Compliance Checks. Reference from docs using 'See claude.md Section X.Y' instead of duplicating.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"grep found 57 files with '^## AI Instructions' pattern. Sample: APPCHECK_SETUP.md (line 318/343=92% down), DOCUMENTATION_STANDARDS.md (line 57/864=6% up), INCIDENT_RESPONSE.md (line 283/300=94% down). DOCUMENTATION_STANDARDS.md itself mandates 'AI Instructions MUST be near top' but 30% of docs violate this.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-A002","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-A002","category":"code-quality","severity":"S1","type":"code-smell","file":"docs/DOCUMENTATION_STANDARDS.md, docs/APPCHECK_SETUP.md, docs/INCIDENT_RESPONSE.md, docs/SONARCLOUD_CLEANUP_RUNBOOK.md (and others)","line":0,"title":"AI Instructions placement violates own DOCUMENTATION_STANDARDS","description":"DOCUMENTATION_STANDARDS.md (line 57-67) explicitly states: 'AI Instructions section MUST be near the top (after title and metadata)' with rationale that 'LLMs read top-to-bottom; instructions at bottom are often missed'. However, at least 8 sampled docs place AI Instructions at 90%+ through the document (APPCHECK_SETUP.md at line 318/343, INCIDENT_RESPONSE.md at line 283/300), making instructions invisible to most LLM attention spans.","recommendation":"(1) Add check to pre-commit hook: validate AI Instructions at line <50 for docs >100 lines. (2) Audit all 57 docs for placement violation. (3) Move misplaced instructions to line 20-30 (post-metadata).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_STANDARDS.md lines 63-66 state placement rule. Grep shows APPCHECK_SETUP.md, INCIDENT_RESPONSE.md, SONARCLOUD_CLEANUP_RUNBOOK.md all >90% through their files. Compare: MCP_SETUP.md correctly places at line 16/178 (9%), DOCUMENTATION_STANDARDS.md at line 57/864 (6%).","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-A003","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-A003","category":"code-quality","severity":"S2","type":"code-smell","file":"docs/APPCHECK_SETUP.md, docs/RECAPTCHA_REMOVAL_GUIDE.md, claude.md","line":0,"title":"Redundant App Check instructions across 3 documents","description":"App Check setup guidance appears in 3 separate files: (1) APPCHECK_SETUP.md (tier 3 setup guide), (2) RECAPTCHA_REMOVAL_GUIDE.md (tier 3 procedure), (3) claude.md Section 2 (security rule about App Check Required). Each has overlapping instructions: check env var â†’ verify config â†’ test deployment. Estimated 3-5 tokens wasted per session per doc.","recommendation":"Keep only one authoritative guide (APPCHECK_SETUP.md). Remove App Check instructions from RECAPTCHA_REMOVAL_GUIDE.md and cross-reference. Simplify claude.md Section 2 security rule to: 'App Check Required - see APPCHECK_SETUP.md for troubleshooting' (reduce from current advisory to single pointer).","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Sampled 3 App Check docs: all have setup guidance. Cross-references not present. DOCUMENTATION_STANDARDS.md at line 200-207 defines 'sync triggers' but no sync trigger exists for App Check instructions.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-A004","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-A004","category":"code-quality","severity":"S2","type":"code-smell","file":"docs/SESSION_HISTORY.md (lines 25-50), docs/AI_REVIEW_LEARNINGS_LOG.md (lines with 'IMSâ†’TDMS'), docs/AUDIT_TRACKER.md (version 2.6-2.7)","line":0,"title":"Outdated IMS references in SESSION_HISTORY.md and AI_REVIEW_LEARNINGS_LOG.md","description":"Session #152 (2026-02-12) merged IMS (Improvement Management System) into TDMS (Technical Debt Management System). Multiple documents contain historical references to 'IMS', 'docs/improvements/', and 'MASTER_IMPROVEMENTS.jsonl' that are now obsolete. While SESSION_HISTORY.md correctly documents the merge in version history, the operational guidance hasn't been updated. Risk: AI agents reading outdated session logs may attempt to reference deleted systems.","recommendation":"(1) Update SESSION_HISTORY.md entries for Sessions #150-152 to add post-merge callout: [IMS DEPRECATED - see Session #152 for merger details]. (2) Update AI Instructions in AUDIT_TRACKER.md to reference TDMS exclusively. (3) Add one-time cross-reference in SESSION_DECISIONS.md: 'IMS merged to TDMS in Session #152 - historical refs may be outdated'.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"docs/improvements/ does not exist (verified). AUDIT_TRACKER.md version history shows merge (2.6â†’2.7). PLAN_MAP.md v2.1 explicitly documents removal. Historical session logs still reference old system.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-A006","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-A006","category":"code-quality","severity":"S3","type":"code-smell","file":"docs/*.md (all 57 files)","line":0,"title":"Inconsistent AI Instructions format across document tiers","description":"Different document tiers use different AI Instructions formats: (1) Tier 2 docs (setup guides) use numbered lists with actions (e.g., APPCHECK_SETUP.md), (2) Tier 4 docs (references) use bullet lists with guidelines (e.g., REVIEW_POLICY_QUICK_REF.md), (3) Some use 'When X do Y' format, others use 'Rules are', others use 'Do not'. DOCUMENTATION_STANDARDS.md doesn't prescribe a format for AI Instructions subsections. This inconsistency costs ~50-100 tokens per doc as LLMs must re-parse format variations.","recommendation":"Add Section 5 to DOCUMENTATION_STANDARDS.md: 'AI Instructions Format Spec'. Prescribe: (1) Use bulleted list (not numbered) for generality, (2) Start each bullet with imperative verb: 'Check/Validate/Review/When/Always/Never', (3) Keep <15 words per bullet, (4) Max 5-8 bullets per section, (5) Example: '- Check file status before modifying\n- Validate with npm run docs:check\n- Reference ROADMAP.md for sync triggers'.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Sampled 5 files show 5 different formats. No DOCUMENTATION_STANDARDS.md section covers format. Format variations add parsing overhead without benefit.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-A007","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-A007","category":"code-quality","severity":"S3","type":"code-smell","file":"docs/TESTING_PLAN.md, docs/SONARCLOUD_CLEANUP_RUNBOOK.md, docs/RECAPTCHA_REMOVAL_GUIDE.md","line":0,"title":"Disconnected AI Instructions from actual automation - TESTING_PLAN.md example","description":"TESTING_PLAN.md AI Instructions (line ~1050) say 'First run npm test to check automated test status'. But there is no automatic test runner integrated into the documented workflow. The instruction assumes synchronous CLI execution, while modern SoNash uses async /test-suite skill. The instruction is not obsolete but outdated (pre-skill era). Similar issue in 3-5 other docs.","recommendation":"(1) Update TESTING_PLAN.md AI Instructions: 'Use /test-suite skill (recommended) or npm test for local verification'. (2) Add 'Preferred Automation' subsection to claude.md Section 7 that lists 10 core skills with their trigger contexts. (3) Mark AI Instructions with version (['Updated Session #X'] to help AI understand freshness.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"TESTING_PLAN.md created ~Session #110, /test-suite skill created Session #141. Session #141 notes say 'comprehensive testing suite'. But TESTING_PLAN.md AI Instructions never updated to recommend new skill. Compare: TESTING_USER_MANUAL.md (Session #141) correctly references /test-suite.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-A008","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-A008","category":"code-quality","severity":"S3","type":"code-smell","file":"SESSION_CONTEXT.md, AI_WORKFLOW.md, ROADMAP.md","line":0,"title":"AI Instructions in SESSION_CONTEXT.md creates circular reference risk","description":"SESSION_CONTEXT.md AI Instructions (line 7-43) prescribe 6 steps for session start and 5 rules for updating, concluding with 'Check Navigation' section that links to ROADMAP.md, AI_WORKFLOW.md, etc. These links then loop back and reference SESSION_CONTEXT.md. While not contradictory, this creates a 3-hop dependency (SESSION_CONTEXT â†’ AI_WORKFLOW â†’ ROADMAP â†’ SESSION_CONTEXT) that costs tokens every session load and makes updates harder.","recommendation":"(1) Establish clear hierarchy: ROADMAP.md is canonical, SESSION_CONTEXT.md is current-state read-only snapshot, AI_WORKFLOW.md is navigation. (2) Simplify SESSION_CONTEXT.md AI Instructions to 3 points: 'Read first â†’ check Next Session Goals â†’ see ROADMAP for priorities'. Remove internal navigation links. (3) Move navigation links to ai-navigation section in claude.md.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"SESSION_CONTEXT.md lines 45-51 (Navigation section) create 3-hop links. Actual reading order is SESSION_CONTEXT â†’ ROADMAP (for goals) â†’ ROADMAP_LOG (for history) â†’ back to SESSION_CONTEXT (for current session). Circular dependency confirmed.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-A009","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-A009","category":"code-quality","severity":"S0","type":"code-smell","file":"All 57 files with AI Instructions sections","line":0,"title":"CRITICAL: 57 separate AI Instructions sections = ~4,500+ unnecessary tokens per session","description":"COMPREHENSIVE FINDING: The project maintains 57 separate 'AI Instructions' sections across documentation. At ~80 tokens per section average (320-1,200 chars / 4 chars per token), this represents ~4,500+ tokens loaded per session. However, claudee.md (118 lines, ~30 tokens) is the only file that MUST be loaded every session for AI context. The other 56 sections are redundantly included in documentation that's selectively read. Token waste estimate: 90% of AI Instructions are never referenced in a given session (only 1-2 docs are read per session on average). This violates the project's own principle (claude.md line 10-13: 'Kept minimal (~120 lines) to reduce token waste').","recommendation":"IMMEDIATE: Audit all 57 docs by tier. Keep AI Instructions ONLY in Tier 1-2 docs (ROADMAP.md, SESSION_CONTEXT.md, DOCUMENTATION_STANDARDS.md, claude.md, maybe 10-15 others). Remove from Tier 3-4 docs entirely. Instead, add single 'See claude.md Section X for instructions' pointer. Create centralized 'Per-Document AI Instructions' section in claude.md that covers all document types and scenarios. Estimated savings: ~4,000 tokens per session (90% reduction in AI Instructions bloat).","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Grep found 57 files. Manual sampling of 5-10 files shows average ~120-200 chars per AI Instructions section. Extrapolate: 57 * 150 chars = 8,550 chars = ~2,137 tokens. HOWEVER: if ~30 are Tier 3-4 (reference only), then unnecessary load = 30 * 150 = 4,500 chars = ~1,125 tokens PER SESSION. Conservative estimate with weighted averaging: ~4,500 tokens waste due to over-distribution of AI Instructions.","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-P001","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-P001","category":"code-quality","severity":"S0","type":"code-smell","file":"/home/user/sonash-v0/.claude/hooks/commit-tracker.js","line":0,"title":"SESSION_CONTEXT.md Session Counter Regex in 5 hooks","description":"Multiple hooks parse **Current Session Count**: using regex /\\*\\*Current Session Count\\*\\*:\\s*(\\d+)/ to extract session numbers. This pattern appears in: commit-tracker.js (line 130), compaction-handoff.js (line 165), check-remote-session-context.js (line 63), pre-compaction-save.js (line 149), and generate-pending-alerts.js (indirectly via check-session-gaps.js line 77). If markdown formatting changes (spacing, capitalization, or bold marker), session counter extraction fails silently, breaking compaction tracking.","recommendation":"Extract to shared utility function with fallback parsing strategies (e.g., case-insensitive, flexible whitespace). Add validation to ensure extracted value is numeric.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"grep found pattern in 5 files; commit-tracker.js line 130 is primary; compaction-handoff.js line 165, check-remote-session-context.js line 63, pre-compaction-save.js line 149, check-session-gaps.js line 77 depend on same pattern","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-P002","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-P002","category":"code-quality","severity":"S0","type":"code-smell","file":"/home/user/sonash-v0/.claude/hooks/auto-save-context.js","line":0,"title":"SESSION_DECISIONS.md Decision Block Regex in auto-save-context.js","description":"auto-save-context.js (line 124) parses recent decisions using regex /^### \\[(\\d{4}-\\d{2}-\\d{2})\\] - (.+?)\\n([\\s\\S]*?)(?=^### \\[|^## |$)/gm. This pattern is brittle: requires exact date format (YYYY-MM-DD), assumes specific header structure with dash separator, and expects newline immediately after header. Any markdown reformatting (adding spaces, changing header level, altering date format) breaks decision extraction, losing session context that survives compaction.","recommendation":"Use more flexible regex with optional whitespace: /^###\\s+\\[(\\d{4}-\\d{2}-\\d{2})\\]\\s*-\\s*(.+?)\\n([\\s\\S]*?)(?=^###|$)/gm. Add guard against empty date/title fields.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Line 124 in auto-save-context.js; used to save session context before compaction; runs every file read (PostToolUse hook)","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-P003","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-P003","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/scripts/check-backlog-health.js","line":0,"title":"AUDIT_FINDINGS_BACKLOG.md markdown parsing in check-backlog-health.js","description":"check-backlog-health.js (lines 160, 179-191) parses backlog items using split(/^### \\[/gm) and multiple regex patterns for severity, status, and CANON-ID extraction. The parser expects exact format: ### [Category] Item Name with specific field formatting (**Severity**: S[0-3], **Status**: PENDING|IN_PROGRESS|DONE|DEFERRED, **CANON-ID**: CANON-\\d+). If fields are reordered, spacing changes, or status values vary, parsing fails to identify critical S0 items, creating blocker detection failures.","recommendation":"Use multiline section parsing with flexible field detection. Support YAML-like format detection. Add fallback to unstructured search for severity keywords. Validate that all required fields exist before processing item.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Lines 160-195 in check-backlog-health.js; called during pre-push hook validation; failure to detect S0 items blocks push but might be bypassed if parsing fails silently","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-P004","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-P004","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/scripts/update-readme-status.js","line":0,"title":"Markdown table parsing in update-readme-status.js with pipe delimiter fragility","description":"update-readme-status.js (lines 156-171, 203) parses markdown tables from ROADMAP.md using regex and split('|'). Line 203 uses split('|') on table rows without escaping for pipe characters in cell content. Pattern expects exact alignment (| header | ... |) but doesn't handle pipes in cell values. Line 156 regex /## ðŸ“Š Milestones Overview[\\s\\S]{0,5000}?\\n\\|[^\\n]+\\| requires specific heading emoji; if changed, parsing fails. Table parsing is fragile to format changes.","recommendation":"Detect table by structure (| + separator row) instead of emoji. Parse cells carefully: use regex match on table row including escaped pipes (\\\\|). Validate column count matches header. Consider using markdown parsing library instead of regex.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Lines 156-171 for table detection, line 203 for cell splitting; used in update-readme-status.js which updates README from ROADMAP","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-P005","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-P005","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/scripts/check-session-gaps.js","line":0,"title":"check-session-gaps.js relies on hardcoded Session Context markdown format","description":"check-session-gaps.js (line 59) extracts documented sessions using /\\*\\*Session #(\\d+) Summary\\*\\*/g and current counter using /\\*\\*Current Session Count\\*\\*:\\s*(\\d+)/ (line 77). The pattern is hardcoded to expect exact bold formatting. If SESSION_CONTEXT.md switches to different header style (# Session N Summary, or [Session N]), gap detection fails, missing undocumented sessions and potentially allowing orphaned commits.","recommendation":"Create markdown format abstraction. Support multiple formats: **Session #N Summary**, # Session N Summary, [Session N]. Use case-insensitive matching. Add logging for parsing failures.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Lines 59 and 77 in check-session-gaps.js; used to detect missing session documentation; failure means orphaned commits go untracked","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-P006","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-P006","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/scripts/aggregate-audit-findings.js","line":0,"title":"aggregate-audit-findings.js markdown parsing fragility in multiple functions","description":"aggregate-audit-findings.js has four fragile markdown parsing points: (1) parseMarkdownBacklog (line 322) splits by '|' without handling escaped pipes, assumes fixed column positions; (2) parseAuditFindingsBacklog (line 377) regex /^### \\[([^\\]]+)\\] / expects exact bracket format; (3) section.match() for CANON-ID, Severity, Effort (lines 384-386) expect exact bold format and field names. If backlog markdown structure changesâ€”field reordering, different ID format, heading changesâ€”parsing silently skips items or extracts wrong data.","recommendation":"Implement markdown AST parsing or use remark/unified. Support format variants: ### [Cat] or ### Cat or # Cat/Item. For tables, parse cell-aware (handle escaped pipes). Add schema validation for required fields per item type.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Lines 279-348 (markdown table parsing), 354-399 (section-based parsing); used in master aggregation for all findings; cascading failures affect entire audit pipeline","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-P007","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-P007","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/scripts/generate-pending-alerts.js","line":0,"title":"generate-pending-alerts.js fragile DEFERRED item extraction","description":"generate-pending-alerts.js (lines 45-86) parses AI_REVIEW_LEARNINGS_LOG.md for DEFERRED items using multiple regexes: /\\*\\*DEFERRED \\(Review #(\\d+)\\)\\*\\*/ (line 45), /\\*\\*DEFERRED \\((\\d+)\\):\\*\\*/ (line 67-68). The patterns expect exact formatting with specific punctuation placement (colon inside or outside asterisks). If deferred items are reformatted, patterns fail, causing DEFERRED alerts to be missed entirelyâ€”losing visibility of deferred work.","recommendation":"Unify DEFERRED detection with flexible regex: /\\*\\*DEFERRED\\s*\\(\\s*(?:Review\\s*#)?(\\d+)\\s*\\):\\**/i. Test both colon-inside and colon-outside variants. Add logging for detected vs missed items.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Lines 45 and 67-68 in generate-pending-alerts.js; used to surface deferred review items in session start alerts; silent failure means alerts lost","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-P008","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-P008","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/scripts/check-roadmap-health.js","line":0,"title":"check-roadmap-health.js version parsing regex scoped to section only","description":"check-roadmap-health.js (line 56) extracts version header using /\\*\\*Document Version:\\*\\*\\s*(\\d+\\.\\d+)/ but this appears once per document. Line 69 further restricts version history parsing to a specific section using /##\\s*ðŸ—“ï¸?\\s*Version History[\\s\\S]*?(?=\\r?\\n##\\s|\\r?\\n---\\s*$|$)/. While scoped (good), the patterns are fragile: emoji optional but section name hardcoded; line number regex assumes specific format. If document structure changes, version validation silently passes or fails incorrectly.","recommendation":"Make section header detection case-insensitive and emoji-agnostic: /##\\s*version\\s*history/i. Support version formats: X.Y, X.Y.Z, vX.Y. Validate at least one version entry exists in history section.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Lines 56, 61-69 in check-roadmap-health.js; health check runs before pushing; silent failures bypass validation","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-P009","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-P009","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/scripts/multi-ai/normalize-format.js","line":0,"title":"Multi-AI normalize-format.js markdown table detection and parsing","description":"normalize-format.js (line 194) detects markdown tables using /\\|[^\\n]+\\|\\s*\\n\\|[-:\\s|]+\\|\\s*\\n/, but this pattern assumes: (1) first row has pipes, (2) separator row immediately follows, (3) separator contains only hyphens/colons/pipes/spaces. If table has leading/trailing spaces, multiple blank lines, or non-standard separators, detection fails. Line 485, 507 split by '|' without handling escaped pipes in content. Multi-AI uses this to parse ANY audit input format; parsing failures cascade to all downstream processing.","recommendation":"Add flexible whitespace: /\\|[^\\n]*\\|\\s*\\n\\s*\\|\\s*[-:\\s|]+\\s*\\|/. Detect separator row by content: all cells match /^\\s*[-:]+\\s*$/. Parse cells aware of escaped pipes: split by unescaped pipes only. Test with real audit markdown examples.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Lines 194-196 for detection, 485/507 for cell splitting; this module handles format detection for ALL multi-AI aggregation; failures affect entire pipeline","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-P010","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-P010","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/scripts/verify-sonar-phase.js","line":0,"title":"verify-sonar-phase.js hardcoded security section header detection","description":"verify-sonar-phase.js (line 163) detects security section by exact string match: if (line.startsWith('## ðŸ”’ Security Hotspots')). If emoji changes, section name varies, or spacing differs, detection fails, causing hotspots to be miscategorized as regular issues. Line 203 regex /### ðŸ“ `([^`]+)`/ for file sections also hardcodes emoji; changing it breaks file grouping. Phase verification is used to validate sonar fixes; parsing failure allows miscategorized issues to slip through.","recommendation":"Use regex for section detection: /##\\s+(?:ðŸ”’)?\\s*security\\s+hotspots/i. For files: /###\\s+ðŸ“?\\s*`([^`]+)`/. Fall back to text search without emoji. Log parse warnings.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Lines 163 and 203 in verify-sonar-phase.js; used to verify phase completion; emoji change breaks categorization","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D001","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D001","category":"code-quality","severity":"S0","type":"code-smell","file":"/home/user/sonash-v0/docs/SoNash_Technical_Ideation_Multi_AI 1.20.26.md","line":0,"title":"SoNash_Technical_Ideation_Multi_AI 1.20.26.md - 4.1KB ideation document never linked","description":"Large ideation document (4118 lines) containing multi-AI technical proposals that is never referenced by any script, hook, skill, or other documentation. Listed as orphaned in DOCUMENTATION_INDEX.md.","recommendation":"Move to docs/archive/ or consolidate findings into EXPANSION_EVALUATION_TRACKER.md. The content appears to be exploratory AI-generated technical ideation that should either be integrated into active roadmaps or archived as historical reference.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md lists as orphaned with â†“0 â†‘0 references; grep search for filename returns 14 refs but all are in technical-debt views or archive-related docs, not active docs","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D002","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D002","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/docs/HOOKIFY_STRATEGY.md","line":0,"title":"HOOKIFY_STRATEGY.md - 1.1KB implementation plan unused","description":"Hookify Strategy & Implementation Plan (1059 lines) documents a hooks implementation strategy that is never referenced by any active documentation, scripts, or hooks themselves.","recommendation":"Review content and either: (1) integrate strategy into .claude/HOOKS.md, (2) create references from hook files, or (3) move to archive if strategy was superseded. Check if hookification is still planned or completed.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md orphaned list; zero grep references outside the file itself","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D003","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D003","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/docs/RECAPTCHA_REMOVAL_GUIDE.md","line":0,"title":"RECAPTCHA_REMOVAL_GUIDE.md - 745 lines about Firebase configuration rarely used","description":"Comprehensive guide (745 lines) for reCAPTCHA removal and fresh App Check setup. While server-side docs reference it (2 refs), it has zero inbound document references and is primarily a reference/procedural doc.","recommendation":"Integrate into APPCHECK_SETUP.md or SERVER_SIDE_SECURITY.md as a section. Update README or index to reference it, or move obsolete sections to archive if reCAPTCHA removal was already completed.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md shows â†“0 outbound refs; referenced by 2 server-side docs but not by index or guides","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D004","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D004","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/docs/REVIEW_POLICY_INDEX.md","line":0,"title":"REVIEW_POLICY_INDEX.md - 370 lines index without inbound refs","description":"Review Policy Index (370 lines) serves as a directory for review policies but has zero inbound references despite 9 upward references. Not referenced by README, DOCUMENTATION_INDEX, or navigation docs.","recommendation":"Add explicit reference in DOCUMENTATION_INDEX.md index section and link from REVIEW_POLICY_ARCHITECTURE.md. If serving as index, ensure it's discoverable from main navigation (README, DOCUMENTATION_INDEX).","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md lists as orphaned; found referenced in DOCUMENTATION_INDEX.md and scripts/check-docs-light.js only","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D005","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D005","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/docs/PLAN_MAP.md","line":0,"title":"PLAN_MAP.md - 242 lines documentation hierarchy map never referenced","description":"SoNash Documentation Plan Map (242 lines) provides visual hierarchy and relationships but is completely orphaned with zero inbound references.","recommendation":"Either (1) add reference from DOCUMENTATION_INDEX.md as navigation aid, (2) integrate content into README.md or DOCUMENTATION_STANDARDS.md, or (3) move to docs/archive/ if superseded by DOCUMENTATION_INDEX.md.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md orphaned list; zero external references; grep shows only DOCUMENTATION_INDEX references the filename","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D006","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D006","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/docs/MCP_SERVER_AUDIT.md","line":0,"title":"MCP_SERVER_AUDIT.md - 374 lines about MCP consumption never referenced","description":"MCP Server Usage Audit (374 lines) designed to identify MCP servers consuming context but never referenced by any documentation or audit processes.","recommendation":"Link from multi-ai-audit coordinator or create reference in relevant audit plans. If this audit should be run, add to AUDIT_TRACKER.md. Consider if this aligns with actual audit workflows.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md orphaned list; grep finds only DOCUMENTATION_INDEX reference","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D007","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D007","category":"code-quality","severity":"S3","type":"code-smell","file":"/home/user/sonash-v0/docs/MCP_SETUP.md","line":0,"title":"MCP_SETUP.md - 178 lines configuration guide without traction","description":"MCP Server Setup Guide (178 lines) provides configuration instructions but is unused - no references from setup docs, deployment guides, or DEVELOPMENT.md.","recommendation":"Integrate into DEVELOPMENT.md or .claude/REQUIRED_PLUGINS.md. If MCP is critical for setup, ensure onboarding docs reference it. Otherwise archive as legacy.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md orphaned; DEVELOPMENT.md and onboarding docs do not reference it; zero grep matches in active code","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D008","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D008","category":"code-quality","severity":"S3","type":"code-smell","file":"/home/user/sonash-v0/docs/LEARNING_METRICS.md","line":0,"title":"LEARNING_METRICS.md - 84 lines metrics tracking document","description":"Learning Effectiveness Metrics (84 lines) auto-generated tracker showing pattern learning effectiveness but with zero inbound references despite being produced by scripts/analyze-learning-effectiveness.js.","recommendation":"Add reference from AUDIT_TRACKER.md or alerts system. If auto-generated, ensure output is mentioned in the generating script's documentation. Consider if metrics should feed into decision-making workflows.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md orphaned list; generated by script but not integrated into dashboards or tracking systems","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D009","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D009","category":"code-quality","severity":"S3","type":"code-smell","file":"/home/user/sonash-v0/docs/AUTOMATION_AUDIT_REPORT.md","line":0,"title":"AUTOMATION_AUDIT_REPORT.md - 255 lines audit results never integrated","description":"Automation Audit Report (255 lines) appears to be a standalone audit report with no inbound references, likely superseded by newer audit structure in docs/audits/ subdirectory.","recommendation":"Remove duplicate at root level (keep versioned audit in subdirectory). Update references to point to dated audit instance in docs/audits/. Consider if root-level report should be generated or archived.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md orphaned; newer dated version exists in docs/audits/; no scripts or workflows reference the root-level version","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D010","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D010","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/docs/agent_docs/FIX_TEMPLATES.md","line":0,"title":"FIX_TEMPLATES.md - 0 outbound refs for Qodo PR fixes","description":"Fix Templates for Qodo PR Review Findings (docs/agent_docs/FIX_TEMPLATES.md) provides copy-paste templates but is never referenced by code reviewer agents, PR review skills, or documentation.","recommendation":"Link from code-reviewer skill, CODE_PATTERNS.md, and review process documentation. If templates are intended for use, ensure they're discoverable from agent workflows that need them.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md orphaned list; grep shows zero references from skills or agent configs","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D011","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D011","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/docs/agent_docs/SKILL_AGENT_POLICY.md","line":0,"title":"SKILL_AGENT_POLICY.md - 0 refs despite defining usage policy","description":"Skill and Agent Usage Policy (docs/agent_docs/SKILL_AGENT_POLICY.md) defines critical policies for skill/agent creation but has zero inbound references despite having 3 upward references.","recommendation":"Link from agent creation guides, DEVELOPMENT.md, and main agent documentation README. Add to onboarding checklist. Ensure policy is discoverable before agents are created.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md orphaned; not referenced from main skill/agent docs or onboarding materials","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D012","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D012","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/docs/audits/single-session/process/audit-2026-02-09/stage-1*.md","line":0,"title":"Audit inventory stage files (6 files) - generated but unreferenced","description":"Six stage-1 audit inventory files (stage-1a through stage-1f) generated 2026-02-09 but never integrated into audit workflows or referenced by audit aggregation processes.","recommendation":"Either (1) integrate into AUDIT_TRACKER with findings aggregation, or (2) move dated audit outputs to completed audit archive with clear retention policy. If stage outputs are intermediate, don't persist in docs.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md marks all 6 as orphaned; audit-related grep shows references only in index, not in active audit workflows","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D013","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D013","category":"code-quality","severity":"S3","type":"code-smell","file":"/home/user/sonash-v0/docs/decisions/","line":0,"title":"ADR template and decisions/README - decision framework underutilized","description":"Architecture Decision Records framework (docs/decisions/README.md) with template (docs/decisions/TEMPLATE.md) but only one ADR documented; framework appears unused.","recommendation":"Either (1) activate ADR process and link from ARCHITECTURE.md + AI_WORKFLOW.md, or (2) consolidate decision tracking into SESSION_DECISIONS.md format. Clean up if not part of current workflow.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md shows ADR README as orphaned (â†“0); only one historical ADR exists (ADR-001); no recent decisions recorded","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D014","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D014","category":"code-quality","severity":"S2","type":"code-smell","file":"/home/user/sonash-v0/docs/plans/CI_GATES_BLOCKING_PLAN.md and 4 others","line":0,"title":"Plan documents with zero inbound refs - 5 planning files orphaned","description":"Five planning documents (CI_GATES_BLOCKING_PLAN, SESSION_CONTEXT_REDUCTION_PLAN, TRACK_A_MANUAL_TEST_CHECKLIST, alerts-enhancement-plan, and roadmap-assignment-report) all marked orphaned in DOCUMENTATION_INDEX.","recommendation":"For each plan: (1) check if completed and archive to docs/archive/completed-plans/, or (2) if active, add explicit reference from ROADMAP.md and PLAN_MAP.md. Consolidate similar plans (e.g., testing checklists).","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"DOCUMENTATION_INDEX.md orphaned list includes all 5 files; ROADMAP.md and PLAN_MAP.md do not reference most of them","sources":[],"merged_from":[]}
{"source_id":"audit:OPT-D015","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-13\\findings.jsonl","original_id":"OPT-D015","category":"code-quality","severity":"S1","type":"code-smell","file":"/home/user/sonash-v0/docs/technical-debt/views/","line":0,"title":"Technical debt view files - generated views without integration","description":"Three technical debt view files (by-category, by-severity, by-status) plus views/unplaced-items are generated by TDMS but not integrated into monitoring dashboards or alerting systems.","recommendation":"Integrate into alerts system or create monitoring dashboard that references these views. Update TECHNICAL_DEBT_MANAGEMENT_SYSTEM_PLAN.md to document how views feed into workflows. Consider if view files should be excluded from docs and generated-only.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":"Views marked orphaned in DOCUMENTATION_INDEX; generated by TDMS but no active consumption in workflows; greps show only TDMS procedure references them","sources":[],"merged_from":[]}
{"source_id":"audit:hash-Q3Jvc3MtY3V0","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-3b-cross-cutting.jsonl","original_id":null,"category":"code-quality","severity":"S1","type":"code-smell","file":".claude/settings.json","line":0,"title":"Cross-cutting: Hook process spawn explosion â€” 10+ Node.js processes per tool use across all hook domains","description":"The single most impactful architectural issue: every Write spawns 10 processes, every Edit spawns 9, every Read spawns 3, every UserPromptSubmit spawns 4. This is 26+ process spawns per edit-read-prompt cycle. At ~40ms cold-start each, that is over 1 second of pure process overhead per interaction cycle. Connects findings: 2a::posttooluse-write::10-sequential-hooks, 2a::posttooluse-edit::9-sequential-hooks, 2a::posttooluse-read::redundant-state-tracking, 2a::userpromptsubmit::4-hooks-every-message, 2a::posttooluse::duplicate-file-reads, 2d::write-hook-overload, 2d::read-hook-overhead.","recommendation":"Consolidate into 3 unified hook entry points: (1) post-tool-use-write-edit.js for all Write/Edit/MultiEdit checks, (2) context-monitor.js for all Read tracking, (3) user-prompt-handler.js for all UserPromptSubmit logic. Each reads target file once, runs all checks in-process, shares state. Reduces 26 spawns to 3 per cycle.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-Tm8gYXV0b21h","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-3a-automation-gaps.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/SKILL_INDEX.md","line":0,"title":"No automated SKILL_INDEX.md sync â€” 9 orphaned skills discovered manually","description":"Stage 2b found 9 skills missing from SKILL_INDEX.md and 2 duplicate listings. There is no pre-commit hook or CI check that validates SKILL_INDEX.md matches the filesystem. New skills can be added without updating the index indefinitely.","recommendation":"Create scripts/check-skill-index.js that scans .claude/skills/*/SKILL.md directories and validates every directory has a SKILL_INDEX.md entry with no duplicates. Add to pre-commit hook chain after doc-index check.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-Tm8gcHJlLWNv","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-3a-automation-gaps.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/skills/audit-process/SKILL.md","line":0,"title":"No pre-commit check for FALSE_POSITIVES.jsonl integration in audit skills","description":"Stage 2b found audit-process and audit-enhancements do not reference FALSE_POSITIVES.jsonl, meaning they re-report known false positives. No CI or hook check validates that all audit skills integrate with the false-positives exclusion list.","recommendation":"Add a check to scripts/check-skill-index.js (or new script) that greps each audit-*/SKILL.md for FALSE_POSITIVES.jsonl reference. Fail if any audit skill lacks it. Add to pre-commit chain.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-U2Vzc2lvbi1l","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-3a-automation-gaps.jsonl","original_id":null,"category":"code-quality","severity":"S1","type":"code-smell","file":".claude/hooks/.session-state.json","line":0,"title":"Session-end cleanup is practically dead code â€” needs migration to session-start","description":"Stage 2e found 166 session begins vs 16 session ends (10:1 ratio). All state cleanup logic in session-end step 6 is effectively dead code since it runs in <10% of sessions. Temp files, ephemeral state, stale tasks, and JSONL rotation all depend on session-end which almost never runs.","recommendation":"Move all critical cleanup operations from session-end step 6 to session-start.js: (1) remove stale temp files, (2) prune old task states, (3) clean ephemeral agent tracking files, (4) check JSONL sizes and rotate if needed. Session-end becomes optional nice-to-have, not a critical cleanup dependency.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-Y29tbWl0LWxv","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2e-memory-state.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/state/commit-log.jsonl","line":0,"title":"commit-log.jsonl has no rotation/size-cap logic (39KB, append-only)","description":"commit-log.jsonl is append-only with no rotation, archival, or size-cap logic. At 39KB after ~10 days of use, it will grow unbounded over months. STATE_SCHEMA.md explicitly notes 'Append-only. No automatic cleanup.' The handoff.json pre-compaction save embeds the FULL commitLog array (currently 15+ entries with filesList arrays), amplifying the bloat.","recommendation":"Add rotation logic to commit-tracker.js: when line count exceeds 500, archive older entries to commit-log.archive.jsonl and truncate to most recent 200. Alternatively, add a prune step to session-end that keeps only the last 30 days of entries.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-aGFuZG9mZi5q","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2e-memory-state.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/state/handoff.json","line":0,"title":"handoff.json embeds full commitLog array (25KB, unbounded growth)","description":"handoff.json is 25KB, with the commitLog array consuming most of that space. Each commit entry includes a filesList array of up to 30 paths. pre-compaction-save.js copies the entire commit-log.jsonl into handoff.json on every compaction. This duplicates data and makes handoff.json grow proportionally to total commit history.","recommendation":"In pre-compaction-save.js, limit commitLog to the last 15 entries (most recent session's commits). The full history remains in commit-log.jsonl for gap detection. compact-restore.js only needs recent context, not full history.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-cmV2aWV3cy5q","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2e-memory-state.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/state/reviews.jsonl","line":0,"title":"reviews.jsonl has no rotation (19KB, 30+ entries, threshold is 50)","description":"STATE_SCHEMA.md recommends 'Archive recommended at >50 entries' but no code implements this archival. Currently at ~30 entries (19KB). Each entry contains patterns and learnings arrays. At current review velocity (~10/session), the 50-entry threshold will be hit within 2 sessions.","recommendation":"Add rotation logic to the consolidation script or session-end: when reviews.jsonl exceeds 50 lines, archive entries older than consolidation.lastConsolidatedReview to reviews.archive.jsonl.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-d2FybmVkLWZp","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2e-memory-state.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/state/warned-files.json","line":0,"title":"warned-files.json grows unbounded (4KB, 48 entries, no expiry)","description":"warned-files.json tracks file::pattern combos that have been warned. Entries accumulate indefinitely with no expiry or cleanup. Currently 48 entries at 4KB. As more files are scanned and patterns added, this will grow linearly. Old entries for deleted files or resolved patterns waste space and could cause false graduation to blocking.","recommendation":"Add expiry logic to check-pattern-compliance.js: remove entries older than 30 days or where the referenced file no longer exists. Run cleanup at the start of each pattern check.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-dGFzay1hdWRp","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2e-memory-state.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/state/task-audit-template-overhaul.state.json","line":0,"title":"task-audit-template-overhaul.state.json is stale (7 days, all steps pending)","description":"This task state file was last updated 2026-02-07, all 6 steps show 'pending' status, yet the audit template overhaul was completed (commit 360f714 on 2026-02-07). The session-end cleanup only removes task state files if no tasks are in_progress, but this file has status 'planned' (not 'in_progress'), so it persists. The handoff.json also embeds this stale state in taskStates, wasting space.","recommendation":"Delete the stale file manually now. Fix session-end cleanup logic to also remove task files with status 'planned' that are older than 7 days, or where all steps are 'pending' and lastUpdated is >3 days old.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-b3ZlcnJpZGUt","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2e-memory-state.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/override-log.jsonl","line":0,"title":"override-log.jsonl has no rotation or size cap","description":"override-log.jsonl is append-only with no rotation logic. Currently small (5 entries) but will grow unbounded over time. Unlike commit-log.jsonl which at least has documentation noting the gap, override-log.jsonl has no size management mentioned anywhere.","recommendation":"Add rotation in log-override.js: when entries exceed 100, archive older entries. Alternatively, add to session-end cleanup to prune entries older than 90 days.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-YWdlbnQtaW52","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2e-memory-state.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/state/agent-invocations.jsonl","line":0,"title":"agent-invocations.jsonl has no rotation or size cap","description":"agent-invocations.jsonl is append-only with no cleanup logic. Currently tiny (1 entry, 123 bytes) but designed to log every agent invocation. With increased agent usage, this will accumulate rapidly. No rotation, archival, or session-based cleanup exists.","recommendation":"Add size-cap logic to track-agent-invocation.js: if file exceeds 200 lines, truncate to most recent 100. Or add to session-end cleanup.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-dmVsb2NpdHkt","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2e-memory-state.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/state/velocity-log.jsonl","line":0,"title":"velocity-log.jsonl has malformed sprint field data","description":"All 5 entries in velocity-log.jsonl have a malformed sprint field containing what appears to be a full markdown table row instead of a sprint name (e.g., 'M1 - Foundation** | Complete | 100% | Foundation | P0 | - |'). The regex parsing in track-session.js is capturing too much from ROADMAP.md. This corrupts velocity analytics and trend calculations.","recommendation":"Fix the sprint extraction regex in track-session.js to capture only the sprint name (e.g., 'M1 - Foundation'). Retroactively fix existing entries or delete and re-seed the file.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-LmNsYXVkZS90","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2e-memory-state.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/tmp-alerts.json","line":0,"title":".claude/tmp-alerts.json persists across sessions (temp file not cleaned up)","description":"tmp-alerts.json is an untracked temporary file that persists across sessions. It appears in git status as untracked. session-end cleanup does not remove it. Temp files should be cleaned up after use or on session start.","recommendation":"Add cleanup of .claude/tmp-*.json to session-end step 6 or session-start.js. The alerts script should use a proper temp directory or clean up after itself.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-c2Vzc2lvbi1l","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2e-memory-state.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/skills/session-end/SKILL.md","line":0,"title":"session-end does not clean .session-agents.json or .agent-trigger-state.json","description":"session-end step 4 reads .session-agents.json and .agent-trigger-state.json for compliance review but never deletes them. Step 6 only removes pending-reviews.json and conditionally handoff.json. These ephemeral state files should be cleaned up at session end to prevent stale data from affecting the next session's agent tracking.","recommendation":"Add rm -f commands in session-end step 6 for .claude/hooks/.session-agents.json and .claude/hooks/.agent-trigger-state.json after the compliance review is complete.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-TUVNT1JZLm1k","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2e-memory-state.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":"C:\\Users\\jason\\.claude\\projects\\C--Users-jason-Workspace-dev-projects-sonash-v0\\memory\\MEMORY.md","line":0,"title":"MEMORY.md is within size limits but contains completed/stale entries","description":"MEMORY.md contains entries marked as 'COMPLETE' (Multi-AI Audit Skill Evaluation) that no longer need to occupy memory space. The 'eval wrappers removed' note is historical context that could be archived. At ~55 lines the file is well within the 200-line limit, but proactive cleanup prevents accumulation. The currentDate entry (2026-02-14) also needs manual updating.","recommendation":"Remove the completed Multi-AI Audit Skill Evaluation section. The currentDate field should be set automatically rather than manually maintained.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-LnNlc3Npb24t","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2e-memory-state.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/.session-state.json","line":0,"title":".session-state.json shows 166 begins vs 16 ends (10:1 ratio)","description":"The session state shows 166 session begins but only 16 session ends, a 10:1 ratio. This means ~90% of sessions end without running session-end, which means state cleanup (step 6) almost never runs. All the cleanup issues identified in this audit are compounded by the fact that session-end is rarely executed. State files accumulate because the cleanup mechanism is practically never triggered.","recommendation":"Add lightweight automatic cleanup to session-start.js that runs the critical cleanup steps (remove stale temp files, prune old task states, check JSONL sizes). This ensures cleanup happens even when session-end is skipped. Consider adding a session-start alert when begin/end ratio exceeds 5:1.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-c2Vzc2lvbi1i","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2d-context-optimization.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/session-begin/SKILL.md","line":0,"title":"session-begin SKILL.md reads ROADMAP.md (3164 lines) in full at every session start","description":"ROADMAP.md is 3164 lines. Reading it fully at session start consumes ~4K tokens of context window. Session-begin only needs the Active Sprint section (typically <100 lines). This happens every session, so cumulative token waste is significant.","recommendation":"Change session-begin Step 1 and Step 3 to read only the Active Sprint section of ROADMAP.md using offset/limit parameters (e.g., Read with limit:100 from the Active Sprint header). Add a grep-first pattern to find the section offset dynamically.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-c2Vzc2lvbi1z","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2d-context-optimization.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/session-start.js","line":0,"title":"session-start.js hook outputs ~50 lines of checklist text injected into context","description":"Lines 482-505 of session-start.js output a 25-line SESSION CHECKLIST and Tips section to stdout, which gets injected into Claude's context. This duplicates information already in session-begin SKILL.md (which Claude reads anyway when /session-begin is invoked). The hook also outputs environment info, pattern check results, consolidation status, and TDMS metrics totaling ~50 lines of stdout context injection per session.","recommendation":"Move the SESSION CHECKLIST output (lines 482-505) from stdout to stderr so it's visible to the user but not injected into Claude's context. Keep only actionable warnings (failed checks, S0 items) on stdout. The SKILL.md already contains the full checklist.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-Y29tcGFjdC1y","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2d-context-optimization.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/compact-restore.js","line":0,"title":"compact-restore.js injects full recovery context (~30 lines) even when handoff is stale","description":"compact-restore.js always outputs the full recovery block to stdout (lines 122-155, ~30 lines) regardless of how old the handoff data is. If handoff.json is hours or days old (from a previous session), it injects irrelevant recovery context. There is no staleness check beyond displaying the age.","recommendation":"Add a staleness threshold (e.g., 60 minutes). If handoff.json is older than threshold, output a 1-line summary to stdout ('Stale handoff found (Xh ago) - run /session-begin for fresh context') instead of the full recovery block. Keep full output only for recent compactions.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-cHItcmV2aWV3","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2d-context-optimization.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/pr-review/SKILL.md","line":0,"title":"pr-review SKILL.md is 840 lines loaded entirely when skill is invoked","description":"At 840 lines, pr-review is one of the largest skill files. It includes detailed examples, anti-patterns, agent selection matrices, and error handling that consume ~10K tokens. Much of this (Steps 6.5 TDMS integration, Step 7.5 health check, future enhancements) is rarely needed in a single invocation.","recommendation":"Split pr-review SKILL.md into a core protocol (~300 lines covering Steps 0-5) and a reference section in a separate file (pr-review/REFERENCE.md) covering Steps 6-9, TDMS integration, and examples. The core file instructs to read REFERENCE.md only when reaching those steps.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-YXVkaXQtY29t","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2d-context-optimization.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/audit-comprehensive/SKILL.md","line":0,"title":"audit-comprehensive SKILL.md is 854 lines loaded entirely for every comprehensive audit","description":"At 854 lines, audit-comprehensive is the largest skill file. It includes detailed stage instructions, recovery matrices, triage formulas, and version history. The recovery section (lines 716-752) and triage section (lines 559-654) are only needed in specific circumstances, not on every invocation.","recommendation":"Extract the Recovery section and Triage & Roadmap Integration section into separate reference files. The main SKILL.md should reference them with 'If context compacts, read audit-comprehensive/RECOVERY.md' and 'After TDMS intake, read audit-comprehensive/TRIAGE.md'.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-MTAgUG9zdFRv","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2d-context-optimization.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/settings.json","line":0,"title":"10 PostToolUse hooks fire on every Write operation creating cumulative latency","description":"Every Write tool invocation triggers 10 PostToolUse hooks sequentially: check-requirements, audit-s0s1-validator, pattern-check, component-size-check, firestore-write-block, test-mocking-validator, app-check-validator, typescript-strict-check, repository-pattern-check, agent-trigger-enforcer. Most of these produce 'ok' (no context injection) but each adds ~50-100ms latency. For sessions with 50+ writes, this adds 25-50 seconds of cumulative delay.","recommendation":"Consolidate domain-specific validators (firestore-write-block, test-mocking-validator, app-check-validator, typescript-strict-check, repository-pattern-check) into a single check-code-patterns.js hook that does file-path-based routing internally. This reduces 10 process spawns to 4-5 per Write. Each duplicate hook also has its own path parsing and security validation boilerplate.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-RWRpdCBhbmQg","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2d-context-optimization.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/settings.json","line":0,"title":"Edit and MultiEdit hook lists are identical 9-hook copies of Write hooks","description":"The Edit and MultiEdit PostToolUse hook arrays (settings.json lines 114-210) are near-exact copies of the Write hooks (minus audit-s0s1-validator). This triplicates configuration maintenance. When a hook is added or removed, all three sections must be updated. The settings.json file is 293 lines, with ~180 lines being duplicated hook definitions.","recommendation":"Use a single matcher pattern like '^(?i)(write|edit|multiedit)$' for the shared hooks, and keep Write-only hooks (audit-s0s1-validator) in a separate Write-specific entry. This reduces settings.json by ~120 lines and eliminates sync drift risk.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-YWxlcnRzLXJl","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2d-context-optimization.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/alerts-reminder.js","line":0,"title":"alerts-reminder.js injects context-consuming messages on every UserPromptSubmit","description":"alerts-reminder.js runs on every UserPromptSubmit and outputs to stdout (console.log) which injects into Claude's context. When alerts exist and context threshold is exceeded, it injects 3-4 lines per user message. Over a session with 30+ user messages, this adds ~100 lines of repeated 'ALERTS: N pending' and 'CONTEXT: N files read' text to the conversation context.","recommendation":"Add a cooldown mechanism: only inject context-consuming alert text once per 10 minutes (or once per session). Use a state file to track last injection time. Subsequent prompts within the cooldown should output to stderr (user-visible) instead of stdout (context-consuming).","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-YW5hbHl6ZS11","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2d-context-optimization.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/analyze-user-request.js","line":0,"title":"analyze-user-request.js high-confidence matches inject directive into context on every match","description":"When analyze-user-request.js matches a high-confidence pattern (e.g., any mention of 'bug', 'broken', 'css', 'tailwind'), it injects a PRE-TASK directive to stdout. This happens on every matching UserPromptSubmit. In a debugging session, every message mentioning 'bug' reinjects 'PRE-TASK: MUST use systematic-debugging skill FIRST' consuming context tokens. The agent-trigger-enforcer already tracks this per-session.","recommendation":"Add session-level deduplication: track which directives have been emitted this session (via state file). Only inject to stdout on first occurrence per directive type. Subsequent matches should go to stderr or be suppressed entirely.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-MyBob29rcyBv","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2d-context-optimization.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/settings.json","line":0,"title":"3 hooks on every Read operation add latency without context benefit for most reads","description":"Every Read tool invocation triggers 3 PostToolUse hooks: large-context-warning.js, auto-save-context.js, and compaction-handoff.js. Each spawns a Node.js process, reads state files, and checks thresholds. For the first 15-20 reads in a session (before any threshold is hit), all 3 hooks do nothing but return 'ok'. In a typical session with 30+ reads, the first 15 reads each spawn 3 unnecessary processes.","recommendation":"Consolidate the 3 Read hooks into a single context-monitor.js hook that handles all tracking, warnings, auto-save, and handoff preparation in one process spawn. This reduces per-Read overhead from 3 process spawns to 1, and shares state file reads.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-Y29kZS1yZXZp","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2d-context-optimization.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/skills/code-reviewer/SKILL.md","line":0,"title":"code-reviewer SKILL.md contains generic boilerplate not specific to this project","description":"code-reviewer SKILL.md is 313 lines, of which ~150 lines (sections: Tech Stack, Development Workflow, Common Commands, Troubleshooting) are generic boilerplate mentioning Python, Docker, Kubernetes, PostgreSQL, Flutter, etc. that are not part of this project's stack. This wastes ~2K tokens of context with irrelevant information every time the code-reviewer skill is invoked.","recommendation":"Remove generic boilerplate sections (Tech Stack generic list, Development Workflow generic setup, Common Commands generic docker/k8s, Troubleshooting generic). Keep only the SoNash-specific Script Checklist (lines 188-204) and the project-relevant review guidance. Target: under 200 lines.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-TUNQIGdpdCBz","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2c-mcp-config.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".mcp.json","line":0,"title":"MCP git server configured but unused and redundant with native Bash git","description":"The git MCP server is configured in .mcp.json but (a) not listed in enabledMcpjsonServers, (b) has zero references in any skill or hook, and (c) duplicates native git CLI via Bash. It adds startup overhead if ever enabled and creates confusion about which git interface to use.","recommendation":"Remove the 'git' entry from .mcp.json mcpServers since all git operations use Bash(git ...) natively.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-TUNQIG1lbW9y","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2c-mcp-config.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".mcp.json","line":0,"title":"MCP memory server configured but not enabled â€” 3 skills have dead mcp__memory__ code paths","description":"The memory server is in .mcp.json but missing from enabledMcpjsonServers in settings.local.json. Three skills (save-context, audit-enhancements, pr-retro) reference mcp__memory__ tools that will silently fail. Either enable it or remove dead references.","recommendation":"Decision needed: (1) Add 'memory' to enabledMcpjsonServers if memory persistence is wanted, OR (2) Remove mcp__memory__ references from the 3 skills and delete the memory entry from .mcp.json.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-TUNQIGZpbGVz","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2c-mcp-config.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".mcp.json","line":0,"title":"MCP filesystem server duplicates native Claude Code Read/Write/Glob/Grep tools","description":"The filesystem MCP server provides list_directory, read_file, directory_tree etc. but Claude Code has native Read, Write, Edit, Glob, Grep tools that are faster (no MCP serialization overhead) and more capable. The server consumes a process slot, adds startup latency, and 7 filesystem MCP tools are pre-approved in settings.local.json permissions despite zero skill references.","recommendation":"Remove 'filesystem' from .mcp.json and enabledMcpjsonServers. Remove the 7 mcp__filesystem__* permission entries from settings.local.json. Use native Read/Write/Glob/Grep instead.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-TUNQIGdpdGh1","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2c-mcp-config.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/settings.local.json","line":0,"title":"MCP github server enabled but not configured in .mcp.json â€” likely plugin-only, redundant with gh CLI","description":"'github' is listed in enabledMcpjsonServers but has no entry in .mcp.json. It may work via the Claude plugin system, but zero skills reference mcp__github__ tools. All GitHub operations use gh CLI via Bash (gh pr view, gh run list, etc.). The enabled entry is either stale or redundant.","recommendation":"Remove 'github' from enabledMcpjsonServers since no skills use mcp__github__ tools and gh CLI covers all GitHub operations.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-NyBtY3BfX2Zp","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2c-mcp-config.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/settings.local.json","line":0,"title":"7 mcp__filesystem__ permissions approved in settings.local.json for unused server","description":"settings.local.json has 7 pre-approved mcp__filesystem__* permissions (list_directory, directory_tree, read_text_file, read_multiple_files, get_file_info, list_allowed_directories, list_directory_with_sizes inferred). These bloat the permissions list and could be confusing since native tools should be preferred.","recommendation":"Remove all mcp__filesystem__* entries from settings.local.json permissions.allow array.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-MyBtY3BfX3Nl","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2c-mcp-config.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/settings.local.json","line":0,"title":"3 mcp__serena__ permissions approved but serena is in disabledMcpjsonServers","description":"settings.local.json has 3 approved mcp__serena__ permissions (list_dir, get_current_config, activate_project) but serena is explicitly disabled in settings.json disabledMcpjsonServers. These permissions are dead entries that add clutter.","recommendation":"Remove all mcp__serena__* entries from settings.local.json permissions.allow array.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-TUNQIGdsb2Jh","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2c-mcp-config.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/mcp.global-template.json","line":0,"title":"MCP global template (.claude/mcp.global-template.json) includes puppeteer which conflicts with playwright","description":"The global MCP template suggests puppeteer for browser automation, but the project uses playwright MCP server for all browser testing (test-suite skill, 15+ tool references). If a user follows the template and installs both, they get two competing browser automation servers consuming resources.","recommendation":"Update mcp.global-template.json to suggest playwright instead of puppeteer, matching the actual project configuration.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-TUNQIGZpcmVi","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2c-mcp-config.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/settings.local.json","line":0,"title":"MCP firebase server enabled but not configured â€” may rely on plugin auto-discovery","description":"'firebase' is in enabledMcpjsonServers but has no entry in .mcp.json. Three mcp__firebase__ permissions exist in settings.local.json. If this relies on plugin auto-discovery it works but is fragile. If it is stale configuration from a removed server, it adds confusion.","recommendation":"Either add firebase server configuration to .mcp.json for explicit setup, or verify it works via plugin and document that dependency.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-U2tpbGwgb3Zl","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2b-skill-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/sonarcloud-sprint/SKILL.md","line":0,"title":"Skill overlap: sonarcloud-sprint is superseded by sonarcloud","description":"sonarcloud skill explicitly states it consolidates sonarcloud-sprint, yet both still exist as separate skills. Users may invoke the older skill and miss newer capabilities (sync, resolve, status modes). Wastes context loading the wrong one.","recommendation":"Delete .claude/skills/sonarcloud-sprint/ directory and update SKILL_INDEX.md to list /sonarcloud with all modes","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-T3JwaGFuZWQg","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2b-skill-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/find-skills/SKILL.md","line":0,"title":"Orphaned skill: find-skills not in SKILL_INDEX.md","description":"find-skills exists in filesystem but is not listed in SKILL_INDEX.md. Users consulting the index will not discover it. The skill helps users find installable skills â€” ironic that it itself is unfindable.","recommendation":"Add find-skills to the Infrastructure & Setup category in SKILL_INDEX.md","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-U0tJTExfSU5E","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2b-skill-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/skills/SKILL_INDEX.md","line":0,"title":"SKILL_INDEX.md claims 55 total skills but count is inaccurate","description":"Header claims 55 skills but filesystem has 56 directories, index has duplicate listings (code-reviewer, senior-fullstack), and 9 skills are missing from the index. Misleading metadata.","recommendation":"Recalculate: count unique skill directories, remove duplicates, add missing skills, update header count","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-QWdlbnQgcHJv","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2b-skill-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/audit-process/SKILL.md","line":0,"title":"Agent prompts in audit-process lack CRITICAL RETURN PROTOCOL","description":"audit-process has 12+ Task() agent prompts that say 'Do NOT return findings as text' but lack the CRITICAL RETURN PROTOCOL ('Return ONLY: COMPLETE...'). Agents may return verbose summaries, bloating orchestrator context and risking overflow (Session #140 lesson).","recommendation":"Add 'Return ONLY: COMPLETE: [id] wrote N findings to [path]' to every Task() prompt in audit-process, matching the pattern used in doc-optimizer and audit-ai-optimization","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-RkFMU0VfUE9T","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2b-skill-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/audit-process/SKILL.md","line":0,"title":"FALSE_POSITIVES.jsonl not referenced in audit-process or audit-enhancements","description":"audit-code, audit-security, audit-performance, audit-documentation, audit-refactoring, and audit-comprehensive all reference FALSE_POSITIVES.jsonl for exclusion. audit-process and audit-enhancements do not, meaning they will re-report known false positives every run.","recommendation":"Add FALSE_POSITIVES.jsonl loading step to Pre-Audit Setup in both audit-process and audit-enhancements","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-cHJlLWNvbW1p","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2b-skill-architecture.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/skills/pre-commit-fixer/SKILL.md","line":0,"title":"pre-commit-fixer agent prompts are generic category dispatches, not structured","description":"pre-commit-fixer spawns subagents for ESLint and pattern compliance fixes but references agent types generically ('debugger', 'code-reviewer', 'general-purpose') without structured prompts. Agents receive error output but no explicit fix-and-verify workflow.","recommendation":"Add structured prompt templates for each Category B fix type: include the error output, affected files, expected fix pattern, and verification command","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-U2Vzc2lvblN0","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2a-hook-efficiency.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/session-start.js","line":0,"title":"SessionStart: session-start.js spawns 6+ sequential execSync subprocesses","description":"SessionStart is on the critical path for every new session. session-start.js calls execSync sequentially for: node -v, npm -v, npm ci (root), npm ci (functions), npm run build (functions), npm run test:build, node scripts/check-pattern-compliance.js, node scripts/run-consolidation.js. Each execSync spawns a new shell process (~50-150ms overhead each). Total hook time can exceed 30-60 seconds.","recommendation":"Parallelize independent operations using Promise.all with child_process.exec (async). npm ci root and npm ci functions are independent. node -v and npm -v can run in parallel. Build steps that depend on install can be chained but run concurrently with unrelated checks. Pattern compliance and consolidation checks are independent of each other.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-UG9zdFRvb2xV","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2a-hook-efficiency.jsonl","original_id":null,"category":"code-quality","severity":"S1","type":"code-smell","file":".claude/settings.json","line":0,"title":"PostToolUse Write: 10 sequential hook processes spawned per Write operation","description":"Every Write tool call spawns 10 separate Node.js processes sequentially: check-requirements, audit-s0s1-validator, pattern-check, component-size-check, firestore-write-block, test-mocking-validator, app-check-validator, typescript-strict-check, repository-pattern-check, agent-trigger-enforcer. Node.js cold start is ~30-50ms each, so minimum overhead is 300-500ms per Write even when all hooks bail out early. This is the most frequently triggered hook chain.","recommendation":"Consolidate into a single unified PostToolUse hook that runs all checks in-process. Each current hook does simple regex/string matching on file paths and content -- no reason to spawn separate processes. A single entry point could import check functions and run them sequentially in one process, saving ~9 process spawns (~270-450ms) per Write.","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-Y29tcGFjdGlv","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2a-hook-efficiency.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/compaction-handoff.js","line":0,"title":"compaction-handoff.js uses execSync (shell) instead of execFileSync for git commands","description":"compaction-handoff.js gitExec() uses execSync (which spawns a shell) instead of execFileSync (which calls git directly). Shell spawn adds ~20-50ms overhead per call. The function is called 5 times for git operations: rev-parse, log --oneline -1, diff --name-only, ls-files, diff --cached, log --oneline -10. This is both a performance issue and a security issue (shell injection risk via crafted branch names).","recommendation":"Change execSync to execFileSync with array arguments, matching the pattern already used in commit-tracker.js and pre-compaction-save.js. This is a simple find-and-replace: execSync('git rev-parse --abbrev-ref HEAD') becomes execFileSync('git', ['rev-parse', '--abbrev-ref', 'HEAD']).","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-YWdlbnQtdHJp","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2a-hook-efficiency.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/agent-trigger-enforcer.js","line":0,"title":"agent-trigger-enforcer.js loads external config via require() on every invocation","description":"agent-trigger-enforcer.js calls loadConfigWithRegex('agent-triggers') which reads and parses a JSON file from disk, then recursively converts regex descriptor objects to RegExp instances. This happens on every Write, Edit, and MultiEdit operation. While Node.js caches require(), the loadConfig function always re-reads from disk since it uses fs.readFileSync directly rather than require().","recommendation":"Cache the parsed config in-memory by using require() for the JSON file instead of fs.readFileSync, or add a module-level cache with a file mtime check. Better yet, if hooks are consolidated into a single process, this config only needs to be loaded once.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-bGFyZ2UtY29u","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2a-hook-efficiency.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/large-context-warning.js","line":0,"title":"large-context-warning.js reads entire file just to count lines","description":"large-context-warning.js reads the entire file content with fs.readFileSync then splits by newline to count lines. For large files (the exact ones this hook is designed to warn about), this can be slow and memory-intensive. A 5000-line file could be several hundred KB.","recommendation":"Use fs.statSync to get file size and estimate line count (average ~40-80 bytes per line for code files), or read only the first N bytes to count newlines. Alternatively, use a streaming approach with fs.createReadStream to count newlines without loading the entire file into memory.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-Y2hlY2stZWRp","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2a-hook-efficiency.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/check-edit-requirements.js","line":0,"title":"check-edit-requirements.js and check-write-requirements.js are superseded but still present","description":"check-requirements.js was created as a unified replacement for check-edit-requirements.js and check-write-requirements.js. The settings.json now uses check-requirements.js for all three tool types (Write, Edit, MultiEdit). The old files are still present on disk but are not referenced in settings.json hooks configuration. They add confusion and maintenance burden.","recommendation":"Delete check-edit-requirements.js and check-write-requirements.js since they are fully superseded by check-requirements.js. Verify no other configuration or script references them before deletion.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-VXNlclByb21w","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2a-hook-efficiency.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/alerts-reminder.js","line":0,"title":"UserPromptSubmit: 4 hooks run on every user message including heavy regex matching","description":"Every user message triggers 4 separate Node.js process spawns. analyze-user-request.js constructs multiple RegExp objects dynamically per invocation for word boundary matching. plan-mode-suggestion.js tests 20+ regex patterns against the user message. alerts-reminder.js reads 4 separate JSON files from disk. Total overhead is ~200-400ms per user message.","recommendation":"Consolidate into a single user-prompt-handler.js. Read all necessary state files once. Run all pattern matching in a single pass. The four hooks have zero dependencies on each other and could trivially be combined into sequential function calls in one process.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-cHJlLWNvbXBh","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2a-hook-efficiency.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/pre-compaction-save.js","line":0,"title":"pre-compaction-save.js calls 7 git subprocess operations sequentially","description":"pre-compaction-save.js gatherGitContext() calls execFileSync 7 times sequentially for: rev-parse, log -1, log -15, diff --name-only, diff --cached, ls-files, and another rev-parse. Each spawns a separate git process. While PreCompact is less frequent than PostToolUse, it adds unnecessary latency during compaction.","recommendation":"Combine git commands where possible. Use git status --porcelain=v2 to get staged, unstaged, and untracked files in a single call. Combine the two log commands into one (git log --oneline -15 subsumes git log --oneline -1). This reduces 7 calls to approximately 3.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-Y29tbWl0LXRy","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2a-hook-efficiency.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/commit-tracker.js","line":0,"title":"commit-tracker.js spawns 4 git subprocesses on commit detection","description":"When a commit is detected, commit-tracker.js calls gitExec 4 times: rev-parse HEAD, git log --format, rev-parse --abbrev-ref HEAD, and diff-tree. It also reads SESSION_CONTEXT.md to extract the session counter. While the fast-path regex bail-out is good (~1ms for non-commit commands), the commit detection path spawns 4 git processes sequentially.","recommendation":"Combine rev-parse HEAD and rev-parse --abbrev-ref HEAD into a single call using git log --format='%H%x1f%h%x1f%s%x1f%an%x1f%ad%x1f%D' which includes the ref names. This reduces from 4 to 2 git calls.","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-cGF0dGVybi1j","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-2a-hook-efficiency.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/pattern-check.js","line":0,"title":"pattern-check.js calls fs.realpathSync twice (file and project dir)","description":"pattern-check.js calls fs.realpathSync on both the target file path and the project directory on every invocation. realpathSync resolves symlinks by making multiple OS calls. The project directory never changes within a session, so resolving it every time is wasteful. Combined with the statSync call, this is 3 synchronous filesystem metadata operations per hook invocation.","recommendation":"Cache the resolved project directory path. It could be computed once at module load time since it will not change. This saves one realpathSync call per invocation.","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-ZG9jLW9wdGlt","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-1c-instruction-bloat.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/doc-optimizer/SKILL.md","line":0,"title":"doc-optimizer SKILL.md has duplicated CRITICAL RETURN PROTOCOL across 13 agent prompts","description":"58KB SKILL.md with ~800+ lines; each of 13 agent prompts repeats the same CRITICAL RETURN PROTOCOL boilerplate, adding ~400 lines of duplication that increases token cost when the skill is loaded","recommendation":"Extract the CRITICAL RETURN PROTOCOL to a shared section at the top of the skill and reference it from each agent prompt, or create an external template file","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-QXVkaXQgc2tp","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-1c-instruction-bloat.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/skills/audit-comprehensive/SKILL.md","line":0,"title":"Audit skills share >50% boilerplate content without shared base","description":"Each audit skill independently defines Persistence Rules, Stage Structure, Output Format, and checkpoint logic â€” over 50% shared content across 9 audit skills totaling ~4000 duplicated lines","recommendation":"Create .claude/skills/audit-shared-base.md with common sections (persistence rules, output format, checkpoints, TDMS integration) and reference it from each audit SKILL.md","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-TXVsdGktbGlu","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-1b-parsing-format.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/auto-save-context.js","line":0,"title":"Multi-line regex with greedy \\[\\s\\S\\]*? quantifier in auto-save-context.js","description":"Line 126 uses [\\s\\S]*? for multi-line markdown section matching â€” potential ReDoS on crafted input; should use line-by-line parsing per two-strikes rule","recommendation":"Replace regex with line-split parsing: split on \\n, find section start, scan until next ## or ---","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-SlNPTkwgbGlu","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-1b-parsing-format.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/session-start.js","line":0,"title":"JSONL line counting pattern repeated 5x without abstraction","description":".trim().split('\\n').filter(Boolean).length pattern repeated 5 times across hooks/scripts; a shared helper would reduce duplication and ensure consistent behavior","recommendation":"Extract to scripts/lib/count-jsonl-lines.js helper and import in all 5 locations","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-U3RyaW5nIGNv","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-1b-parsing-format.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":"scripts/archive-doc.js","line":0,"title":"String concatenation for file paths instead of path.join","description":"Using filePath + '.tmp' and filePath + '.bak' instead of path operations; while unlikely to break on consistent OS, violates cross-platform safety patterns","recommendation":"Replace with path-based operations or template literals with documented intent","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-RnJhZ2lsZSBt","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-1b-parsing-format.jsonl","original_id":null,"category":"code-quality","severity":"S2","type":"code-smell","file":".claude/hooks/commit-tracker.js","line":0,"title":"Fragile markdown field parsing regex repeated across 3+ scripts","description":"Patterns like /Current Session Count(er)?.*:(\\d+)/i appear in 3+ scripts with slight variations; fragile to markdown formatting changes and duplicated maintenance burden","recommendation":"Extract to scripts/lib/parse-markdown-field.js with parseMarkdownField(content, fieldName) helper","effort":"E1","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-SG9va3Mgb3V0","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-1b-parsing-format.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/hooks/auto-save-context.js","line":0,"title":"Hooks output decorative error messages instead of machine-readable format","description":"Hook console.error blocks use 5-12 line decorative formatting with borders and emoji for errors that are only machine-consumed; wastes tokens in conversation context","recommendation":"Use structured JSON stderr output for hook errors; reserve decorative output for user-facing messages only","effort":"E2","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-VW51c2VkIGZp","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-1b-parsing-format.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":".claude/state/commit-log.jsonl","line":0,"title":"Unused fields in commit-log.jsonl state file","description":"authorDate and author fields in commit-log.jsonl entries are often null or unused by any consumer script; removing could reduce file size by 15-20%","recommendation":"Remove unused fields from commit-tracker.js output; verify no script reads these fields","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-VEVDSE5JQ0FM","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-1a-dead-assets.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":"docs/plans/TECHNICAL_DEBT_MANAGEMENT_SYSTEM_PLAN.md","line":0,"title":"TECHNICAL_DEBT_MANAGEMENT_SYSTEM_PLAN.md marked COMPLETE but still in active plans/","description":"Completed plan marked as 'âœ… COMPLETE - All 18 Phases Implemented' remains in active docs/plans/ directory instead of docs/archive/completed-plans/, adding clutter to active planning docs","recommendation":"Move to docs/archive/completed-plans/ and update any cross-references","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
{"source_id":"audit:hash-U0VTU0lPTl9I","source_file":"docs\\audits\\single-session\\ai-optimization\\audit-2026-02-14\\stage-1a-dead-assets.jsonl","original_id":null,"category":"code-quality","severity":"S3","type":"code-smell","file":"docs/SESSION_HISTORY.md","line":0,"title":"SESSION_HISTORY.md and SESSION_DECISIONS.md not in DOCUMENTATION_INDEX.md","description":"Two active documentation files exist in docs/ but are not tracked in the documentation index, making them invisible to automated doc tooling and cross-reference checks","recommendation":"Run npm run docs:index to regenerate DOCUMENTATION_INDEX.md, or add entries manually","effort":"E0","status":"NEW","roadmap_ref":null,"created":"2026-02-15","verified_by":null,"resolution":null,"evidence":[],"sources":[],"merged_from":[]}
