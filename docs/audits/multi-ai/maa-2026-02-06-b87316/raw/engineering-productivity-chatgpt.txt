CAPABILITIES: browse_files=yes, run_commands=yes, repo_checkout=yes, limitations="I can inspect the uploaded repo snapshot and run local grep/node commands, but I cannot measure real CI runtimes or validate external services from here."

BASELINE_METRICS_JSON

{"audit_date":"2026-02-06","npm_scripts_count":65,"console_log_count":13,"logger_count":158,"structured_logging_ratio":"13:158","service_worker":false,"firebase_persistence":false,"ci_workflow_count":10,"test_count":20}

FINDINGS_JSONL

{"category":"GoldenPath","title":"Install/build can fail due to missing local file dependency (@dataconnect/generated)","fingerprint":"engineering-productivity::package.json::missing-local-file-dependency","severity":"S0","effort":"E1","confidence":95,"files":["package.json","DOCUMENTATION_INDEX.md"],"why_it_matters":"A fresh clone may fail npm ci because a dependency points at a missing local folder. This blocks onboarding and makes CI brittle.","suggested_fix":"Decide the source of truth for src/dataconnect-generated: (A) commit it, (B) generate it deterministically in a preinstall/prepare step + CI, or (C) remove/replace the dependency with a published package.","acceptance_tests":["rm -rf node_modules && npm ci","npm run build","npm run test:coverage"],"evidence":["package.json:76\n \"@dataconnect/generated\": \"file:src/dataconnect-generated\",","repo filesystem check:\n $ test -d src/dataconnect-generated && echo OK || echo MISSING\n MISSING","DOCUMENTATION_INDEX.md:464+ links to src/dataconnect-generated/* that do not exist"],"line":76}
{"category":"GoldenPath","title":"DEVELOPMENT.md references npm script type-check that does not exist","fingerprint":"engineering-productivity::DEVELOPMENT.md::missing-npm-script-type-check","severity":"S1","effort":"E0","confidence":100,"files":["DEVELOPMENT.md","package.json"],"why_it_matters":"Onboarding instructions point to a command that fails immediately, creating trust erosion and wasted time during setup and troubleshooting.","suggested_fix":"Either add \"type-check\": \"tsc --noEmit\" to package.json or update DEVELOPMENT.md to reference the existing command (npx tsc --noEmit) consistently.","acceptance_tests":["npm run --list | grep type-check (or verify doc references npx tsc)","npm run type-check (if added)"],"evidence":["DEVELOPMENT.md:848\n - Check TypeScript errors: npm run type-check","DEVELOPMENT.md:986\n - [ ] Type check passes (npm run type-check)","package.json scripts: no \"type-check\" entry (only CI uses npx tsc --noEmit)"],"line":848}
{"category":"GoldenPath","title":"README lacks runnable Quick Start commands (no npm ci/dev/test instructions)","fingerprint":"engineering-productivity::README.md::missing-quick-start-commands","severity":"S2","effort":"E0","confidence":90,"files":["README.md","DEVELOPMENT.md"],"why_it_matters":"New contributors expect the README to get them to a running app fast. When Quick Start is not actionable, setup time increases and people bounce to other docs or guess.","suggested_fix":"Add a minimal Quick Start block to README: npm ci, copy .env.local.example â†’ .env.local, npm run dev, and the fastest test command. Link to DEVELOPMENT.md for deeper setup.","acceptance_tests":["grep -n \"npm ci\\|npm run dev\\|npm test\" README.md returns matches","Fresh clone: follow README only to start dev server"],"evidence":["README.md has a Quick Start bullet in the Purpose section but no commands present.\n $ grep -n \"npm ci\\|npm run dev\\|npm test\" README.md\n (no matches)"],"line":1}
{"category":"GoldenPath","title":"Environment example file naming is non-standard (.env.local.example; no .env.example)","fingerprint":"engineering-productivity::.env.local.example::nonstandard-env-example-naming","severity":"S3","effort":"E0","confidence":95,"files":[".env.local.example"],"why_it_matters":"Many tools and contributor expectations look for .env.example. Non-standard naming causes small but recurring friction and confusion in onboarding guides.","suggested_fix":"Either add a copy/symlinked .env.example (pointing to .env.local.example) or document explicitly in README Quick Start which file to copy and why.","acceptance_tests":["test -f .env.example (if added)","README Quick Start references the correct env example file"],"evidence":["Repo contains .env.local.example but not .env.example:\n $ test -f .env.example && echo YES || echo NO\n NO"],"line":1}
{"category":"Testing","title":"Tests require a compile step (tsc + tsc-alias) before running; slows local feedback and hooks","fingerprint":"engineering-productivity::package.json::tests-require-build-step","severity":"S1","effort":"E2","confidence":95,"files":["package.json","tsconfig.test.json"],"why_it_matters":"Requiring a build step before every test run increases iteration time and makes pre-commit slower, especially for small changes. Developers feel this daily.","suggested_fix":"Consider a faster test runner flow: (A) run Node tests directly on TS via tsx in dev, or (B) switch to a TS-native runner (Vitest) while keeping current build-based flow for CI if desired. At minimum, add npm run test:watch and a test:fast path that skips full build when possible.","acceptance_tests":["Introduce npm run test:watch and confirm it runs in <5s startup on a warm cache","CI still runs a deterministic test command (coverage)"],"evidence":["package.json:10-12\n10 \"test\": \"npm run test:build && ... node --test \\\"dist-tests/tests//*.test.js\\\"\",\n11 \"test:build\": \"tsc -p tsconfig.test.json && tsc-alias -p tsconfig.test.json\",\n12 \"test:coverage\": \"npm run test:build && c8 ... node --test ...\""],"line":10}
{"category":"Testing","title":"Repo context expects Jest but project uses Node's built-in test runner; tooling expectation mismatch","fingerprint":"engineering-productivity::package.json::jest-mismatch","severity":"S2","effort":"E0","confidence":90,"files":["package.json",".github/workflows/ci.yml"],"why_it_matters":"When teams assume Jest, they look for jest.config.*, Jest CLI flags, and Jest ecosystem docs. Mismatch causes confusion and slows debugging test failures.","suggested_fix":"Update DEVELOPMENT.md/README to explicitly state the test runner (node --test) and how to run focused tests. Optionally add convenience scripts like test:file or test:watch to match what people expect from Jest workflows.","acceptance_tests":["Docs clearly state runner and show a \"run one test file\" command","New contributors can run a single test without reading scripts"],"evidence":["package.json uses node --test (no jest dependency):\n \"test\": \"... node --test \\\"dist-tests/tests//.test.js\\\"\"","node -e check shows no jest/vitest devDependency (both undefined)"],"line":10}
{"category":"Debugging","title":"Console.log used in production paths (firestore-service, TodayPage) instead of logger with levels","fingerprint":"engineering-productivity::lib/firestore-service.ts::console-log-in-prod-paths","severity":"S2","effort":"E1","confidence":95,"files":["lib/firestore-service.ts","components/notebook/pages/today-page.tsx"],"why_it_matters":"Ad-hoc console logging is noisy, inconsistent, and hard to filter. It also risks leaking data and makes debugging harder because logs aren't normalized.","suggested_fix":"Replace non-example console logs with logger.debug/info (or remove if purely diagnostic). Add a single debug flag mechanism (env var) to enable verbose logs locally without shipping noisy logs.","acceptance_tests":["grep -R \"console\\.log\" app lib components returns only examples/docs or zero","Run a key flow and confirm logs are structured and level-controlled"],"evidence":["lib/firestore-service.ts:229\n console.log(\"ðŸ“¤ Sending to Cloud Function:\", JSON.stringify(sanitizedPayload, null, 2));","components/notebook/pages/today-page.tsx:728+\n console.log(\"ðŸ“Š Weekly Stats Debug:\", { ... });"],"line":229}
{"category":"Debugging","title":"Logger is present but lacks correlation/request IDs; tracing across async flows is difficult","fingerprint":"engineering-productivity::lib/logger.ts::missing-correlation-ids","severity":"S2","effort":"E2","confidence":90,"files":["lib/logger.ts","components/providers/error-boundary.tsx"],"why_it_matters":"Without a correlation ID, it's hard to connect logs from UI actions, cloud function calls, and error boundaries into a single timeline. Debug sessions take longer and fixes are riskier.","suggested_fix":"Add optional requestId/correlationId to logger context and propagate it through: UI action â†’ secure-caller â†’ callable payload (and back). Consider generating one per user interaction and attaching to error boundary exports.","acceptance_tests":["grep -R \"correlationId\\|x-request-id\" shows new implementation points","A single user action emits multiple logs sharing the same correlationId"],"evidence":["lib/logger.ts:99-106 payload has level/message/timestamp/environment but no request/correlation id field","repo-wide search:\n $ grep -R \"correlationId\\|x-request-id\" -n --include='.ts' --include='.tsx' .\n (no matches)"],"line":99}
{"category":"Offline","title":"Offline UI promises \"changes will sync\", but no service worker or Firestore persistence is enabled","fingerprint":"engineering-productivity::components/status/offline-indicator.tsx::offline-promise provides-no-infra","severity":"S1","effort":"E2","confidence":95,"files":["components/status/offline-indicator.tsx","public/manifest.json"],"why_it_matters":"Users/devs will assume offline writes are queued and safe. Without persistence/queue, offline edits can fail silently or be lost, and devs will spend time chasing inconsistent behavior.","suggested_fix":"Either (A) implement real offline support (Firestore persistence + queued writes + conflict strategy), or (B) downgrade the messaging to a truthful indicator (offline status only) until infra exists. Add a clear offline support section in docs.","acceptance_tests":["Confirm Firestore persistence call exists (e.g., enableIndexedDbPersistence or equivalent) OR offline indicator text no longer claims sync","Add a basic offline write test plan (manual) in DEVELOPMENT.md"],"evidence":["components/status/offline-indicator.tsx:42-47\n42 if (!isOnline) {\n46 <span>Offline - changes will sync when reconnected</span>","Service worker check:\n $ test -f public/sw.js && echo YES || echo NO\n NO","Persistence check:\n $ grep -R \"enableIndexedDbPersistence\" -n lib\n (no matches)"],"line":46}
{"category":"Offline","title":"PWA install prompt exists but no service worker; offline shell likely incomplete","fingerprint":"engineering-productivity::components/pwa/install-prompt.tsx::pwa-without-sw","severity":"S2","effort":"E2","confidence":85,"files":["components/pwa/install-prompt.tsx","public/manifest.json"],"why_it_matters":"Developers will expect PWA install + offline shell to work together. Having install UX without the offline plumbing causes confusing QA and user-reported bugs.","suggested_fix":"Add a service worker strategy (Next/PWA tooling or a minimal custom SW) or clearly scope the feature as \"install-only\" until SW is implemented. Ensure caching strategy is documented and tested.","acceptance_tests":["public/sw.js exists and is registered OR docs/UX clearly state install-only","Manual: go offline and confirm app shell loads (if SW implemented)"],"evidence":["components/pwa/install-prompt.tsx includes offline install messaging:\n \"Install SoNash to your home screen for quick access offline!\"","public contains manifest.json but no sw.js/service-worker.js"],"line":104}
{"category":"CI_CD","title":"CI is a single large job (lint+format+deps+tests+validations); missed parallelization opportunities","fingerprint":"engineering-productivity::.github/workflows/ci.yml::single-job-bottleneck","severity":"S2","effort":"E2","confidence":90,"files":[".github/workflows/ci.yml"],"why_it_matters":"When everything runs serially, CI latency increases and developers wait longer for feedback. Splitting independent steps into parallel jobs can materially improve throughput.","suggested_fix":"Split CI into parallel jobs: (1) lint/format/patterns, (2) typecheck, (3) tests+coverage, (4) docs/schema validations. Use needs/outputs to keep gating behavior. Add path filters so doc-only PRs skip heavy jobs.","acceptance_tests":["CI completes faster on typical PRs (measure in GitHub Actions)","All required checks still gate merges appropriately"],"evidence":[".github/workflows/ci.yml:8-16\njobs:\n lint-typecheck-test:\n name: Lint, Type Check & Test\n runs-on: ubuntu-latest\n(only one job defined)"],"line":8}
{"category":"Engineering","title":"No single 'golden path' command that mirrors CI gates (missing npm run check)","fingerprint":"engineering-productivity::package.json::missing-unified-check-command","severity":"S2","effort":"E0","confidence":95,"files":["package.json"],"why_it_matters":"Developers shouldn't have to remember many separate commands to match CI. A single local command reduces mistakes and speeds up pre-PR confidence checks.","suggested_fix":"Add npm run check that runs the same core gates as CI in a sensible order (lint â†’ format:check â†’ deps: â†’ patterns:check â†’ typecheck â†’ test). Keep check:fast for quick iterations.","acceptance_tests":["npm run check exits 0 on clean tree","npm run check fails when lint/test/typecheck fails (and messages are actionable)"],"evidence":["package.json includes many *:check scripts but none named check:\n scripts include: agents:check, docs:check, format:check, patterns:check, security:check, triggers:check"],"line":1}

SUSPECTED_FINDINGS_JSONL

{"category":"CI_CD","title":"CI runtime may exceed 10 minutes for typical PRs due to serial job + coverage build step","fingerprint":"engineering-productivity::.github/workflows/ci.yml::ci-runtime-unknown","severity":"S2","effort":"E2","confidence":35,"files":[".github/workflows/ci.yml","package.json"],"why_it_matters":"If CI routinely runs long, developers batch changes and context-switch, which reduces velocity. Without measurements, this is a hypothesis.","suggested_fix":"Measure average CI duration on last 20 PRs, then apply the parallelization + test speedups if needed.","acceptance_tests":["Export CI timing metrics (GitHub Actions)","CI median time drops after changes"],"evidence":["No CI timing data is stored in repo; must be measured from GitHub Actions UI/logs."],"line":1}
{"category":"Testing","title":"Coverage percentage target and current baseline are not visible in repo docs; may cause unclear expectations","fingerprint":"engineering-productivity::package.json::coverage-baseline-unknown","severity":"S3","effort":"E0","confidence":40,"files":["package.json","DEVELOPMENT.md"],"why_it_matters":"When coverage expectations aren't explicit, teams argue about what's \"enough,\" and regressions slip in unnoticed.","suggested_fix":"Document current coverage baseline and a target (or \"no regression\" policy). If already tracked elsewhere, link it from DEVELOPMENT.md.","acceptance_tests":["CI logs/report publish the % and enforce policy (threshold or non-regression)"],"evidence":["package.json runs c8 coverage, but no visible threshold/policy in docs was confirmed in this pass."],"line":1}
