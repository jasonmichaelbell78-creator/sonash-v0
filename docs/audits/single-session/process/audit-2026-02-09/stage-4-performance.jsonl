{"category":"process","title":"Slow: ESLint full codebase scan in pre-commit","fingerprint":"process::.husky/pre-commit::slow-eslint-full-scan","severity":"S2","effort":"E1","confidence":"HIGH","files":[".husky/pre-commit:9"],"why_it_matters":"ESLint scans entire codebase (~3-10s) on every commit instead of only staged files. Developers wait unnecessarily even for small changes. Over time this encourages --no-verify bypassing.","suggested_fix":"Use 'npm run lint -- --cache' for caching, or integrate with lint-staged to check only staged files. lint-staged already runs Prettier (line 23), could also run ESLint on same files.","acceptance_tests":["ESLint completes in <2s for typical commits","ESLint still catches errors before commit","Cache works between commits"],"file":".husky/pre-commit","line":9,"description":"ESLint scans entire codebase (~3-10s) on every commit instead of only staged files. Developers wait unnecessarily even for small changes. Over time this encourages --no-verify bypassing.","recommendation":"Use 'npm run lint -- --cache' for caching, or integrate with lint-staged to check only staged files. lint-staged already runs Prettier (line 23), could also run ESLint on same files.","id":"process::.husky/pre-commit::slow-eslint-full-scan"}
{"category":"process","title":"Duplicate: Pattern compliance runs in both pre-commit and pre-push","fingerprint":"process::.husky/pre-commit+pre-push::duplicate-pattern-check","severity":"S2","effort":"E1","confidence":"HIGH","files":[".husky/pre-commit:35",".husky/pre-push:26"],"why_it_matters":"'npm run patterns:check' runs twice - once in pre-commit (line 35) and again in pre-push (line 26). Same files checked twice adds 1-3s per push. Pure waste since files don't change between commit and push.","suggested_fix":"Remove pattern check from pre-commit OR pre-push. Recommend keeping in pre-commit only (fail fast) and removing from pre-push line 21-35. Pattern violations should be caught at commit time.","acceptance_tests":["Pattern check runs only once per commit/push cycle","Total hook time reduced by 1-3s","All pattern violations still caught"],"file":".husky/pre-commit","line":35,"description":"'npm run patterns:check' runs twice - once in pre-commit (line 35) and again in pre-push (line 26). Same files checked twice adds 1-3s per push. Pure waste since files don't change between commit and push.","recommendation":"Remove pattern check from pre-commit OR pre-push. Recommend keeping in pre-commit only (fail fast) and removing from pre-push line 21-35. Pattern violations should be caught at commit time.","id":"process::.husky/pre-commit+pre-push::duplicate-pattern-check"}
{"category":"process","title":"Slow: TypeScript full project type check on every push","fingerprint":"process::.husky/pre-push::slow-tsc-no-incremental","severity":"S2","effort":"E2","confidence":"HIGH","files":[".husky/pre-push:83"],"why_it_matters":"'npx tsc --noEmit' does full project type check (5-15s) on every push with no incremental caching. For large projects this becomes painful. Developers may skip with git push --no-verify.","suggested_fix":"1) Use 'tsc --noEmit --incremental' with tsbuildinfo caching, OR 2) Use 'tsc --noEmit --pretty' with file filtering for changed files only, OR 3) Move to CI and make optional in pre-push with SKIP_TYPE_CHECK=1 override.","acceptance_tests":["Type check completes in <5s with caching","Incremental builds work correctly","Type errors still caught before push"],"file":".husky/pre-push","line":83,"description":"'npx tsc --noEmit' does full project type check (5-15s) on every push with no incremental caching. For large projects this becomes painful. Developers may skip with git push --no-verify.","recommendation":"1) Use 'tsc --noEmit --incremental' with tsbuildinfo caching, OR 2) Use 'tsc --noEmit --pretty' with file filtering for changed files only, OR 3) Move to CI and make optional in pre-push with SKIP_TYPE_CHECK=1 override.","id":"process::.husky/pre-push::slow-tsc-no-incremental"}
{"category":"process","title":"Slow: Circular dependency scan on entire codebase every push","fingerprint":"process::.husky/pre-push::slow-madge-full-scan","severity":"S2","effort":"E2","confidence":"HIGH","files":[".husky/pre-push:12"],"why_it_matters":"'madge --circular' scans lib/, components/, app/ directories (2-5s) on every push regardless of what changed. Circular deps are architectural issues that rarely appear in normal development. Full scan is overkill.","suggested_fix":"1) Add madge caching or run incrementally on changed files only, OR 2) Move to CI as a scheduled check (daily/weekly), OR 3) Make optional with SKIP_CIRCULAR_CHECK=1 and run only when dependency files change (package.json, imports in changed files).","acceptance_tests":["Circular dep check skipped when no dependency changes","Optional override documented","Architectural issues still caught in CI"],"file":".husky/pre-push","line":12,"description":"'madge --circular' scans lib/, components/, app/ directories (2-5s) on every push regardless of what changed. Circular deps are architectural issues that rarely appear in normal development. Full scan is overkill.","recommendation":"1) Add madge caching or run incrementally on changed files only, OR 2) Move to CI as a scheduled check (daily/weekly), OR 3) Make optional with SKIP_CIRCULAR_CHECK=1 and run only when dependency files change (package.json, imports in changed files).","id":"process::.husky/pre-push::slow-madge-full-scan"}
{"category":"process","title":"Slow: Test suite rebuilds TypeScript on every test run","fingerprint":"process::package.json::slow-test-build","severity":"S2","effort":"E2","confidence":"HIGH","files":["package.json:11"],"why_it_matters":"'npm test' runs 'test:build' which compiles ALL TypeScript (tsc + tsc-alias) before tests. Pre-commit runs tests (line 59/80) adding 10-30s for full compilation even for small changes. No incremental compilation.","suggested_fix":"1) Use 'tsc --incremental' in test:build for caching, OR 2) Use tsx/ts-node for on-the-fly compilation without build step, OR 3) Only run test:build when test files or dependencies change (check git diff).","acceptance_tests":["Test build time reduced by 60-80% for incremental changes","Tests still run correctly","tsconfig.test.json still respected"],"file":"package.json","line":11,"description":"'npm test' runs 'test:build' which compiles ALL TypeScript (tsc + tsc-alias) before tests. Pre-commit runs tests (line 59/80) adding 10-30s for full compilation even for small changes. No incremental compilation.","recommendation":"1) Use 'tsc --incremental' in test:build for caching, OR 2) Use tsx/ts-node for on-the-fly compilation without build step, OR 3) Only run test:build when test files or dependencies change (check git diff).","id":"process::package.json::slow-test-build"}
{"category":"process","title":"Inefficient: Sequential security checks in pre-push","fingerprint":"process::.husky/pre-push::sequential-security-checks","severity":"S3","effort":"E2","confidence":"HIGH","files":[".husky/pre-push:50-65"],"why_it_matters":"Pre-push security checks loop through changed files sequentially (lines 50-65), running 'node scripts/security-check.js --file' once per file. For 10+ changed files, this adds 5-15s. No parallelization means cores sit idle.","suggested_fix":"1) Modify security-check.js to accept multiple files at once, OR 2) Use xargs -P4 for parallel execution, OR 3) Rewrite loop to spawn checks in parallel and wait for all results. Example: 'echo \"$changed_files\" | xargs -P4 -I{} node scripts/security-check.js --file {}'","acceptance_tests":["Security checks run in parallel (4+ concurrent)","Total time reduced by 50-70% for multi-file changes","All security issues still detected"],"file":".husky/pre-push","line":50,"description":"Pre-push security checks loop through changed files sequentially (lines 50-65), running 'node scripts/security-check.js --file' once per file. For 10+ changed files, this adds 5-15s. No parallelization means cores sit idle.","recommendation":"1) Modify security-check.js to accept multiple files at once, OR 2) Use xargs -P4 for parallel execution, OR 3) Rewrite loop to spawn checks in parallel and wait for all results. Example: 'echo \"$changed_files\" | xargs -P4 -I{} node scripts/security-check.js --file {}'","id":"process::.husky/pre-push::sequential-security-checks"}
{"category":"process","title":"Performance: 10 Claude hooks run on every Write/Edit operation","fingerprint":"process::.claude/settings.json::many-posttooluse-hooks","severity":"S2","effort":"E3","confidence":"HIGH","files":[".claude/settings.json:57-111"],"why_it_matters":"Every Write/Edit/MultiEdit triggers 10 separate hooks (check-write-requirements, audit-s0s1-validator, pattern-check, component-size-check, firestore-write-block, test-mocking-validator, app-check-validator, typescript-strict-check, repository-pattern-check, agent-trigger-enforcer). Total: ~5818 lines of hook code. Each hook spawns node process, adds 1-3s latency per file write. Poor DX during active development.","suggested_fix":"1) Batch-execute hooks in single node process to avoid spawn overhead, OR 2) Make hooks async/parallel where possible, OR 3) Add smart skipping - only run relevant hooks based on file type (e.g., firestore-write-block only for firestore files), OR 4) Move some non-critical checks to pre-commit only.","acceptance_tests":["Write/Edit latency reduced to <1s total","Hook logic preserved (no false negatives)","File type filtering works correctly"],"file":".claude/settings.json","line":57,"description":"Every Write/Edit/MultiEdit triggers 10 separate hooks (check-write-requirements, audit-s0s1-validator, pattern-check, component-size-check, firestore-write-block, test-mocking-validator, app-check-validator, typescript-strict-check, repository-pattern-check, agent-trigger-enforcer). Total: ~5818 lines of hook code. Each hook spawns node process, adds 1-3s latency per file write. Poor DX during active development.","recommendation":"1) Batch-execute hooks in single node process to avoid spawn overhead, OR 2) Make hooks async/parallel where possible, OR 3) Add smart skipping - only run relevant hooks based on file type (e.g., firestore-write-block only for firestore files), OR 4) Move some non-critical checks to pre-commit only.","id":"process::.claude/settings.json::many-posttooluse-hooks"}
{"category":"process","title":"Inefficient: Pattern check runs on every file write via Claude hook","fingerprint":"process::.claude/hooks/pattern-check.js::per-file-overhead","severity":"S3","effort":"E2","confidence":"HIGH","files":[".claude/hooks/pattern-check.js:1",".claude/settings.json:73,123"],"why_it_matters":"Pattern-check.js hook (line 73, 123 in settings.json) runs check-pattern-compliance.js on EVERY file write/edit during Claude sessions. For quick doc edits or small changes, running full pattern checker adds 0.5-2s overhead. Pre-commit already runs patterns:check (line 35), making this redundant during development.","suggested_fix":"1) Make pattern-check.js conditional - skip for .md/.txt/docs files, OR 2) Add debouncing - only check after N writes or M seconds, OR 3) Remove hook and rely solely on pre-commit pattern check (fail fast at commit, not during writing), OR 4) Make hook informational only (warn but don't block).","acceptance_tests":["Pattern check skipped for non-code files","Write latency <0.5s for docs","Pattern violations still caught at commit time"],"file":".claude/hooks/pattern-check.js","line":1,"description":"Pattern-check.js hook (line 73, 123 in settings.json) runs check-pattern-compliance.js on EVERY file write/edit during Claude sessions. For quick doc edits or small changes, running full pattern checker adds 0.5-2s overhead. Pre-commit already runs patterns:check (line 35), making this redundant during development.","recommendation":"1) Make pattern-check.js conditional - skip for .md/.txt/docs files, OR 2) Add debouncing - only check after N writes or M seconds, OR 3) Remove hook and rely solely on pre-commit pattern check (fail fast at commit, not during writing), OR 4) Make hook informational only (warn but don't block).","id":"process::.claude/hooks/pattern-check.js::per-file-overhead"}
{"category":"process","title":"Inefficient: Multiple git status/diff scans in pre-commit","fingerprint":"process::.husky/pre-commit::multiple-git-scans","severity":"S3","effort":"E2","confidence":"MEDIUM","files":[".husky/pre-commit:47,94,138,159,194"],"why_it_matters":"Pre-commit runs 'git diff --cached --name-only' separately at lines 47, 94, 138, 159, 194. Each git call adds 50-200ms overhead. For repos with many files, this compounds to 0.5-1s wasted on duplicate filesystem scans.","suggested_fix":"Run 'git diff --cached --name-only' ONCE at the top of pre-commit hook and store in STAGED_FILES variable. Reuse this variable throughout. Already done partially (line 47, 94 reuse), but lines 138, 159, 194 run fresh git commands. Consolidate all into single scan.","acceptance_tests":["Only one git diff command executed","Hook behavior unchanged","Time savings: 0.3-0.8s per commit"],"file":".husky/pre-commit","line":47,"description":"Pre-commit runs 'git diff --cached --name-only' separately at lines 47, 94, 138, 159, 194. Each git call adds 50-200ms overhead. For repos with many files, this compounds to 0.5-1s wasted on duplicate filesystem scans.","recommendation":"Run 'git diff --cached --name-only' ONCE at the top of pre-commit hook and store in STAGED_FILES variable. Reuse this variable throughout. Already done partially (line 47, 94 reuse), but lines 138, 159, 194 run fresh git commands. Consolidate all into single scan.","id":"process::.husky/pre-commit::multiple-git-scans"}
{"category":"process","title":"Risk: No timeout on npm test in pre-commit","fingerprint":"process::.husky/pre-commit::no-test-timeout","severity":"S3","effort":"E1","confidence":"MEDIUM","files":[".husky/pre-commit:59,80"],"why_it_matters":"'npm test' runs without timeout in pre-commit (lines 59, 80). If test hangs due to async issue, developer waits indefinitely or force-quits, losing context. No escape hatch besides killing terminal. Degraded DX for rare but frustrating hangs.","suggested_fix":"Add timeout wrapper: 'timeout 120 npm test' (120s = 2 min). If tests hang beyond reasonable time, hook fails fast with clear timeout message. Document with: 'Tests timed out after 120s. Check for hanging async operations or use SKIP_TESTS=1 for emergency commit.'","acceptance_tests":["Tests timeout after 120s if hung","Clear timeout error message shown","Normal test runs unaffected"],"file":".husky/pre-commit","line":59,"description":"'npm test' runs without timeout in pre-commit (lines 59, 80). If test hangs due to async issue, developer waits indefinitely or force-quits, losing context. No escape hatch besides killing terminal. Degraded DX for rare but frustrating hangs.","recommendation":"Add timeout wrapper: 'timeout 120 npm test' (120s = 2 min). If tests hang beyond reasonable time, hook fails fast with clear timeout message. Document with: 'Tests timed out after 120s. Check for hanging async operations or use SKIP_TESTS=1 for emergency commit.'","id":"process::.husky/pre-commit::no-test-timeout"}
{"category":"process","title":"Optimization: Doc-only commit detection could be smarter","fingerprint":"process::.husky/pre-commit::doc-only-detection-complexity","severity":"S3","effort":"E2","confidence":"MEDIUM","files":[".husky/pre-commit:68-88"],"why_it_matters":"Doc-only commit detection (lines 68-88) uses complex regex grep filtering to skip tests. Logic is hard to maintain and test. False positives (docs with critical info) or false negatives (code masquerading as docs) could occur. Current regex at line 71 is 100+ chars long.","suggested_fix":"1) Extract doc-only detection to dedicated script 'scripts/is-doc-only-commit.js' with unit tests, OR 2) Use git diff --name-status with explicit allowlist (docs/, *.md, *.png, *.jsonl) instead of complex exclusion regex, OR 3) Make SKIP_TESTS=1 the default for docs and auto-detect risky files (package.json, tsconfig, etc.) to force tests.","acceptance_tests":["Doc-only detection logic unit tested","Clear separation of concerns","False positive/negative rate <1%"],"file":".husky/pre-commit","line":68,"description":"Doc-only commit detection (lines 68-88) uses complex regex grep filtering to skip tests. Logic is hard to maintain and test. False positives (docs with critical info) or false negatives (code masquerading as docs) could occur. Current regex at line 71 is 100+ chars long.","recommendation":"1) Extract doc-only detection to dedicated script 'scripts/is-doc-only-commit.js' with unit tests, OR 2) Use git diff --name-status with explicit allowlist (docs/, *.md, *.png, *.jsonl) instead of complex exclusion regex, OR 3) Make SKIP_TESTS=1 the default for docs and auto-detect risky files (package.json, tsconfig, etc.) to force tests.","id":"process::.husky/pre-commit::doc-only-detection-complexity"}
{"category":"process","title":"Slow: Session start hooks add 2-5s latency to every session","fingerprint":"process::.claude/settings.json::slow-session-start","severity":"S3","effort":"E3","confidence":"MEDIUM","files":[".claude/settings.json:8-44"],"why_it_matters":"SessionStart hooks run 4 sequential node processes (session-start.js, check-mcp-servers.js, check-remote-session-context.js, stop-serena-dashboard.js) on EVERY Claude session start. Total latency: 2-5s before developer can begin work. Compounds frustration for quick questions/checks. Remote session check can be slow if network latency high.","suggested_fix":"1) Parallelize independent hooks (session-start, check-mcp-servers, stop-serena can run concurrently), OR 2) Make check-remote-session-context.js async/non-blocking with background notification, OR 3) Add cache TTL - skip checks if last run was <5min ago, OR 4) Optimize scripts - combine into single process to avoid spawn overhead.","acceptance_tests":["Session start latency <1s for cached/local operations","Hooks run in parallel where possible","Functionality preserved (no missed checks)"],"file":".claude/settings.json","line":8,"description":"SessionStart hooks run 4 sequential node processes (session-start.js, check-mcp-servers.js, check-remote-session-context.js, stop-serena-dashboard.js) on EVERY Claude session start. Total latency: 2-5s before developer can begin work. Compounds frustration for quick questions/checks. Remote session check can be slow if network latency high.","recommendation":"1) Parallelize independent hooks (session-start, check-mcp-servers, stop-serena can run concurrently), OR 2) Make check-remote-session-context.js async/non-blocking with background notification, OR 3) Add cache TTL - skip checks if last run was <5min ago, OR 4) Optimize scripts - combine into single process to avoid spawn overhead.","id":"process::.claude/settings.json::slow-session-start"}
{"category":"process","title":"Optimization: UserPromptSubmit hooks run before every user message","fingerprint":"process::.claude/settings.json::user-prompt-overhead","severity":"S3","effort":"E2","confidence":"MEDIUM","files":[".claude/settings.json:265-290"],"why_it_matters":"UserPromptSubmit hooks (lines 265-290) run 4 checks (alerts-reminder, analyze-user-request, session-end-reminder, plan-mode-suggestion) before processing EVERY user prompt. Adds 0.5-2s perceived latency. For rapid back-and-forth conversations, this degrades conversational flow.","suggested_fix":"1) Make hooks async - run in background and surface results after response, OR 2) Add smart throttling - only run every Nth prompt or when certain keywords detected, OR 3) Batch hooks into single process to reduce spawn overhead, OR 4) Cache results - skip redundant checks if recent prompt was similar.","acceptance_tests":["Prompt processing starts immediately (<200ms)","Hooks run asynchronously without blocking","Important alerts still surfaced reliably"],"file":".claude/settings.json","line":265,"description":"UserPromptSubmit hooks (lines 265-290) run 4 checks (alerts-reminder, analyze-user-request, session-end-reminder, plan-mode-suggestion) before processing EVERY user prompt. Adds 0.5-2s perceived latency. For rapid back-and-forth conversations, this degrades conversational flow.","recommendation":"1) Make hooks async - run in background and surface results after response, OR 2) Add smart throttling - only run every Nth prompt or when certain keywords detected, OR 3) Batch hooks into single process to reduce spawn overhead, OR 4) Cache results - skip redundant checks if recent prompt was similar.","id":"process::.claude/settings.json::user-prompt-overhead"}
{"category":"process","title":"CI slow: Build job waits unnecessarily for all lint/test steps","fingerprint":"process::.github/workflows/ci.yml::build-sequential","severity":"S2","effort":"E2","confidence":"HIGH","files":[".github/workflows/ci.yml:136"],"why_it_matters":"Build could start after type checking completes, saving 2-3 minutes per CI run. Lint/test don't need to block build since both are independent verification steps.","suggested_fix":"Split into 3 parallel jobs: (1) lint+format+deps checks, (2) typecheck+test, (3) build. Then add a final 'all-checks' job that depends on all three. This reduces critical path from ~8min to ~5min.","acceptance_tests":["CI completes 30-40% faster","Build starts before all lint steps finish","No test coverage lost","All checks still block merge"],"file":".github/workflows/ci.yml","line":136,"description":"Build could start after type checking completes, saving 2-3 minutes per CI run. Lint/test don't need to block build since both are independent verification steps.","recommendation":"Split into 3 parallel jobs: (1) lint+format+deps checks, (2) typecheck+test, (3) build. Then add a final 'all-checks' job that depends on all three. This reduces critical path from ~8min to ~5min.","id":"process::.github/workflows/ci.yml::build-sequential"}
{"category":"process","title":"CI slow: Redundant npm ci in build job","fingerprint":"process::.github/workflows/ci.yml::duplicate-npm-ci","severity":"S2","effort":"E1","confidence":"HIGH","files":[".github/workflows/ci.yml:149",".github/workflows/ci.yml:25"],"why_it_matters":"npm ci runs twice (once in lint-typecheck-test, once in build), wasting 30-60s per run. node_modules could be cached as artifact and reused.","suggested_fix":"Use actions/cache or actions/upload-artifact to cache node_modules after first npm ci, then restore in build job. Or use a shared 'setup' job that both depend on.","acceptance_tests":["npm ci runs only once per workflow","Build job reuses node_modules from cache/artifact","CI completes 30-60s faster"],"file":".github/workflows/ci.yml","line":149,"description":"npm ci runs twice (once in lint-typecheck-test, once in build), wasting 30-60s per run. node_modules could be cached as artifact and reused.","recommendation":"Use actions/cache or actions/upload-artifact to cache node_modules after first npm ci, then restore in build job. Or use a shared 'setup' job that both depend on.","id":"process::.github/workflows/ci.yml::duplicate-npm-ci"}
{"category":"process","title":"CI slow: No Next.js build cache","fingerprint":"process::.github/workflows/ci.yml::no-nextjs-cache","severity":"S2","effort":"E1","confidence":"HIGH","files":[".github/workflows/ci.yml:152"],"why_it_matters":"Next.js builds take 2-4 minutes but .next directory isn't cached between runs. Incremental builds could reduce this to 30-60s for small changes.","suggested_fix":"Add actions/cache step to cache .next/cache directory using hash of source files as key. Next.js supports incremental builds when cache is present.","acceptance_tests":["Build time reduced 50-75% for incremental changes","Cache hit rate >70% after initial runs","Full builds still work when cache misses"],"file":".github/workflows/ci.yml","line":152,"description":"Next.js builds take 2-4 minutes but .next directory isn't cached between runs. Incremental builds could reduce this to 30-60s for small changes.","recommendation":"Add actions/cache step to cache .next/cache directory using hash of source files as key. Next.js supports incremental builds when cache is present.","id":"process::.github/workflows/ci.yml::no-nextjs-cache"}
{"category":"process","title":"CI slow: No path filters on main CI workflow","fingerprint":"process::.github/workflows/ci.yml::no-path-filters","severity":"S2","effort":"E1","confidence":"HIGH","files":[".github/workflows/ci.yml:3"],"why_it_matters":"CI runs full test suite even for docs-only changes (*.md files). Adds 5-8 minutes of unnecessary CI time for documentation PRs.","suggested_fix":"Add path-ignore filter to skip CI when only docs, markdown, or non-code files change. Keep required checks but mark as skipped for docs PRs.","acceptance_tests":["CI skips for docs-only PRs","CI still runs for any code changes","Required status checks don't block docs PRs"],"file":".github/workflows/ci.yml","line":3,"description":"CI runs full test suite even for docs-only changes (*.md files). Adds 5-8 minutes of unnecessary CI time for documentation PRs.","recommendation":"Add path-ignore filter to skip CI when only docs, markdown, or non-code files change. Keep required checks but mark as skipped for docs PRs.","id":"process::.github/workflows/ci.yml::no-path-filters"}
{"category":"process","title":"CI slow: Firebase deploy builds app twice","fingerprint":"process::.github/workflows/deploy-firebase.yml::duplicate-build","severity":"S2","effort":"E2","confidence":"HIGH","files":[".github/workflows/deploy-firebase.yml:47",".github/workflows/deploy-firebase.yml:100"],"why_it_matters":"Both preview-deploy and deploy jobs run 'npm run build' independently (2-4 min each). If CI workflow also builds, that's 3 separate builds of the same code.","suggested_fix":"Make deploy workflow depend on ci.yml's build job using workflow_run trigger, then download build artifact. Or create a reusable workflow that both can call.","acceptance_tests":["Build runs only once across all workflows","Deploy workflows reuse build artifacts","Total CI+deploy time reduced by 4-8 minutes"],"file":".github/workflows/deploy-firebase.yml","line":47,"description":"Both preview-deploy and deploy jobs run 'npm run build' independently (2-4 min each). If CI workflow also builds, that's 3 separate builds of the same code.","recommendation":"Make deploy workflow depend on ci.yml's build job using workflow_run trigger, then download build artifact. Or create a reusable workflow that both can call.","id":"process::.github/workflows/deploy-firebase.yml::duplicate-build"}
{"category":"process","title":"CI slow: Firebase deploys run sequentially","fingerprint":"process::.github/workflows/deploy-firebase.yml::sequential-deploys","severity":"S3","effort":"E2","confidence":"MEDIUM","files":[".github/workflows/deploy-firebase.yml:141",".github/workflows/deploy-firebase.yml:144",".github/workflows/deploy-firebase.yml:147"],"why_it_matters":"Functions, firestore rules, and hosting deploy sequentially (line 141, 144, 147). Each takes 30-90s. Running in parallel could save 1-2 minutes.","suggested_fix":"Use 3 parallel jobs or firebase deploy with multiple targets in one command (firebase deploy --only functions,firestore:rules,hosting). Verify Firebase CLI supports parallel deploys without conflicts.","acceptance_tests":["Deploy completes 40-60% faster","All three targets deploy successfully","No race conditions or conflicts","Rollback still works if any target fails"],"file":".github/workflows/deploy-firebase.yml","line":141,"description":"Functions, firestore rules, and hosting deploy sequentially (line 141, 144, 147). Each takes 30-90s. Running in parallel could save 1-2 minutes.","recommendation":"Use 3 parallel jobs or firebase deploy with multiple targets in one command (firebase deploy --only functions,firestore:rules,hosting). Verify Firebase CLI supports parallel deploys without conflicts.","id":"process::.github/workflows/deploy-firebase.yml::sequential-deploys"}
{"category":"process","title":"CI slow: SonarCloud runs on all changes including docs","fingerprint":"process::.github/workflows/sonarcloud.yml::no-path-filters","severity":"S3","effort":"E1","confidence":"HIGH","files":[".github/workflows/sonarcloud.yml:7"],"why_it_matters":"SonarCloud analysis runs on every push/PR even for docs-only changes. Takes 1-2 minutes and consumes analysis quota unnecessarily.","suggested_fix":"Add paths filter to only run when code files change (*.ts, *.tsx, *.js, *.jsx). Skip for docs/markdown-only changes.","acceptance_tests":["SonarCloud skips for docs-only PRs","Analysis still runs for any code changes","No impact on code quality metrics"],"file":".github/workflows/sonarcloud.yml","line":7,"description":"SonarCloud analysis runs on every push/PR even for docs-only changes. Takes 1-2 minutes and consumes analysis quota unnecessarily.","recommendation":"Add paths filter to only run when code files change (*.ts, *.tsx, *.js, *.jsx). Skip for docs/markdown-only changes.","id":"process::.github/workflows/sonarcloud.yml::no-path-filters"}
{"category":"process","title":"CI slow: Full git history fetched unnecessarily","fingerprint":"process::.github/workflows/sonarcloud.yml::full-fetch-depth","severity":"S3","effort":"E1","confidence":"MEDIUM","files":[".github/workflows/sonarcloud.yml:31",".github/workflows/review-check.yml:20"],"why_it_matters":"fetch-depth: 0 downloads entire git history (can be 100MB+ and take 30-60s). Most workflows only need recent commits for diffs.","suggested_fix":"Change fetch-depth to a reasonable number (e.g., 50) or remove if not needed. Only use fetch-depth: 0 when truly necessary (e.g., for blame analysis).","acceptance_tests":["Checkout completes 30-60s faster","Workflows still function correctly","Diff operations still work for PR analysis"],"file":".github/workflows/sonarcloud.yml","line":31,"description":"fetch-depth: 0 downloads entire git history (can be 100MB+ and take 30-60s). Most workflows only need recent commits for diffs.","recommendation":"Change fetch-depth to a reasonable number (e.g., 50) or remove if not needed. Only use fetch-depth: 0 when truly necessary (e.g., for blame analysis).","id":"process::.github/workflows/sonarcloud.yml::full-fetch-depth"}
{"category":"process","title":"CI slow: Backlog workflow installs deps twice","fingerprint":"process::.github/workflows/backlog-enforcement.yml::duplicate-npm-ci","severity":"S3","effort":"E1","confidence":"HIGH","files":[".github/workflows/backlog-enforcement.yml:26",".github/workflows/backlog-enforcement.yml:119"],"why_it_matters":"backlog-health and security-patterns jobs both run npm ci independently (30-60s each). They could share a setup job or reuse cache.","suggested_fix":"Create a shared 'setup' job that runs npm ci once and uploads node_modules as artifact. Both jobs depend on setup and download artifact instead of running npm ci.","acceptance_tests":["npm ci runs once instead of twice","Both jobs still function correctly","Workflow completes 30-60s faster"],"file":".github/workflows/backlog-enforcement.yml","line":26,"description":"backlog-health and security-patterns jobs both run npm ci independently (30-60s each). They could share a setup job or reuse cache.","recommendation":"Create a shared 'setup' job that runs npm ci once and uploads node_modules as artifact. Both jobs depend on setup and download artifact instead of running npm ci.","id":"process::.github/workflows/backlog-enforcement.yml::duplicate-npm-ci"}
{"category":"process","title":"CI slow: Review check runs on all PRs without path filters","fingerprint":"process::.github/workflows/review-check.yml::no-path-filters","severity":"S3","effort":"E1","confidence":"HIGH","files":[".github/workflows/review-check.yml:3"],"why_it_matters":"Review trigger check runs on all PRs including docs-only changes. Wastes 30-60s for PRs that don't touch code.","suggested_fix":"Add paths filter to only run when code files change. For docs-only PRs, this check is not relevant.","acceptance_tests":["Review check skips for docs-only PRs","Check still runs for code changes","No false negatives for trigger detection"],"file":".github/workflows/review-check.yml","line":3,"description":"Review trigger check runs on all PRs including docs-only changes. Wastes 30-60s for PRs that don't touch code.","recommendation":"Add paths filter to only run when code files change. For docs-only PRs, this check is not relevant.","id":"process::.github/workflows/review-check.yml::no-path-filters"}
{"category":"process","title":"CI slow: Auto-label workflow has no npm cache","fingerprint":"process::.github/workflows/auto-label-review-tier.yml::no-npm-cache","severity":"S3","effort":"E1","confidence":"MEDIUM","files":[".github/workflows/auto-label-review-tier.yml:22"],"why_it_matters":"setup-node doesn't have cache: 'npm' configured, so npm packages are re-downloaded on every run (adds 10-20s).","suggested_fix":"Add cache: 'npm' to setup-node action to enable npm caching. This is a one-line change.","acceptance_tests":["npm install completes 50-70% faster on cache hit","Workflow still installs dependencies correctly","No stale package issues"],"file":".github/workflows/auto-label-review-tier.yml","line":22,"description":"setup-node doesn't have cache: 'npm' configured, so npm packages are re-downloaded on every run (adds 10-20s).","recommendation":"Add cache: 'npm' to setup-node action to enable npm caching. This is a one-line change.","id":"process::.github/workflows/auto-label-review-tier.yml::no-npm-cache"}
{"category":"process","title":"CI slow: Docs lint processes files sequentially","fingerprint":"process::.github/workflows/docs-lint.yml::sequential-processing","severity":"S3","effort":"E2","confidence":"MEDIUM","files":[".github/workflows/docs-lint.yml:58"],"why_it_matters":"Documentation linter processes files one-by-one in bash loop (line 58-113). For PRs with 10+ markdown files, this can take 2-3 minutes. Parallel processing could reduce to 30-60s.","suggested_fix":"Refactor to run linter in parallel using xargs -P or rewrite the bash logic into a Node.js script that uses Promise.all to check files concurrently.","acceptance_tests":["Docs lint completes 50-70% faster for multi-file PRs","All files still checked correctly","Output remains readable and actionable"],"file":".github/workflows/docs-lint.yml","line":58,"description":"Documentation linter processes files one-by-one in bash loop (line 58-113). For PRs with 10+ markdown files, this can take 2-3 minutes. Parallel processing could reduce to 30-60s.","recommendation":"Refactor to run linter in parallel using xargs -P or rewrite the bash logic into a Node.js script that uses Promise.all to check files concurrently.","id":"process::.github/workflows/docs-lint.yml::sequential-processing"}
{"category":"process","title":"Perf: check-pattern-compliance.js - Synchronous file reads in loop","fingerprint":"process::scripts/check-pattern-compliance.js::sync-file-reads","severity":"S2","effort":"E2","confidence":"HIGH","files":["scripts/check-pattern-compliance.js:799","scripts/check-pattern-compliance.js:721"],"why_it_matters":"Hook script reads 10-20 files synchronously in pre-commit, blocking for ~200-500ms. Async parallel reads could reduce to ~50-100ms","suggested_fix":"Convert checkFile() to async, use Promise.all() to read files in parallel batches of 10. Change readFileSync to fs.promises.readFile","acceptance_tests":["Script runs 3-5x faster on multi-file checks","Pre-commit hook completes in <100ms for staged files","Output unchanged"],"file":"scripts/check-pattern-compliance.js","line":799,"description":"Hook script reads 10-20 files synchronously in pre-commit, blocking for ~200-500ms. Async parallel reads could reduce to ~50-100ms","recommendation":"Convert checkFile() to async, use Promise.all() to read files in parallel batches of 10. Change readFileSync to fs.promises.readFile","id":"process::scripts/check-pattern-compliance.js::sync-file-reads"}
{"category":"process","title":"Perf: check-pattern-compliance.js - O(n*m) pattern matching without optimization","fingerprint":"process::scripts/check-pattern-compliance.js::unoptimized-pattern-matching","severity":"S2","effort":"E3","confidence":"MEDIUM","files":["scripts/check-pattern-compliance.js:733","scripts/check-pattern-compliance.js:680"],"why_it_matters":"For each file, iterates ALL 30+ patterns even if file extension doesn't match. Checking 20 JS files = 600+ regex compilations. Pre-filtering could reduce by 60%","suggested_fix":"Group patterns by fileTypes, only check relevant patterns. Build Map<extension, patterns[]> at startup. Skip patterns where file extension not in fileTypes","acceptance_tests":["Pattern checks 50-60% faster on mixed file types","Memory usage unchanged","No false negatives in test suite"],"file":"scripts/check-pattern-compliance.js","line":733,"description":"For each file, iterates ALL 30+ patterns even if file extension doesn't match. Checking 20 JS files = 600+ regex compilations. Pre-filtering could reduce by 60%","recommendation":"Group patterns by fileTypes, only check relevant patterns. Build Map<extension, patterns[]> at startup. Skip patterns where file extension not in fileTypes","id":"process::scripts/check-pattern-compliance.js::unoptimized-pattern-matching"}
{"category":"process","title":"Perf: check-pattern-compliance.js - Regex recompilation in hot path","fingerprint":"process::scripts/check-pattern-compliance.js::regex-recompilation","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/check-pattern-compliance.js:659","scripts/check-pattern-compliance.js:662"],"why_it_matters":"Creates new RegExp objects for every file checked (line 659, 662). Checking 20 files with 30 patterns = 1200 regex compilations. Cache compiled regexes","suggested_fix":"Pre-compile all regexes at module load: const compiledPatterns = ANTI_PATTERNS.map(p => ({...p, regex: new RegExp(...)})); Use compiledPatterns in checkFile","acceptance_tests":["10-15% faster on multi-file checks","No regex state bugs (lastIndex leaks)","Output identical"],"file":"scripts/check-pattern-compliance.js","line":659,"description":"Creates new RegExp objects for every file checked (line 659, 662). Checking 20 files with 30 patterns = 1200 regex compilations. Cache compiled regexes","recommendation":"Pre-compile all regexes at module load: const compiledPatterns = ANTI_PATTERNS.map(p => ({...p, regex: new RegExp(...)})); Use compiledPatterns in checkFile","id":"process::scripts/check-pattern-compliance.js::regex-recompilation"}
{"category":"process","title":"Perf: check-docs-light.js - Synchronous file reads in map()","fingerprint":"process::scripts/check-docs-light.js::sync-map-file-reads","severity":"S1","effort":"E2","confidence":"HIGH","files":["scripts/check-docs-light.js:825","scripts/check-docs-light.js:495"],"why_it_matters":"CI docs-lint job reads 50+ markdown files synchronously with readFileSync (line 495), taking 2-3 seconds. Async parallel reads could cut time to <500ms","suggested_fix":"Convert lintDocument to async function, use Promise.all() with batch size limit (10 concurrent). Replace readFileSync with fs.promises.readFile in readDocumentContent","acceptance_tests":["docs-lint CI job runs 4-6x faster","Handles 100+ files without memory issues","All validation results identical"],"file":"scripts/check-docs-light.js","line":825,"description":"CI docs-lint job reads 50+ markdown files synchronously with readFileSync (line 495), taking 2-3 seconds. Async parallel reads could cut time to <500ms","recommendation":"Convert lintDocument to async function, use Promise.all() with batch size limit (10 concurrent). Replace readFileSync with fs.promises.readFile in readDocumentContent","id":"process::scripts/check-docs-light.js::sync-map-file-reads","evidence":[{"type":"code_reference","detail":"scripts/check-docs-light.js:825"},{"type":"description","detail":"CI docs-lint job reads 50+ markdown files synchronously with readFileSync (line 495), taking 2-3 seconds. Async parallel reads could cut time to <500ms"}],"verification_steps":{"first_pass":{"method":"code_search","evidence_collected":["scripts/check-docs-light.js:825"]},"second_pass":{"method":"contextual_review","confirmed":true},"tool_confirmation":{"tool":"NONE","result":"confirmed via manual code review","reference":"scripts/check-docs-light.js:825"}}}
{"category":"process","title":"Perf: check-docs-light.js - O(n^2) anchor link validation","fingerprint":"process::scripts/check-docs-light.js::quadratic-anchor-validation","severity":"S2","effort":"E2","confidence":"HIGH","files":["scripts/check-docs-light.js:411","scripts/check-docs-light.js:427"],"why_it_matters":"validateAnchorLinks has nested loop: for each link (50+), iterates all headings (100+) = 5000 comparisons per doc. Large docs like ROADMAP.md take 500ms just for anchor checks","suggested_fix":"Build Set of valid anchors ONCE (line 400-409), then O(1) Set.has() lookup per link. Remove lines 426-432 partial match fallback (causes the O(n) inner loop)","acceptance_tests":["Anchor validation 50-100x faster on large docs","Memory usage <5MB extra for anchor sets","False positive rate unchanged"],"file":"scripts/check-docs-light.js","line":411,"description":"validateAnchorLinks has nested loop: for each link (50+), iterates all headings (100+) = 5000 comparisons per doc. Large docs like ROADMAP.md take 500ms just for anchor checks","recommendation":"Build Set of valid anchors ONCE (line 400-409), then O(1) Set.has() lookup per link. Remove lines 426-432 partial match fallback (causes the O(n) inner loop)","id":"process::scripts/check-docs-light.js::quadratic-anchor-validation"}
{"category":"process","title":"Perf: check-docs-light.js - Repeated realpath/stat calls","fingerprint":"process::scripts/check-docs-light.js::repeated-fstat-calls","severity":"S2","effort":"E2","confidence":"HIGH","files":["scripts/check-docs-light.js:686","scripts/check-docs-light.js:707","scripts/check-docs-light.js:714"],"why_it_matters":"resolveFileArgs calls realpathSync 3x per file (lines 686, 707, 714) plus lstatSync. For 50 files = 200 syscalls. Caching realpath(ROOT) saves 100+ calls","suggested_fix":"Compute rootRealResolved once (done at line 683), cache realpath results in Map<path, realpath>. Skip redundant lstatSync at line 714 (containment already verified)","acceptance_tests":["File arg resolution 2-3x faster","Symlink protection unchanged","Works with symlinked project directories"],"file":"scripts/check-docs-light.js","line":686,"description":"resolveFileArgs calls realpathSync 3x per file (lines 686, 707, 714) plus lstatSync. For 50 files = 200 syscalls. Caching realpath(ROOT) saves 100+ calls","recommendation":"Compute rootRealResolved once (done at line 683), cache realpath results in Map<path, realpath>. Skip redundant lstatSync at line 714 (containment already verified)","id":"process::scripts/check-docs-light.js::repeated-fstat-calls"}
{"category":"process","title":"Perf: generate-documentation-index.js - Synchronous file reads in loop","fingerprint":"process::scripts/generate-documentation-index.js::sync-doc-processing","severity":"S1","effort":"E2","confidence":"HIGH","files":["scripts/generate-documentation-index.js:913","scripts/generate-documentation-index.js:485"],"why_it_matters":"npm run docs:index reads 80+ markdown files sequentially with readFileSync (line 485), taking 3-4 seconds. CI job blocks during this time. Async could reduce to <1 second","suggested_fix":"Convert processFile to async, use Promise.all with batch limit (15 concurrent): const batches = chunk(activeFiles, 15); for (batch of batches) await Promise.all(batch.map(processFile))","acceptance_tests":["docs:index runs 3-4x faster","Successfully processes 200+ docs","Generated index identical"],"file":"scripts/generate-documentation-index.js","line":913,"description":"npm run docs:index reads 80+ markdown files sequentially with readFileSync (line 485), taking 3-4 seconds. CI job blocks during this time. Async could reduce to <1 second","recommendation":"Convert processFile to async, use Promise.all with batch limit (15 concurrent): const batches = chunk(activeFiles, 15); for (batch of batches) await Promise.all(batch.map(processFile))","id":"process::scripts/generate-documentation-index.js::sync-doc-processing","evidence":[{"type":"code_reference","detail":"scripts/generate-documentation-index.js:913"},{"type":"description","detail":"npm run docs:index reads 80+ markdown files sequentially with readFileSync (line 485), taking 3-4 seconds. CI job blocks during this time. Async could reduce to <1 second"}],"verification_steps":{"first_pass":{"method":"code_search","evidence_collected":["scripts/generate-documentation-index.js:913"]},"second_pass":{"method":"contextual_review","confirmed":true},"tool_confirmation":{"tool":"NONE","result":"confirmed via manual code review","reference":"scripts/generate-documentation-index.js:913"}}}
{"category":"process","title":"Perf: generate-documentation-index.js - Regex compilation in extractLinks loop","fingerprint":"process::scripts/generate-documentation-index.js::regex-in-loop","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/generate-documentation-index.js:376"],"why_it_matters":"extractLinks creates new RegExp on line 376 for EVERY document processed (80+ times). This regex is complex with capturing groups. Move to module level","suggested_fix":"Move linkRegex to module-level constant: const LINK_REGEX = /\\[([^\\]]{1,500})\\]\\(([^)]{1,500})\\)/g; In extractLinks, clone it: const linkRegex = new RegExp(LINK_REGEX.source, LINK_REGEX.flags)","acceptance_tests":["5-10% faster link extraction","No lastIndex state bugs between files","Same links extracted"],"file":"scripts/generate-documentation-index.js","line":376,"description":"extractLinks creates new RegExp on line 376 for EVERY document processed (80+ times). This regex is complex with capturing groups. Move to module level","recommendation":"Move linkRegex to module-level constant: const LINK_REGEX = /\\[([^\\]]{1,500})\\]\\(([^)]{1,500})\\)/g; In extractLinks, clone it: const linkRegex = new RegExp(LINK_REGEX.source, LINK_REGEX.flags)","id":"process::scripts/generate-documentation-index.js::regex-in-loop"}
{"category":"process","title":"Perf: generate-documentation-index.js - O(n*m) reference graph building","fingerprint":"process::scripts/generate-documentation-index.js::quadratic-reference-graph","severity":"S2","effort":"E3","confidence":"MEDIUM","files":["scripts/generate-documentation-index.js:524","scripts/generate-documentation-index.js:528"],"why_it_matters":"buildReferenceGraph: for 80 docs with 20 links each = 1600 iterations. Each does Map.get() twice (lines 535-536). With 200+ docs this becomes noticeable (500ms+)","suggested_fix":"Use Map.get once, assign to variable. Combine lines 534-536 into: const targetNode = graph.get(target); if (targetNode) { node.outbound.push(target); targetNode.inbound.push(doc.path); }","acceptance_tests":["Reference graph builds 20-30% faster","Large doc sets (200+) don't timeout","Graph structure identical"],"file":"scripts/generate-documentation-index.js","line":524,"description":"buildReferenceGraph: for 80 docs with 20 links each = 1600 iterations. Each does Map.get() twice (lines 535-536). With 200+ docs this becomes noticeable (500ms+)","recommendation":"Use Map.get once, assign to variable. Combine lines 534-536 into: const targetNode = graph.get(target); if (targetNode) { node.outbound.push(target); targetNode.inbound.push(doc.path); }","id":"process::scripts/generate-documentation-index.js::quadratic-reference-graph"}
{"category":"process","title":"Perf: aggregate-audit-findings.js - O(n^2) deduplication with large buckets","fingerprint":"process::scripts/aggregate-audit-findings.js::quadratic-dedup","severity":"S1","effort":"E3","confidence":"HIGH","files":["scripts/aggregate-audit-findings.js:1309","scripts/aggregate-audit-findings.js:1320"],"why_it_matters":"processBucketPairs has O(k^2) nested loop for each bucket. With 250 item bucket cap, worst case is 31,250 comparisons per bucket. Full aggregation takes 10-15 seconds","suggested_fix":"Add early termination: if bucket size > threshold AND no merges in last N comparisons, skip rest. Or use LSH (Locality Sensitive Hashing) to reduce comparison space. Lower MAX_FILE_BUCKET from 250 to 100","acceptance_tests":["Aggregation runs 2-3x faster","Dedup quality unchanged (same merge count)","Handles 500+ findings without timeout"],"file":"scripts/aggregate-audit-findings.js","line":1309,"description":"processBucketPairs has O(k^2) nested loop for each bucket. With 250 item bucket cap, worst case is 31,250 comparisons per bucket. Full aggregation takes 10-15 seconds","recommendation":"Add early termination: if bucket size > threshold AND no merges in last N comparisons, skip rest. Or use LSH (Locality Sensitive Hashing) to reduce comparison space. Lower MAX_FILE_BUCKET from 250 to 100","id":"process::scripts/aggregate-audit-findings.js::quadratic-dedup","evidence":[{"type":"code_reference","detail":"scripts/aggregate-audit-findings.js:1309"},{"type":"description","detail":"processBucketPairs has O(k^2) nested loop for each bucket. With 250 item bucket cap, worst case is 31,250 comparisons per bucket. Full aggregation takes 10-15 seconds"}],"verification_steps":{"first_pass":{"method":"code_search","evidence_collected":["scripts/aggregate-audit-findings.js:1309"]},"second_pass":{"method":"contextual_review","confirmed":true},"tool_confirmation":{"tool":"NONE","result":"confirmed via manual code review","reference":"scripts/aggregate-audit-findings.js:1309"}}}
{"category":"process","title":"Perf: aggregate-audit-findings.js - Synchronous JSONL file reads","fingerprint":"process::scripts/aggregate-audit-findings.js::sync-jsonl-reads","severity":"S2","effort":"E2","confidence":"HIGH","files":["scripts/aggregate-audit-findings.js:1201","scripts/aggregate-audit-findings.js:250"],"why_it_matters":"Reads 7 JSONL files sequentially in parseSingleSessionAudits and parseCanonFiles (lines 1201, 1226). Each readFileSync blocks. Total time ~300-500ms. Parallel reads could reduce to <100ms","suggested_fix":"Convert parseJsonlFile to async with fs.promises.readFile. Use Promise.all to read all 7 category files in parallel. Await results before proceeding to phase 2","acceptance_tests":["Phase 1 parsing 3-5x faster","All findings still captured","JSONL parse errors still logged"],"file":"scripts/aggregate-audit-findings.js","line":1201,"description":"Reads 7 JSONL files sequentially in parseSingleSessionAudits and parseCanonFiles (lines 1201, 1226). Each readFileSync blocks. Total time ~300-500ms. Parallel reads could reduce to <100ms","recommendation":"Convert parseJsonlFile to async with fs.promises.readFile. Use Promise.all to read all 7 category files in parallel. Await results before proceeding to phase 2","id":"process::scripts/aggregate-audit-findings.js::sync-jsonl-reads"}
{"category":"process","title":"Perf: aggregate-audit-findings.js - Expensive Levenshtein in hot path","fingerprint":"process::scripts/aggregate-audit-findings.js::expensive-levenshtein","severity":"S2","effort":"E2","confidence":"HIGH","files":["scripts/aggregate-audit-findings.js:1013","scripts/aggregate-audit-findings.js:1025"],"why_it_matters":"levenshteinDistance called in dedup loop with O(m*n) DP algorithm. For 500 findings, potentially 10,000+ calls. Each 500-char comparison = 250,000 operations. Truncate earlier or cache","suggested_fix":"Reduce MAX_LEVENSHTEIN_LENGTH from 500 to 200 chars (still enough for titles). Add memoization: const cache = new Map(); Check cache before computing. Clear cache between dedup passes","acceptance_tests":["Dedup 30-50% faster","Title similarity accuracy unchanged","No cache memory leaks"],"file":"scripts/aggregate-audit-findings.js","line":1013,"description":"levenshteinDistance called in dedup loop with O(m*n) DP algorithm. For 500 findings, potentially 10,000+ calls. Each 500-char comparison = 250,000 operations. Truncate earlier or cache","recommendation":"Reduce MAX_LEVENSHTEIN_LENGTH from 500 to 200 chars (still enough for titles). Add memoization: const cache = new Map(); Check cache before computing. Clear cache between dedup passes","id":"process::scripts/aggregate-audit-findings.js::expensive-levenshtein"}
{"category":"process","title":"Perf: aggregate-audit-findings.js - Repeated string normalization","fingerprint":"process::scripts/aggregate-audit-findings.js::repeated-normalization","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/aggregate-audit-findings.js:1044"],"why_it_matters":"similarityScore does replaceAll(/[^a-z0-9\\s]/g, '') twice (line 1044) for EVERY comparison. Regex replacement is expensive. Called 10,000+ times during dedup","suggested_fix":"Cache normalized strings: const normalized = new Map(); function getNormalized(str) { if (!normalized.has(str)) normalized.set(str, str.toLowerCase().replaceAll(...)); return normalized.get(str); }","acceptance_tests":["Similarity scoring 20-40% faster","Same similarity scores produced","Cache size bounded (clear between passes)"],"file":"scripts/aggregate-audit-findings.js","line":1044,"description":"similarityScore does replaceAll(/[^a-z0-9\\s]/g, '') twice (line 1044) for EVERY comparison. Regex replacement is expensive. Called 10,000+ times during dedup","recommendation":"Cache normalized strings: const normalized = new Map(); function getNormalized(str) { if (!normalized.has(str)) normalized.set(str, str.toLowerCase().replaceAll(...)); return normalized.get(str); }","id":"process::scripts/aggregate-audit-findings.js::repeated-normalization"}
{"category":"process","title":"Perf: check-content-accuracy.js - Synchronous file reads in loop","fingerprint":"process::scripts/check-content-accuracy.js::sync-reads-loop","severity":"S2","effort":"E2","confidence":"HIGH","files":["scripts/check-content-accuracy.js:458","scripts/check-content-accuracy.js:412"],"why_it_matters":"Reads all markdown files sequentially with readFileSync (line 412). For 50+ docs, takes 1-2 seconds. CI content validation could be 3-4x faster with async","suggested_fix":"Convert checkDocument to async function using fs.promises.readFile. Use Promise.all with batch size 10: const results = []; for (const batch of chunk(files, 10)) results.push(...await Promise.all(batch.map(checkDocument)))","acceptance_tests":["Script runs 3-4x faster","All findings still detected","Memory usage <50MB for 100 files"],"file":"scripts/check-content-accuracy.js","line":458,"description":"Reads all markdown files sequentially with readFileSync (line 412). For 50+ docs, takes 1-2 seconds. CI content validation could be 3-4x faster with async","recommendation":"Convert checkDocument to async function using fs.promises.readFile. Use Promise.all with batch size 10: const results = []; for (const batch of chunk(files, 10)) results.push(...await Promise.all(batch.map(checkDocument)))","id":"process::scripts/check-content-accuracy.js::sync-reads-loop"}
{"category":"process","title":"Perf: check-content-accuracy.js - Regex compilation in hot loops","fingerprint":"process::scripts/check-content-accuracy.js::regex-in-hot-loops","severity":"S2","effort":"E1","confidence":"HIGH","files":["scripts/check-content-accuracy.js:114","scripts/check-content-accuracy.js:197","scripts/check-content-accuracy.js:286"],"why_it_matters":"versionPatterns (line 114), pathPatterns (line 197), npmPatterns (line 286) created fresh for EVERY file. For 50 files = 150+ array allocations with regex literals. Move to module level","suggested_fix":"Declare patterns as module-level constants outside functions: const VERSION_PATTERNS = [...]; const PATH_PATTERNS = [...]; const NPM_PATTERNS = [...]; Reference these in check functions","acceptance_tests":["10-15% faster overall","No regex state leaks","Same findings detected"],"file":"scripts/check-content-accuracy.js","line":114,"description":"versionPatterns (line 114), pathPatterns (line 197), npmPatterns (line 286) created fresh for EVERY file. For 50 files = 150+ array allocations with regex literals. Move to module level","recommendation":"Declare patterns as module-level constants outside functions: const VERSION_PATTERNS = [...]; const PATH_PATTERNS = [...]; const NPM_PATTERNS = [...]; Reference these in check functions","id":"process::scripts/check-content-accuracy.js::regex-in-hot-loops"}
{"category":"process","title":"Perf: check-content-accuracy.js - O(lines * patterns) nested loops","fingerprint":"process::scripts/check-content-accuracy.js::nested-pattern-loops","severity":"S2","effort":"E3","confidence":"MEDIUM","files":["scripts/check-content-accuracy.js:136","scripts/check-content-accuracy.js:217","scripts/check-content-accuracy.js:308"],"why_it_matters":"checkVersionAccuracy, checkPathReferences, checkNpmScriptReferences all have nested loops: for each line, iterate all patterns, exec in while loop. Large docs (500+ lines) with 5+ patterns = 2500+ regex execs","suggested_fix":"Combine all patterns into single alternation regex: /pattern1|pattern2|pattern3/g. Single pass per line with switch on match type. Or use multiline mode to match entire content once","acceptance_tests":["30-50% faster on large docs","All checks still detect issues","No false negatives in test suite"],"file":"scripts/check-content-accuracy.js","line":136,"description":"checkVersionAccuracy, checkPathReferences, checkNpmScriptReferences all have nested loops: for each line, iterate all patterns, exec in while loop. Large docs (500+ lines) with 5+ patterns = 2500+ regex execs","recommendation":"Combine all patterns into single alternation regex: /pattern1|pattern2|pattern3/g. Single pass per line with switch on match type. Or use multiline mode to match entire content once","id":"process::scripts/check-content-accuracy.js::nested-pattern-loops"}
