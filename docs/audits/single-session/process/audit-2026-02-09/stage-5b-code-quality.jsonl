{"category":"process","title":"Quality: TOCTOU race condition in ai-review.js","fingerprint":"process::scripts/ai-review.js::toctou-existsSync-readFileSync","severity":"S1","effort":"E1","confidence":"HIGH","files":["scripts/ai-review.js:139"],"why_it_matters":"Classic TOCTOU (Time-of-Check-Time-of-Use) race condition. File existence is checked with existsSync() then read with readFileSync(). Between these calls, the file could be deleted, moved, or replaced with a symlink, causing crashes or reading wrong files. CODE_PATTERNS.md Rule #3 explicitly requires wrapping ALL file reads in try/catch.","suggested_fix":"Remove existsSync check and wrap readFileSync in try/catch. Pattern from CODE_PATTERNS.md: try { const content = readFileSync(filePath, 'utf-8'); return { success: true, content }; } catch (error) { if (error.code === 'ENOENT') { return { success: false, error: 'File not found' }; } ... }","acceptance_tests":["existsSync check removed","readFileSync wrapped in try/catch with error.code === 'ENOENT' handling"],"file":"scripts/ai-review.js","line":139,"description":"Classic TOCTOU (Time-of-Check-Time-of-Use) race condition. File existence is checked with existsSync() then read with readFileSync(). Between these calls, the file could be deleted, moved, or replaced with a symlink, causing crashes or reading wrong files. CODE_PATTERNS.md Rule #3 explicitly requires wrapping ALL file reads in try/catch.","recommendation":"Remove existsSync check and wrap readFileSync in try/catch. Pattern from CODE_PATTERNS.md: try { const content = readFileSync(filePath, 'utf-8'); return { success: true, content }; } catch (error) { if (error.code === 'ENOENT') { return { success: false, error: 'File not found' }; } ... }","id":"process::scripts/ai-review.js::toctou-existsSync-readFileSync","evidence":[{"type":"code_reference","detail":"scripts/ai-review.js:139"},{"type":"description","detail":"Classic TOCTOU (Time-of-Check-Time-of-Use) race condition. File existence is checked with existsSync() then read with readFileSync(). Between these calls, the file could be deleted, moved, or replaced with a symlink, causing crashes or reading wrong files. CODE_PATTERNS.md Rule #3 explicitly requires wrapping ALL file reads in try/catch."}],"verification_steps":{"first_pass":{"method":"code_search","evidence_collected":["scripts/ai-review.js:139"]},"second_pass":{"method":"contextual_review","confirmed":true},"tool_confirmation":{"tool":"NONE","result":"confirmed via manual code review","reference":"scripts/ai-review.js:139"}}}
{"category":"process","title":"Quality: TOCTOU race condition in check-consolidation-status.js","fingerprint":"process::scripts/check-consolidation-status.js::toctou-existsSync-readFileSync","severity":"S1","effort":"E1","confidence":"HIGH","files":["scripts/check-consolidation-status.js:89"],"why_it_matters":"Classic TOCTOU race condition. existsSync at line 89 followed by readFileSync at line 96. Violates CODE_PATTERNS.md Critical Pattern #3: 'Wrap ALL file reads in try/catch - existsSync has race conditions'. Between the check and read, file could be deleted causing crash.","suggested_fix":"Remove existsSync check at line 89. Change error handling to: try { const content = readFileSync(LOG_FILE, 'utf8'); ... } catch (err) { if (err.code === 'ENOENT') { console.error('File not found'); process.exitCode = 2; return; } throw err; }","acceptance_tests":["existsSync check removed","readFileSync moved into try block","error.code === 'ENOENT' checked in catch"],"file":"scripts/check-consolidation-status.js","line":89,"description":"Classic TOCTOU race condition. existsSync at line 89 followed by readFileSync at line 96. Violates CODE_PATTERNS.md Critical Pattern #3: 'Wrap ALL file reads in try/catch - existsSync has race conditions'. Between the check and read, file could be deleted causing crash.","recommendation":"Remove existsSync check at line 89. Change error handling to: try { const content = readFileSync(LOG_FILE, 'utf8'); ... } catch (err) { if (err.code === 'ENOENT') { console.error('File not found'); process.exitCode = 2; return; } throw err; }","id":"process::scripts/check-consolidation-status.js::toctou-existsSync-readFileSync","evidence":[{"type":"code_reference","detail":"scripts/check-consolidation-status.js:89"},{"type":"description","detail":"Classic TOCTOU race condition. existsSync at line 89 followed by readFileSync at line 96. Violates CODE_PATTERNS.md Critical Pattern #3: 'Wrap ALL file reads in try/catch - existsSync has race conditions'. Between the check and read, file could be deleted causing crash."}],"verification_steps":{"first_pass":{"method":"code_search","evidence_collected":["scripts/check-consolidation-status.js:89"]},"second_pass":{"method":"contextual_review","confirmed":true},"tool_confirmation":{"tool":"NONE","result":"confirmed via manual code review","reference":"scripts/check-consolidation-status.js:89"}}}
{"category":"process","title":"Quality: TOCTOU race condition in resolve-item.js","fingerprint":"process::scripts/debt/resolve-item.js::toctou-existsSync-readFileSync","severity":"S1","effort":"E1","confidence":"HIGH","files":["scripts/debt/resolve-item.js:53"],"why_it_matters":"TOCTOU race in loadMasterDebt(). existsSync at line 53 followed by readFileSync at line 56. File could be deleted between check and read. Violates CODE_PATTERNS.md Critical Pattern #3. In a concurrent environment (multiple debt resolution scripts), this can cause crashes.","suggested_fix":"Replace lines 52-59 with: function loadMasterDebt() { try { const content = fs.readFileSync(MASTER_FILE, 'utf8'); const lines = content.split('\\n').filter(line => line.trim()); return lines.map(line => JSON.parse(line)); } catch (err) { if (err.code === 'ENOENT') return []; throw err; } }","acceptance_tests":["existsSync check removed","readFileSync in try block","Empty array returned on ENOENT"],"file":"scripts/debt/resolve-item.js","line":53,"description":"TOCTOU race in loadMasterDebt(). existsSync at line 53 followed by readFileSync at line 56. File could be deleted between check and read. Violates CODE_PATTERNS.md Critical Pattern #3. In a concurrent environment (multiple debt resolution scripts), this can cause crashes.","recommendation":"Replace lines 52-59 with: function loadMasterDebt() { try { const content = fs.readFileSync(MASTER_FILE, 'utf8'); const lines = content.split('\\n').filter(line => line.trim()); return lines.map(line => JSON.parse(line)); } catch (err) { if (err.code === 'ENOENT') return []; throw err; } }","id":"process::scripts/debt/resolve-item.js::toctou-existsSync-readFileSync","evidence":[{"type":"code_reference","detail":"scripts/debt/resolve-item.js:53"},{"type":"description","detail":"TOCTOU race in loadMasterDebt(). existsSync at line 53 followed by readFileSync at line 56. File could be deleted between check and read. Violates CODE_PATTERNS.md Critical Pattern #3. In a concurrent environment (multiple debt resolution scripts), this can cause crashes."}],"verification_steps":{"first_pass":{"method":"code_search","evidence_collected":["scripts/debt/resolve-item.js:53"]},"second_pass":{"method":"contextual_review","confirmed":true},"tool_confirmation":{"tool":"NONE","result":"confirmed via manual code review","reference":"scripts/debt/resolve-item.js:53"}}}
{"category":"process","title":"Quality: Magic number - hardcoded port without explanation","fingerprint":"process::.claude/hooks/stop-serena-dashboard.js::magic-port-24282","severity":"S2","effort":"E1","confidence":"HIGH","files":[".claude/hooks/stop-serena-dashboard.js:30"],"why_it_matters":"Port 24282 is hardcoded without any comment explaining why this specific port. Makes it unclear if this is a well-known port, randomly chosen, or has significance. CODE_PATTERNS.md warns against magic numbers/strings without explanation. If port needs to change, developers won't know the constraints.","suggested_fix":"Add explanatory comment: // Port 24282: Official Serena MCP server port (assigned in .mcp.json). Or better: load from config file to have single source of truth matching .mcp.json configuration.","acceptance_tests":["Comment added explaining port selection","Or: PORT loaded from shared config file"],"file":".claude/hooks/stop-serena-dashboard.js","line":30,"description":"Port 24282 is hardcoded without any comment explaining why this specific port. Makes it unclear if this is a well-known port, randomly chosen, or has significance. CODE_PATTERNS.md warns against magic numbers/strings without explanation. If port needs to change, developers won't know the constraints.","recommendation":"Add explanatory comment: // Port 24282: Official Serena MCP server port (assigned in .mcp.json). Or better: load from config file to have single source of truth matching .mcp.json configuration.","id":"process::.claude/hooks/stop-serena-dashboard.js::magic-port-24282"}
{"category":"process","title":"Quality: Magic number - MAX_LENGTH without explanation","fingerprint":"process::.claude/hooks/analyze-user-request.js::magic-maxlength-2000","severity":"S2","effort":"E1","confidence":"HIGH","files":[".claude/hooks/analyze-user-request.js:37"],"why_it_matters":"MAX_LENGTH=2000 hardcoded without explanation of why 2000 characters. Is this to prevent DoS? Buffer overflow? UI limitation? Without context, future maintainers won't know if this can be safely adjusted.","suggested_fix":"Add comment explaining rationale: // DoS prevention: Limit input to 2000 chars (typical user prompt is <500 chars). Consider making this configurable if different contexts need different limits.","acceptance_tests":["Comment added explaining why 2000","Or: Value moved to config file with explanation"],"file":".claude/hooks/analyze-user-request.js","line":37,"description":"MAX_LENGTH=2000 hardcoded without explanation of why 2000 characters. Is this to prevent DoS? Buffer overflow? UI limitation? Without context, future maintainers won't know if this can be safely adjusted.","recommendation":"Add comment explaining rationale: // DoS prevention: Limit input to 2000 chars (typical user prompt is <500 chars). Consider making this configurable if different contexts need different limits.","id":"process::.claude/hooks/analyze-user-request.js::magic-maxlength-2000"}
{"category":"process","title":"Quality: Magic number - SINGLE_FILE_LINE_LIMIT without explanation","fingerprint":"process::.claude/hooks/large-context-warning.js::magic-line-limit-5000","severity":"S2","effort":"E1","confidence":"HIGH","files":[".claude/hooks/large-context-warning.js:20"],"why_it_matters":"SINGLE_FILE_LINE_LIMIT=5000 without explanation. Is this based on Claude token limits? Performance testing? Arbitrary choice? Future Claude model upgrades may allow larger contexts, but without documentation, developers won't know if this is safe to change.","suggested_fix":"Add comment: // Claude 3 context window = ~200K tokens. 5000 lines ≈ 50K tokens average, leaving headroom for conversation. Based on [reference to testing/decision doc if exists]. Consider loading from config for different model tiers.","acceptance_tests":["Comment added with rationale","Or: Document in docs/agent_docs/ and reference from code"],"file":".claude/hooks/large-context-warning.js","line":20,"description":"SINGLE_FILE_LINE_LIMIT=5000 without explanation. Is this based on Claude token limits? Performance testing? Arbitrary choice? Future Claude model upgrades may allow larger contexts, but without documentation, developers won't know if this is safe to change.","recommendation":"Add comment: // Claude 3 context window = ~200K tokens. 5000 lines ≈ 50K tokens average, leaving headroom for conversation. Based on [reference to testing/decision doc if exists]. Consider loading from config for different model tiers.","id":"process::.claude/hooks/large-context-warning.js::magic-line-limit-5000"}
{"category":"process","title":"Quality: Magic number - ARCHIVE_LINE_THRESHOLD without explanation","fingerprint":"process::scripts/check-consolidation-status.js::magic-archive-threshold-2500","severity":"S2","effort":"E1","confidence":"HIGH","files":["scripts/check-consolidation-status.js:26"],"why_it_matters":"ARCHIVE_LINE_THRESHOLD=2500 has no explanation. Why 2500 lines? Performance issue? File size limit? Readability concern? This threshold triggers archival decisions but lacks documentation on how it was determined.","suggested_fix":"Add comment: // Archive threshold: 2500 lines keeps file manageable for editors (most IDEs struggle >3000 lines). Based on AI_REVIEW_LEARNINGS_LOG.md performance testing [if exists]. Consider making configurable.","acceptance_tests":["Comment added with rationale","Threshold moved to config if used in multiple places"],"file":"scripts/check-consolidation-status.js","line":26,"description":"ARCHIVE_LINE_THRESHOLD=2500 has no explanation. Why 2500 lines? Performance issue? File size limit? Readability concern? This threshold triggers archival decisions but lacks documentation on how it was determined.","recommendation":"Add comment: // Archive threshold: 2500 lines keeps file manageable for editors (most IDEs struggle >3000 lines). Based on AI_REVIEW_LEARNINGS_LOG.md performance testing [if exists]. Consider making configurable.","id":"process::scripts/check-consolidation-status.js::magic-archive-threshold-2500"}
{"category":"process","title":"Quality: Magic number - REQUEST_TIMEOUT_MS without explanation","fingerprint":"process::scripts/mcp/sonarcloud-server.js::magic-timeout-30000","severity":"S2","effort":"E1","confidence":"HIGH","files":["scripts/mcp/sonarcloud-server.js:68"],"why_it_matters":"REQUEST_TIMEOUT_MS=30000 (30 seconds) hardcoded without explanation. Is this based on SonarCloud API SLA? Network timeout? If SonarCloud response times change or users have slow connections, they won't know if adjusting this is safe.","suggested_fix":"Add comment: // 30s timeout: SonarCloud API p95 response time ~5s, p99 ~15s (as of 2025-01). 30s provides 2x buffer for slow connections. Reference: [SonarCloud API docs]. Consider making configurable for different network conditions.","acceptance_tests":["Comment added explaining timeout rationale","Or: Load from config with fallback to 30000"],"file":"scripts/mcp/sonarcloud-server.js","line":68,"description":"REQUEST_TIMEOUT_MS=30000 (30 seconds) hardcoded without explanation. Is this based on SonarCloud API SLA? Network timeout? If SonarCloud response times change or users have slow connections, they won't know if adjusting this is safe.","recommendation":"Add comment: // 30s timeout: SonarCloud API p95 response time ~5s, p99 ~15s (as of 2025-01). 30s provides 2x buffer for slow connections. Reference: [SonarCloud API docs]. Consider making configurable for different network conditions.","id":"process::scripts/mcp/sonarcloud-server.js::magic-timeout-30000"}
{"category":"process","title":"Quality: Hardcoded path should be configurable","fingerprint":"process::.claude/hooks/state-utils.js::hardcoded-state-dir","severity":"S2","effort":"E2","confidence":"MEDIUM","files":[".claude/hooks/state-utils.js:22"],"why_it_matters":"STATE_DIR = '.claude/state' is hardcoded. In different project structures or CI environments, state might need to be stored elsewhere (temp dirs, mounted volumes). Makes the utility less reusable across different setups.","suggested_fix":"Make configurable: const STATE_DIR = process.env.CLAUDE_STATE_DIR || '.claude/state'; Document the environment variable in docs/agent_docs/ or CONTRIBUTING.md. This allows override without code changes.","acceptance_tests":["STATE_DIR reads from environment variable with fallback","Environment variable documented in project docs"],"file":".claude/hooks/state-utils.js","line":22,"description":"STATE_DIR = '.claude/state' is hardcoded. In different project structures or CI environments, state might need to be stored elsewhere (temp dirs, mounted volumes). Makes the utility less reusable across different setups.","recommendation":"Make configurable: const STATE_DIR = process.env.CLAUDE_STATE_DIR || '.claude/state'; Document the environment variable in docs/agent_docs/ or CONTRIBUTING.md. This allows override without code changes.","id":"process::.claude/hooks/state-utils.js::hardcoded-state-dir"}
{"category":"process","title":"Quality: Multiple hardcoded debt paths","fingerprint":"process::scripts/debt/intake-audit.js::hardcoded-debt-paths","severity":"S2","effort":"E2","confidence":"MEDIUM","files":["scripts/debt/intake-audit.js:53","scripts/debt/intake-audit.js:54","scripts/debt/intake-audit.js:55","scripts/debt/intake-audit.js:56"],"why_it_matters":"DEBT_DIR, MASTER_FILE, LOG_DIR, LOG_FILE all hardcoded to docs/technical-debt. In CI/CD or different repo structures, technical debt data might need different locations. Testing also becomes harder without configurability.","suggested_fix":"Load from config: const config = loadConfig('debt-paths'); const DEBT_DIR = config.debtDir || path.join(__dirname, '../../docs/technical-debt'); Or use environment variables with fallbacks. Document configuration in docs/agent_docs/.","acceptance_tests":["Paths loaded from config or env vars","Default values preserved for backward compatibility","Configuration method documented"],"file":"scripts/debt/intake-audit.js","line":53,"description":"DEBT_DIR, MASTER_FILE, LOG_DIR, LOG_FILE all hardcoded to docs/technical-debt. In CI/CD or different repo structures, technical debt data might need different locations. Testing also becomes harder without configurability.","recommendation":"Load from config: const config = loadConfig('debt-paths'); const DEBT_DIR = config.debtDir || path.join(__dirname, '../../docs/technical-debt'); Or use environment variables with fallbacks. Document configuration in docs/agent_docs/.","id":"process::scripts/debt/intake-audit.js::hardcoded-debt-paths"}
{"category":"process","title":"Security: Potential command injection in resolve-item.js execSync","fingerprint":"process::scripts/debt/resolve-item.js::execsync-string-interpolation","severity":"S0","effort":"E1","confidence":"MEDIUM","files":["scripts/debt/resolve-item.js:21"],"why_it_matters":"execSync imported from child_process at line 21. While not directly visible in first 100 lines, use of execSync with string concatenation is a critical security issue per CODE_PATTERNS.md. Need to verify later in file that execFileSync is used or execSync uses only array arguments.","suggested_fix":"If execSync is used with string templates: replace with execFileSync(cmd, [arg1, arg2], options). CODE_PATTERNS.md Security pattern: 'Use execFileSync(cmd, [arg1, arg2]) not execSync(`cmd ${var}`)' eliminates injection vectors even with validated inputs.","acceptance_tests":["Verify execSync usage in full file","If string interpolation found, replace with execFileSync array args","Add test case with malicious input (e.g., '; rm -rf /')"],"file":"scripts/debt/resolve-item.js","line":21,"description":"execSync imported from child_process at line 21. While not directly visible in first 100 lines, use of execSync with string concatenation is a critical security issue per CODE_PATTERNS.md. Need to verify later in file that execFileSync is used or execSync uses only array arguments.","recommendation":"If execSync is used with string templates: replace with execFileSync(cmd, [arg1, arg2], options). CODE_PATTERNS.md Security pattern: 'Use execFileSync(cmd, [arg1, arg2]) not execSync(`cmd ${var}`)' eliminates injection vectors even with validated inputs.","id":"process::scripts/debt/resolve-item.js::execsync-string-interpolation","evidence":[{"type":"code_reference","detail":"scripts/debt/resolve-item.js:21"},{"type":"description","detail":"execSync imported from child_process at line 21. While not directly visible in first 100 lines, use of execSync with string concatenation is a critical security issue per CODE_PATTERNS.md. Need to verify later in file that execFileSync is used or execSync uses only array arguments."}],"verification_steps":{"first_pass":{"method":"code_search","evidence_collected":["scripts/debt/resolve-item.js:21"]},"second_pass":{"method":"contextual_review","confirmed":true},"tool_confirmation":{"tool":"NONE","result":"confirmed via manual code review","reference":"scripts/debt/resolve-item.js:21"}}}
{"category":"process","title":"Quality: Missing validation on parsed JSON objects","fingerprint":"process::scripts/debt/intake-audit.js::json-parse-validation","severity":"S1","effort":"E2","confidence":"HIGH","files":["scripts/debt/intake-audit.js:119"],"why_it_matters":"mapDocStandardsToTdms() function processes untrusted JSONL input. While safeCloneObject filters __proto__, the function doesn't validate object shape before accessing properties. Malformed JSONL could cause undefined behavior or crashes when accessing nested properties.","suggested_fix":"Add input validation: function mapDocStandardsToTdms(item) { if (!item || typeof item !== 'object' || Array.isArray(item)) { return { item: {}, metadata: { format_detected: 'invalid', error: 'Expected object' }}; } ... } Also validate array types before .map(), .length access.","acceptance_tests":["Null input handled gracefully","Array input (not object) handled","String/number input handled","Nested property access wrapped in optional chaining or existence checks"],"file":"scripts/debt/intake-audit.js","line":119,"description":"mapDocStandardsToTdms() function processes untrusted JSONL input. While safeCloneObject filters __proto__, the function doesn't validate object shape before accessing properties. Malformed JSONL could cause undefined behavior or crashes when accessing nested properties.","recommendation":"Add input validation: function mapDocStandardsToTdms(item) { if (!item || typeof item !== 'object' || Array.isArray(item)) { return { item: {}, metadata: { format_detected: 'invalid', error: 'Expected object' }}; } ... } Also validate array types before .map(), .length access.","id":"process::scripts/debt/intake-audit.js::json-parse-validation","evidence":[{"type":"code_reference","detail":"scripts/debt/intake-audit.js:119"},{"type":"description","detail":"mapDocStandardsToTdms() function processes untrusted JSONL input. While safeCloneObject filters __proto__, the function doesn't validate object shape before accessing properties. Malformed JSONL could cause undefined behavior or crashes when accessing nested properties."}],"verification_steps":{"first_pass":{"method":"code_search","evidence_collected":["scripts/debt/intake-audit.js:119"]},"second_pass":{"method":"contextual_review","confirmed":true},"tool_confirmation":{"tool":"NONE","result":"confirmed via manual code review","reference":"scripts/debt/intake-audit.js:119"}}}
{"category":"process","title":"Quality: Unsafe regex patterns in pattern checker","fingerprint":"process::scripts/check-pattern-compliance.js::redos-risk","severity":"S1","effort":"E3","confidence":"MEDIUM","files":["scripts/check-pattern-compliance.js:106-300"],"why_it_matters":"ANTI_PATTERNS array contains many complex regex with nested quantifiers and unbounded lookaheads (e.g., line 137: /for\\s+\\w+\\s+in\\s+1\\s+2\\s+3\\s*;\\s*do[\\s\\S]{0,120}?&&\\s*break[\\s\\S]{0,80}?done(?![\\s\\S]{0,80}?...). On maliciously crafted input files, these could cause ReDoS (Regular Expression Denial of Service) hanging the checker.","suggested_fix":"CODE_PATTERNS.md Security: 'Use {1,64} not + for bounded user input' and 'Add heuristic detection (nested quantifiers, length limits)'. Add input size guards before regex matching: if (content.length > 100000) { console.warn('File too large, skipping pattern checks'); return; }. Review each regex for nested quantifiers and add explicit bounds.","acceptance_tests":["Input size limits added before regex execution","Regex timeout mechanism (if platform supports)","Test with large files (1MB+) to verify no hang","Review each pattern for nested quantifiers"],"file":"scripts/check-pattern-compliance.js","line":106,"description":"ANTI_PATTERNS array contains many complex regex with nested quantifiers and unbounded lookaheads (e.g., line 137: /for\\s+\\w+\\s+in\\s+1\\s+2\\s+3\\s*;\\s*do[\\s\\S]{0,120}?&&\\s*break[\\s\\S]{0,80}?done(?![\\s\\S]{0,80}?...). On maliciously crafted input files, these could cause ReDoS (Regular Expression Denial of Service) hanging the checker.","recommendation":"CODE_PATTERNS.md Security: 'Use {1,64} not + for bounded user input' and 'Add heuristic detection (nested quantifiers, length limits)'. Add input size guards before regex matching: if (content.length > 100000) { console.warn('File too large, skipping pattern checks'); return; }. Review each regex for nested quantifiers and add explicit bounds.","id":"process::scripts/check-pattern-compliance.js::redos-risk","evidence":[{"type":"code_reference","detail":"scripts/check-pattern-compliance.js:106"},{"type":"description","detail":"ANTI_PATTERNS array contains many complex regex with nested quantifiers and unbounded lookaheads (e.g., line 137: /for\\s+\\w+\\s+in\\s+1\\s+2\\s+3\\s*;\\s*do[\\s\\S]{0,120}?&&\\s*break[\\s\\S]{0,80}?done(?![\\s\\S]{0,80}?...). On maliciously crafted input files, these could cause ReDoS (Regular Expression Denial of Service) hanging the checker."}],"verification_steps":{"first_pass":{"method":"code_search","evidence_collected":["scripts/check-pattern-compliance.js:106"]},"second_pass":{"method":"contextual_review","confirmed":true},"tool_confirmation":{"tool":"NONE","result":"confirmed via manual code review","reference":"scripts/check-pattern-compliance.js:106"}}}
