{"category":"process","title":"Gap: Shell scripts not linted or validated","fingerprint":"process::N/A::shell-script-no-lint","severity":"S2","effort":"E2","confidence":"HIGH","files":[".claude/hooks/analyze-user-request.sh:1",".claude/hooks/check-edit-requirements.sh:1",".claude/hooks/check-mcp-servers.sh:1",".claude/hooks/check-write-requirements.sh:1",".claude/hooks/pattern-check.sh:1",".claude/hooks/session-start.sh:1"],"why_it_matters":"6 shell scripts in .claude/hooks/ (and additional ones in skills/) can be committed with syntax errors, runtime bugs, or security issues. ShellCheck would catch common mistakes like unquoted variables, incorrect conditionals, and unsafe patterns. Shell scripts run at critical points (session start, pre-commit checks) so bugs can break development workflow.","suggested_fix":"Add ShellCheck validation: 1) Install shellcheck as devDependency, 2) Add npm script 'shellcheck:check' that runs on .claude/hooks/*.sh and .claude/skills/**/*.sh, 3) Add to pre-commit hook before other checks, 4) Add to CI workflow as blocking step, 5) Create .shellcheckrc to configure rules.","acceptance_tests":["ShellCheck runs on all .sh files","Pre-commit blocks commits with shell syntax errors","CI fails if shell scripts have issues"],"file":".claude/hooks/analyze-user-request.sh","line":1,"description":"6 shell scripts in .claude/hooks/ (and additional ones in skills/) can be committed with syntax errors, runtime bugs, or security issues. ShellCheck would catch common mistakes like unquoted variables, incorrect conditionals, and unsafe patterns. Shell scripts run at critical points (session start, pre-commit checks) so bugs can break development workflow.","recommendation":"Add ShellCheck validation: 1) Install shellcheck as devDependency, 2) Add npm script 'shellcheck:check' that runs on .claude/hooks/*.sh and .claude/skills/**/*.sh, 3) Add to pre-commit hook before other checks, 4) Add to CI workflow as blocking step, 5) Create .shellcheckrc to configure rules.","id":"process::N/A::shell-script-no-lint"}
{"category":"process","title":"Gap: Config .mjs files excluded from linting","fingerprint":"process::N/A::mjs-config-no-lint","severity":"S3","effort":"E1","confidence":"HIGH","files":["eslint.config.mjs:25","next.config.mjs:1","postcss.config.mjs:1"],"why_it_matters":"eslint.config.mjs explicitly excludes '*.config.mjs' from linting. These are critical configuration files (Next.js, PostCSS, ESLint itself) that affect build and development. Syntax errors or security issues in these files can break builds or introduce vulnerabilities. Currently they can be committed without any validation.","suggested_fix":"Remove '*.config.mjs' from ignores array in eslint.config.mjs. If specific rules need to be relaxed for config files, create a separate configuration block with adjusted rules (e.g., allow 'export default' without explicit types). Verify all config files pass linting before making change blocking.","acceptance_tests":["ESLint runs on all .mjs config files","Changes to config files are validated in pre-commit","No build breaks after enabling linting"],"file":"eslint.config.mjs","line":25,"description":"eslint.config.mjs explicitly excludes '*.config.mjs' from linting. These are critical configuration files (Next.js, PostCSS, ESLint itself) that affect build and development. Syntax errors or security issues in these files can break builds or introduce vulnerabilities. Currently they can be committed without any validation.","recommendation":"Remove '*.config.mjs' from ignores array in eslint.config.mjs. If specific rules need to be relaxed for config files, create a separate configuration block with adjusted rules (e.g., allow 'export default' without explicit types). Verify all config files pass linting before making change blocking.","id":"process::N/A::mjs-config-no-lint"}
{"category":"process","title":"Gap: Scripts directory missing test coverage","fingerprint":"process::N/A::scripts-no-tests","severity":"S2","effort":"E3","confidence":"HIGH","files":["scripts/:1"],"why_it_matters":"89 JavaScript files in scripts/ directory but only 5 have test coverage (check-docs-light, phase-complete-check, surface-lessons-learned, update-readme-status, validate-audit-s0s1). These scripts handle critical automation: debt management, audit validation, security checks, hook health, document sync. Bugs in these scripts can corrupt data, break CI, or cause incorrect validation results. Scripts like validate-audit.js, security-check.js, and debt/validate-schema.js are used as blocking gates in pre-commit/pre-push.","suggested_fix":"Prioritize test coverage for blocking scripts: 1) Start with security-check.js (blocks pre-push), 2) Add tests for debt/validate-schema.js (blocks commits), 3) Cover validate-audit.js (blocks S0/S1), 4) Add tests for check-pattern-compliance.js (blocks commits), 5) Expand to other critical scripts. Create tests/scripts/__helpers__ for common test utilities. Aim for 60%+ coverage of blocking scripts, 30%+ for others.","acceptance_tests":["Tests exist for all blocking validation scripts","Coverage report shows >60% for critical scripts","Test suite catches intentional bugs in validation logic"],"file":"scripts/","line":1,"description":"89 JavaScript files in scripts/ directory but only 5 have test coverage (check-docs-light, phase-complete-check, surface-lessons-learned, update-readme-status, validate-audit-s0s1). These scripts handle critical automation: debt management, audit validation, security checks, hook health, document sync. Bugs in these scripts can corrupt data, break CI, or cause incorrect validation results. Scripts like validate-audit.js, security-check.js, and debt/validate-schema.js are used as blocking gates in pre-commit/pre-push.","recommendation":"Prioritize test coverage for blocking scripts: 1) Start with security-check.js (blocks pre-push), 2) Add tests for debt/validate-schema.js (blocks commits), 3) Cover validate-audit.js (blocks S0/S1), 4) Add tests for check-pattern-compliance.js (blocks commits), 5) Expand to other critical scripts. Create tests/scripts/__helpers__ for common test utilities. Aim for 60%+ coverage of blocking scripts, 30%+ for others.","id":"process::N/A::scripts-no-tests"}
{"category":"process","title":"Gap: Firebase functions lack integration tests","fingerprint":"process::N/A::functions-no-integration-tests","severity":"S2","effort":"E3","confidence":"HIGH","files":["functions/src/admin.ts:1","functions/src/recaptcha-verify.ts:1","functions/src/jobs.ts:1","functions/src/security-wrapper.ts:1"],"why_it_matters":"8 TypeScript files in functions/src/ contain Cloud Functions (admin operations, recaptcha verification, scheduled jobs, security wrappers) but no integration tests exist in functions/ directory. While unit tests exist for the main app, Firebase functions interact with Firestore, authentication, and external APIs. Integration tests would catch: incorrect Firestore rules interactions, auth token validation issues, rate limiting failures, scheduled job execution problems. Functions are deployed to production and handle sensitive operations.","suggested_fix":"Set up Firebase Functions integration test framework: 1) Install firebase-functions-test (already in devDeps), 2) Create functions/test/ directory, 3) Add test files for each function module (admin.test.ts, recaptcha-verify.test.ts, jobs.test.ts), 4) Use Firebase emulators for Firestore/Auth, 5) Add 'test' script to functions/package.json, 6) Run function tests in CI after main tests, 7) Document test setup in functions/README.md.","acceptance_tests":["Integration tests exist for all Cloud Functions","Tests run against Firebase emulators","CI runs function tests and fails on errors","Coverage includes auth, Firestore, and API interactions"],"file":"functions/src/admin.ts","line":1,"description":"8 TypeScript files in functions/src/ contain Cloud Functions (admin operations, recaptcha verification, scheduled jobs, security wrappers) but no integration tests exist in functions/ directory. While unit tests exist for the main app, Firebase functions interact with Firestore, authentication, and external APIs. Integration tests would catch: incorrect Firestore rules interactions, auth token validation issues, rate limiting failures, scheduled job execution problems. Functions are deployed to production and handle sensitive operations.","recommendation":"Set up Firebase Functions integration test framework: 1) Install firebase-functions-test (already in devDeps), 2) Create functions/test/ directory, 3) Add test files for each function module (admin.test.ts, recaptcha-verify.test.ts, jobs.test.ts), 4) Use Firebase emulators for Firestore/Auth, 5) Add 'test' script to functions/package.json, 6) Run function tests in CI after main tests, 7) Document test setup in functions/README.md.","id":"process::N/A::functions-no-integration-tests"}
{"category":"process","title":"Gap: Skills missing usage documentation","fingerprint":"process::N/A::skills-no-usage-docs","severity":"S3","effort":"E3","confidence":"HIGH","files":[".claude/skills/:1"],"why_it_matters":"56 skills exist but 0 have USAGE.md documentation, only 1 has README.md. SKILL_INDEX.md shows skills organized by category (Audit & Code Quality, Session Management, Development Roles, etc.) but individual skills lack: usage examples, parameter documentation, expected outputs, common use cases, troubleshooting tips. This makes skills harder to use correctly and increases likelihood of misuse. New team members or AI agents using these skills lack guidance.","suggested_fix":"Create standardized skill documentation template: 1) Add USAGE.md template to skill-creator skill, 2) Document top 10 most-used skills first (check MCP logs for frequency), 3) Include sections: Synopsis, Parameters, Examples, Expected Output, Common Issues, Related Skills, 4) Add skills:check-docs npm script to validate USAGE.md exists and has required sections, 5) Add to pre-commit check when skill files are modified, 6) Generate missing USAGE.md files in batch using doc-optimizer skill.","acceptance_tests":["All skills have USAGE.md with required sections","skills:check-docs validates documentation completeness","Pre-commit checks USAGE.md when SKILL.md changes","SKILL_INDEX.md links to usage documentation"],"file":".claude/skills/","line":1,"description":"56 skills exist but 0 have USAGE.md documentation, only 1 has README.md. SKILL_INDEX.md shows skills organized by category (Audit & Code Quality, Session Management, Development Roles, etc.) but individual skills lack: usage examples, parameter documentation, expected outputs, common use cases, troubleshooting tips. This makes skills harder to use correctly and increases likelihood of misuse. New team members or AI agents using these skills lack guidance.","recommendation":"Create standardized skill documentation template: 1) Add USAGE.md template to skill-creator skill, 2) Document top 10 most-used skills first (check MCP logs for frequency), 3) Include sections: Synopsis, Parameters, Examples, Expected Output, Common Issues, Related Skills, 4) Add skills:check-docs npm script to validate USAGE.md exists and has required sections, 5) Add to pre-commit check when skill files are modified, 6) Generate missing USAGE.md files in batch using doc-optimizer skill.","id":"process::N/A::skills-no-usage-docs"}
{"category":"process","title":"Gap: YAML workflow files not linted","fingerprint":"process::N/A::yaml-no-lint","severity":"S3","effort":"E1","confidence":"HIGH","files":[".github/workflows/ci.yml:1",".github/workflows/deploy-firebase.yml:1",".github/workflows/docs-lint.yml:1"],"why_it_matters":"10 GitHub workflow YAML files exist (.github/workflows/*.yml) but no YAML linting is configured. Workflow files control CI/CD, deployments, security checks, and automation. YAML syntax errors break CI/CD pipelines. Invalid workflow syntax might not be caught until push, wasting time. Indentation errors, incorrect anchors, or invalid keys can cause silent failures or unexpected behavior.","suggested_fix":"Add YAML linting: 1) Install yamllint as devDependency, 2) Create .yamllint.yml config (set line-length to 120, indent to 2, allow comments), 3) Add npm script 'yaml:lint' that checks .github/workflows/*.yml and .serena/project.yml, 4) Add to pre-commit hook (non-blocking warning first), 5) Add to CI as blocking step, 6) Fix any existing issues before making blocking.","acceptance_tests":["yamllint runs on all YAML files","Pre-commit warns about YAML issues","CI fails if workflow files have syntax errors","Intentional YAML errors are caught"],"file":".github/workflows/ci.yml","line":1,"description":"10 GitHub workflow YAML files exist (.github/workflows/*.yml) but no YAML linting is configured. Workflow files control CI/CD, deployments, security checks, and automation. YAML syntax errors break CI/CD pipelines. Invalid workflow syntax might not be caught until push, wasting time. Indentation errors, incorrect anchors, or invalid keys can cause silent failures or unexpected behavior.","recommendation":"Add YAML linting: 1) Install yamllint as devDependency, 2) Create .yamllint.yml config (set line-length to 120, indent to 2, allow comments), 3) Add npm script 'yaml:lint' that checks .github/workflows/*.yml and .serena/project.yml, 4) Add to pre-commit hook (non-blocking warning first), 5) Add to CI as blocking step, 6) Fix any existing issues before making blocking.","id":"process::N/A::yaml-no-lint"}
{"category":"process","title":"Gap: Environment files not validated","fingerprint":"process::N/A::env-no-validation","severity":"S2","effort":"E2","confidence":"HIGH","files":[".env.local.example:1","functions/.env.local.example:1"],"why_it_matters":"Multiple .env files exist (.env.local.example, functions/.env.local.example, .env.production) but no validation of required variables or format. Missing required env vars cause runtime errors. Incorrect env var formats (URLs without protocols, invalid API keys) fail late. Example files can become stale and missing new required variables. No check that actual .env files match the example structure.","suggested_fix":"Create environment validation: 1) Add scripts/validate-env.js that reads .env.local.example and checks actual .env files have required keys, 2) Validate format (URLs, numeric values, required prefixes like NEXT_PUBLIC_), 3) Add npm script 'env:validate', 4) Run in pre-push as non-blocking warning (can't block since .env is gitignored), 5) Add to CI for production builds, 6) Check functions/.env separately with functions-specific requirements, 7) Document required env vars in README.md.","acceptance_tests":["env:validate checks all required variables exist","CI fails if production env vars missing","Format validation catches malformed values","Example files stay in sync with requirements"],"file":".env.local.example","line":1,"description":"Multiple .env files exist (.env.local.example, functions/.env.local.example, .env.production) but no validation of required variables or format. Missing required env vars cause runtime errors. Incorrect env var formats (URLs without protocols, invalid API keys) fail late. Example files can become stale and missing new required variables. No check that actual .env files match the example structure.","recommendation":"Create environment validation: 1) Add scripts/validate-env.js that reads .env.local.example and checks actual .env files have required keys, 2) Validate format (URLs, numeric values, required prefixes like NEXT_PUBLIC_), 3) Add npm script 'env:validate', 4) Run in pre-push as non-blocking warning (can't block since .env is gitignored), 5) Add to CI for production builds, 6) Check functions/.env separately with functions-specific requirements, 7) Document required env vars in README.md.","id":"process::N/A::env-no-validation"}
{"category":"process","title":"Gap: Firebase functions TypeScript not type-checked in pre-push","fingerprint":"process::N/A::functions-no-type-check-pre-push","severity":"S3","effort":"E1","confidence":"HIGH","files":[".husky/pre-push:82","functions/tsconfig.json:1"],"why_it_matters":"Pre-push hook (line 82-90) runs 'npx tsc --noEmit' for type checking but only for the main project, not for functions/. Firebase functions have their own TypeScript config (functions/tsconfig.json) and can have type errors that slip through. Type errors in functions are only caught during 'npm run build' in functions/ which might not be run before push. CI doesn't explicitly type-check functions directory either (only runs functions build during deploy workflow).","suggested_fix":"Add functions type check to pre-push: 1) After main type check in .husky/pre-push (line 90), add functions type check, 2) Run 'cd functions && npx tsc --noEmit' (or use absolute path), 3) Show appropriate error message if functions type check fails, 4) Consider adding to CI workflow as explicit step before build, 5) Ensure functions TypeScript errors are visible and block push.","acceptance_tests":["Pre-push runs tsc --noEmit on functions/","Type errors in functions/ block push","CI explicitly type-checks functions directory","Intentional type errors in functions are caught"],"file":".husky/pre-push","line":82,"description":"Pre-push hook (line 82-90) runs 'npx tsc --noEmit' for type checking but only for the main project, not for functions/. Firebase functions have their own TypeScript config (functions/tsconfig.json) and can have type errors that slip through. Type errors in functions are only caught during 'npm run build' in functions/ which might not be run before push. CI doesn't explicitly type-check functions directory either (only runs functions build during deploy workflow).","recommendation":"Add functions type check to pre-push: 1) After main type check in .husky/pre-push (line 90), add functions type check, 2) Run 'cd functions && npx tsc --noEmit' (or use absolute path), 3) Show appropriate error message if functions type check fails, 4) Consider adding to CI workflow as explicit step before build, 5) Ensure functions TypeScript errors are visible and block push.","id":"process::N/A::functions-no-type-check-pre-push"}
{"category":"process","title":"Gap: No syntax validation for committed shell scripts in CI","fingerprint":"process::N/A::ci-no-shell-syntax-check","severity":"S2","effort":"E2","confidence":"HIGH","files":[".github/workflows/ci.yml:1",".husky/pre-commit:1"],"why_it_matters":"CI workflow checks many things (ESLint, TypeScript, tests, patterns, security) but doesn't validate shell script syntax. Pre-commit hook is itself a shell script (.husky/pre-commit) - if it has syntax errors, commits can become blocked or validation can silently fail. .claude/hooks/ contains 6 critical shell scripts that run during development. A shell syntax error could break session-start.sh or pattern-check.sh, disrupting development flow. While these might work on developer's machine, different shell versions or environments could expose issues.","suggested_fix":"Add shell script validation to CI: 1) Install ShellCheck in CI environment (add to CI job steps), 2) Add step after 'Checkout code' named 'Validate shell scripts', 3) Run 'shellcheck .husky/pre-commit .husky/pre-push .claude/hooks/*.sh .claude/skills/**/*.sh', 4) Make blocking (don't use continue-on-error), 5) Add corresponding pre-commit check so issues are caught earlier, 6) Document shell script standards in CONTRIBUTING.md.","acceptance_tests":["CI runs shellcheck on all shell scripts","CI fails if shell syntax errors exist","Pull requests blocked by shell script issues","Intentional syntax errors caught in CI"],"file":".github/workflows/ci.yml","line":1,"description":"CI workflow checks many things (ESLint, TypeScript, tests, patterns, security) but doesn't validate shell script syntax. Pre-commit hook is itself a shell script (.husky/pre-commit) - if it has syntax errors, commits can become blocked or validation can silently fail. .claude/hooks/ contains 6 critical shell scripts that run during development. A shell syntax error could break session-start.sh or pattern-check.sh, disrupting development flow. While these might work on developer's machine, different shell versions or environments could expose issues.","recommendation":"Add shell script validation to CI: 1) Install ShellCheck in CI environment (add to CI job steps), 2) Add step after 'Checkout code' named 'Validate shell scripts', 3) Run 'shellcheck .husky/pre-commit .husky/pre-push .claude/hooks/*.sh .claude/skills/**/*.sh', 4) Make blocking (don't use continue-on-error), 5) Add corresponding pre-commit check so issues are caught earlier, 6) Document shell script standards in CONTRIBUTING.md.","id":"process::N/A::ci-no-shell-syntax-check"}
{"category":"process","title":"Gap: No validation that new files are covered by appropriate checks","fingerprint":"process::N/A::new-files-coverage-check","severity":"S3","effort":"E2","confidence":"MEDIUM","files":[".husky/pre-commit:1"],"why_it_matters":"When new file types are added to the project (e.g., .proto files, .graphql, .tf for Terraform), there's no check that they're covered by linting, validation, or security checks. Pre-commit checks specific file types (.md for doc index, .jsonl for debt, .ts/.js for ESLint) but doesn't ensure NEW file types have appropriate validation. Could add Terraform files without terraform validate, GraphQL without schema validation, Protocol Buffers without protolint, etc. Gap would only be noticed during PR review or when issues occur.","suggested_fix":"Create new-file-type detection: 1) Add scripts/check-file-coverage.js that detects file extensions in repo, 2) Maintain config/known-file-types.json mapping extensions to their validators (e.g., .ts->ESLint+TypeScript, .sh->ShellCheck, .yml->yamllint), 3) Check if any committed files have extensions not in known list, 4) Warning in pre-commit (non-blocking), 5) Add 'coverage:files' npm script for manual checking, 6) Run in CI as informational (continue-on-error: true), 7) Prompt to add validation for new file types.","acceptance_tests":["New file types trigger coverage warnings","Known file types have documented validators","Adding .proto file without protolint shows warning","config/known-file-types.json is maintainable"],"file":".husky/pre-commit","line":1,"description":"When new file types are added to the project (e.g., .proto files, .graphql, .tf for Terraform), there's no check that they're covered by linting, validation, or security checks. Pre-commit checks specific file types (.md for doc index, .jsonl for debt, .ts/.js for ESLint) but doesn't ensure NEW file types have appropriate validation. Could add Terraform files without terraform validate, GraphQL without schema validation, Protocol Buffers without protolint, etc. Gap would only be noticed during PR review or when issues occur.","recommendation":"Create new-file-type detection: 1) Add scripts/check-file-coverage.js that detects file extensions in repo, 2) Maintain config/known-file-types.json mapping extensions to their validators (e.g., .ts->ESLint+TypeScript, .sh->ShellCheck, .yml->yamllint), 3) Check if any committed files have extensions not in known list, 4) Warning in pre-commit (non-blocking), 5) Add 'coverage:files' npm script for manual checking, 6) Run in CI as informational (continue-on-error: true), 7) Prompt to add validation for new file types.","id":"process::N/A::new-files-coverage-check"}
{"category":"process","title":"Improve: Consolidate duplicate script patterns into single script with flags","fingerprint":"process::automation::consolidate-flagged-scripts","severity":"S3","effort":"E2","confidence":"HIGH","files":["package.json:1"],"why_it_matters":"8+ script pairs use pattern 'script:action' + 'script:action-variant' calling same script with different flags -> Single consolidated script reduces maintenance and testing surface","suggested_fix":"Replace script pairs (learning:analyze/dashboard/detailed, patterns:check/check-all, security:check/check-all, session:gaps/gaps:fix, override:log/list, agents:check/check-strict) with single scripts that accept flags via npm -- syntax (e.g., 'npm run learning -- --dashboard')","acceptance_tests":["All script pairs consolidated to single script with flags","All existing script calls updated in hooks/CI","All scripts still execute correctly with new flag syntax"],"file":"package.json","line":1,"description":"8+ script pairs use pattern 'script:action' + 'script:action-variant' calling same script with different flags -> Single consolidated script reduces maintenance and testing surface","recommendation":"Replace script pairs (learning:analyze/dashboard/detailed, patterns:check/check-all, security:check/check-all, session:gaps/gaps:fix, override:log/list, agents:check/check-strict) with single scripts that accept flags via npm -- syntax (e.g., 'npm run learning -- --dashboard')","id":"process::automation::consolidate-flagged-scripts"}
{"category":"process","title":"Improve: Reduce pre-commit hook check duplication with CI","fingerprint":"process::automation::hook-ci-duplication","severity":"S3","effort":"E2","confidence":"HIGH","files":[".husky/pre-commit:1",".github/workflows/ci.yml:1"],"why_it_matters":"Pattern compliance runs 3x (pre-commit, pre-push, CI), type checking runs 2x (pre-push, CI), tests run 2x (pre-commit conditional, CI) -> Wastes developer time and CI resources","suggested_fix":"Move expensive checks (type checking, full test suite) exclusively to CI. Keep only fast checks (<5s each) in pre-commit: ESLint, lint-staged, pattern compliance on staged files only. Add --staged flag to pattern compliance script for faster pre-commit checks.","acceptance_tests":["Pre-commit hook completes in <15 seconds for typical commits","CI still catches all issues that pre-commit would have caught","Developer feedback indicates improved commit experience"],"file":".husky/pre-commit","line":1,"description":"Pattern compliance runs 3x (pre-commit, pre-push, CI), type checking runs 2x (pre-push, CI), tests run 2x (pre-commit conditional, CI) -> Wastes developer time and CI resources","recommendation":"Move expensive checks (type checking, full test suite) exclusively to CI. Keep only fast checks (<5s each) in pre-commit: ESLint, lint-staged, pattern compliance on staged files only. Add --staged flag to pattern compliance script for faster pre-commit checks.","id":"process::automation::hook-ci-duplication"}
{"category":"process","title":"Improve: Eliminate redundant npm run then re-run pattern in hooks","fingerprint":"process::automation::hook-output-inefficiency","severity":"S3","effort":"E1","confidence":"HIGH","files":[".husky/pre-commit:9",".husky/pre-commit:35",".husky/pre-push:12",".husky/pre-push:26"],"why_it_matters":"Multiple hooks run 'npm run cmd > /dev/null 2>&1' to check exit code, then re-run 'npm run cmd 2>&1 | tail' to show output on failure -> Doubles execution time for failing checks","suggested_fix":"Capture output to temp file once: 'npm run cmd > $tmpfile 2>&1; code=$?; if [ $code -ne 0 ]; then tail $tmpfile; exit 1; fi'. Already used correctly for tests (line 56-66).","acceptance_tests":["All hook checks run once and cache output","Hook execution time reduced by 30-50% on failures","Error output still displayed correctly"],"file":".husky/pre-commit","line":9,"description":"Multiple hooks run 'npm run cmd > /dev/null 2>&1' to check exit code, then re-run 'npm run cmd 2>&1 | tail' to show output on failure -> Doubles execution time for failing checks","recommendation":"Capture output to temp file once: 'npm run cmd > $tmpfile 2>&1; code=$?; if [ $code -ne 0 ]; then tail $tmpfile; exit 1; fi'. Already used correctly for tests (line 56-66).","id":"process::automation::hook-output-inefficiency"}
{"category":"process","title":"Improve: Add CI caching for test build artifacts","fingerprint":"process::automation::ci-cache-test-build","severity":"S3","effort":"E2","confidence":"HIGH","files":[".github/workflows/ci.yml:114","package.json:11"],"why_it_matters":"test:build compiles TypeScript to dist-tests/ on every CI run, taking 20-30s -> No caching means repeated compilation","suggested_fix":"Add GitHub Actions cache for dist-tests/ directory keyed on hash of src/tests/ and tsconfig.test.json. Skip test:build if cache hit and source unchanged.","acceptance_tests":["Test build cache implemented in CI workflow","Cache hit rate >70% after initial runs","CI test step time reduced by 15-25 seconds on cache hits"],"file":".github/workflows/ci.yml","line":114,"description":"test:build compiles TypeScript to dist-tests/ on every CI run, taking 20-30s -> No caching means repeated compilation","recommendation":"Add GitHub Actions cache for dist-tests/ directory keyed on hash of src/tests/ and tsconfig.test.json. Skip test:build if cache hit and source unchanged.","id":"process::automation::ci-cache-test-build"}
{"category":"process","title":"Improve: Combine test:build and type check into single tsc invocation","fingerprint":"process::automation::redundant-tsc-invocations","severity":"S3","effort":"E2","confidence":"MEDIUM","files":["package.json:11",".github/workflows/ci.yml:112"],"why_it_matters":"CI runs 'npm run test:build' (tsc -p tsconfig.test.json) AND 'tsc --noEmit' separately -> Redundant TypeScript compilation (40-60s total)","suggested_fix":"Refactor test compilation to use 'tsc --noEmit' for type checking, then use esbuild or tsx for faster test execution. OR ensure test:build also validates non-test types and remove separate type check step.","acceptance_tests":["Single type check step validates all TypeScript files","Test execution still works correctly","CI type checking time reduced by 20-30 seconds"],"file":"package.json","line":11,"description":"CI runs 'npm run test:build' (tsc -p tsconfig.test.json) AND 'tsc --noEmit' separately -> Redundant TypeScript compilation (40-60s total)","recommendation":"Refactor test compilation to use 'tsc --noEmit' for type checking, then use esbuild or tsx for faster test execution. OR ensure test:build also validates non-test types and remove separate type check step.","id":"process::automation::redundant-tsc-invocations"}
{"category":"process","title":"Improve: Parallelize independent CI jobs","fingerprint":"process::automation::ci-parallelization","severity":"S3","effort":"E2","confidence":"HIGH","files":[".github/workflows/ci.yml:1"],"why_it_matters":"lint-typecheck-test job runs 15+ steps sequentially, many are independent (lint, format:check, deps:circular, deps:unused) -> Sequential execution adds 2-3 minutes","suggested_fix":"Split CI into parallel jobs: 1) Lint & Format (eslint, prettier, markdown) 2) Dependencies (circular, unused) 3) Type Check & Test 4) Build. Use needs: to sequence only what's required.","acceptance_tests":["CI jobs split into 3-4 parallel jobs","Total CI time reduced from ~8min to ~5min","All checks still execute and catch issues"],"file":".github/workflows/ci.yml","line":1,"description":"lint-typecheck-test job runs 15+ steps sequentially, many are independent (lint, format:check, deps:circular, deps:unused) -> Sequential execution adds 2-3 minutes","recommendation":"Split CI into parallel jobs: 1) Lint & Format (eslint, prettier, markdown) 2) Dependencies (circular, unused) 3) Type Check & Test 4) Build. Use needs: to sequence only what's required.","id":"process::automation::ci-parallelization"}
{"category":"process","title":"Improve: Make npm audit scheduled instead of on every push","fingerprint":"process::automation::scheduled-npm-audit","severity":"S3","effort":"E1","confidence":"HIGH","files":[".husky/pre-push:92"],"why_it_matters":"npm audit runs on every git push (non-blocking, 3-8s) checking for vulnerabilities -> Slows down push, security issues rarely change between pushes","suggested_fix":"Remove npm audit from pre-push hook. Add scheduled workflow (daily/weekly) to run npm audit and create GitHub issue if high/critical vulnerabilities found. Faster feedback loop for developers.","acceptance_tests":["npm audit removed from pre-push hook","Scheduled workflow runs npm audit daily","GitHub issues auto-created for high/critical vulnerabilities","Developer push time reduced by 3-8 seconds"],"file":".husky/pre-push","line":92,"description":"npm audit runs on every git push (non-blocking, 3-8s) checking for vulnerabilities -> Slows down push, security issues rarely change between pushes","recommendation":"Remove npm audit from pre-push hook. Add scheduled workflow (daily/weekly) to run npm audit and create GitHub issue if high/critical vulnerabilities found. Faster feedback loop for developers.","id":"process::automation::scheduled-npm-audit"}
{"category":"process","title":"Improve: Auto-update DOCUMENTATION_INDEX.md in pre-commit hook","fingerprint":"process::automation::auto-update-doc-index","severity":"S3","effort":"E2","confidence":"HIGH","files":[".husky/pre-commit:134","package.json:18"],"why_it_matters":"Pre-commit hook BLOCKS if .md files changed but DOCUMENTATION_INDEX.md not updated, requiring manual 'npm run docs:index && git add' -> Friction in commit workflow","suggested_fix":"Auto-run 'npm run docs:index' and auto-stage DOCUMENTATION_INDEX.md when .md files are in commit. Show diff and ask for confirmation, or make it automatic with override flag SKIP_DOC_INDEX_AUTO=1.","acceptance_tests":["DOCUMENTATION_INDEX.md auto-updates when .md files staged","Index automatically added to commit","Warning shown to user with diff summary","Override flag available for edge cases"],"file":".husky/pre-commit","line":134,"description":"Pre-commit hook BLOCKS if .md files changed but DOCUMENTATION_INDEX.md not updated, requiring manual 'npm run docs:index && git add' -> Friction in commit workflow","recommendation":"Auto-run 'npm run docs:index' and auto-stage DOCUMENTATION_INDEX.md when .md files are in commit. Show diff and ask for confirmation, or make it automatic with override flag SKIP_DOC_INDEX_AUTO=1.","id":"process::automation::auto-update-doc-index"}
{"category":"process","title":"Improve: Combine docs-lint.yml checks into main CI workflow","fingerprint":"process::automation::consolidate-docs-workflow","severity":"S3","effort":"E2","confidence":"HIGH","files":[".github/workflows/docs-lint.yml:1",".github/workflows/ci.yml:77"],"why_it_matters":"docs-lint.yml and ci.yml both trigger on PRs touching docs, both run documentation checks -> Duplicate workflow runs and maintenance burden","suggested_fix":"Move docs-lint functionality into ci.yml as a separate job that runs conditionally (if: contains(changed-files, '*.md')). Reduces workflows from 10 to 9, single place for doc linting logic.","acceptance_tests":["docs-lint.yml archived or removed","Doc linting integrated into ci.yml as conditional job","PR comments still posted for doc lint failures","No regression in doc validation coverage"],"file":".github/workflows/docs-lint.yml","line":1,"description":"docs-lint.yml and ci.yml both trigger on PRs touching docs, both run documentation checks -> Duplicate workflow runs and maintenance burden","recommendation":"Move docs-lint functionality into ci.yml as a separate job that runs conditionally (if: contains(changed-files, '*.md')). Reduces workflows from 10 to 9, single place for doc linting logic.","id":"process::automation::consolidate-docs-workflow"}
{"category":"process","title":"Improve: Use task runner (turbo/nx) for script orchestration","fingerprint":"process::automation::task-runner-migration","severity":"S3","effort":"E3","confidence":"MEDIUM","files":["package.json:1",".husky/pre-commit:1",".github/workflows/ci.yml:1"],"why_it_matters":"70+ npm scripts with complex dependencies, no dependency caching, sequential execution in hooks/CI -> Slow execution, hard to maintain, no incremental builds","suggested_fix":"Introduce turbo or nx for: 1) Dependency graph (test depends on test:build) 2) Caching (hash-based) 3) Parallel execution 4) Incremental rebuilds. Start with test pipeline, expand to lint/build.","acceptance_tests":["Turbo/nx installed and configured for test pipeline","Test execution time reduced by 40-60% on repeat runs","CI leverages remote caching for cross-run speedups","Migration path documented for remaining scripts"],"file":"package.json","line":1,"description":"70+ npm scripts with complex dependencies, no dependency caching, sequential execution in hooks/CI -> Slow execution, hard to maintain, no incremental builds","recommendation":"Introduce turbo or nx for: 1) Dependency graph (test depends on test:build) 2) Caching (hash-based) 3) Parallel execution 4) Incremental rebuilds. Start with test pipeline, expand to lint/build.","id":"process::automation::task-runner-migration"}
{"category":"process","title":"Improve: Simplify Firebase deployment workflow","fingerprint":"process::automation::firebase-deploy-simplification","severity":"S3","effort":"E2","confidence":"MEDIUM","files":[".github/workflows/deploy-firebase.yml:73",".github/workflows/deploy-firebase.yml:140"],"why_it_matters":"Deploy workflow has 3 sequential deployment steps (functions, rules, hosting) that could run in parallel -> Adds 2-3 minutes to deployment time","suggested_fix":"Use 'firebase deploy --only functions,firestore:rules,hosting' single command OR parallelize as separate jobs with proper sequencing. Also remove deprecated function deletion step (line 131-138) which is continue-on-error anyway.","acceptance_tests":["Firebase deployment streamlined to single command or parallel jobs","Deployment time reduced by 1-2 minutes","Deprecated function deletion step removed","All targets (functions, rules, hosting) still deploy correctly"],"file":".github/workflows/deploy-firebase.yml","line":73,"description":"Deploy workflow has 3 sequential deployment steps (functions, rules, hosting) that could run in parallel -> Adds 2-3 minutes to deployment time","recommendation":"Use 'firebase deploy --only functions,firestore:rules,hosting' single command OR parallelize as separate jobs with proper sequencing. Also remove deprecated function deletion step (line 131-138) which is continue-on-error anyway.","id":"process::automation::firebase-deploy-simplification"}
{"category":"process","title":"Improve: Backlog enforcement workflow references archived file","fingerprint":"process::automation::backlog-stale-reference","severity":"S3","effort":"E1","confidence":"HIGH","files":[".github/workflows/backlog-enforcement.yml:32"],"why_it_matters":"Workflow checks AUDIT_FINDINGS_BACKLOG.md which was archived in TDMS Phase 2 -> Workflow always exits early, provides no value, maintenance burden","suggested_fix":"Either: 1) Update workflow to check docs/technical-debt/MASTER_DEBT.jsonl for backlog health (count open items, S0/S1 thresholds) OR 2) Archive/remove workflow if backlog-health is checked elsewhere.","acceptance_tests":["Workflow updated to check MASTER_DEBT.jsonl OR archived","If updated: workflow correctly validates debt backlog health","If updated: workflow fails when S0 items exist or total >threshold","If archived: workflow disabled and documented in CHANGELOG"],"file":".github/workflows/backlog-enforcement.yml","line":32,"description":"Workflow checks AUDIT_FINDINGS_BACKLOG.md which was archived in TDMS Phase 2 -> Workflow always exits early, provides no value, maintenance burden","recommendation":"Either: 1) Update workflow to check docs/technical-debt/MASTER_DEBT.jsonl for backlog health (count open items, S0/S1 thresholds) OR 2) Archive/remove workflow if backlog-health is checked elsewhere.","id":"process::automation::backlog-stale-reference"}
{"category":"process","title":"Improve: Add hook timing instrumentation","fingerprint":"process::automation::hook-timing-visibility","severity":"S3","effort":"E1","confidence":"HIGH","files":[".husky/pre-commit:1",".husky/pre-push:1"],"why_it_matters":"Pre-commit has 13 check steps (280 lines), pre-push has 7 steps (155 lines), but no visibility into which steps are slow -> Can't identify optimization opportunities","suggested_fix":"Add timing instrumentation: 'START=$(date +%s); ... ; echo \"  ⏱️  Took $(($(date +%s) - START))s\"' for each major step. OR use time command. Log slow steps (>3s) to .git/hooks/timing.log for analysis.","acceptance_tests":["Each hook step logs execution time","Slow steps (>3s) identified in hook output","Timing data helps prioritize future optimizations","Optional: aggregate timing logged to file for analysis"],"file":".husky/pre-commit","line":1,"description":"Pre-commit has 13 check steps (280 lines), pre-push has 7 steps (155 lines), but no visibility into which steps are slow -> Can't identify optimization opportunities","recommendation":"Add timing instrumentation: 'START=$(date +%s); ... ; echo \"  ⏱️  Took $(($(date +%s) - START))s\"' for each major step. OR use time command. Log slow steps (>3s) to .git/hooks/timing.log for analysis.","id":"process::automation::hook-timing-visibility"}
{"category":"process","title":"Improve: Consolidate security checking into single workflow","fingerprint":"process::automation::consolidate-security-checks","severity":"S3","effort":"E2","confidence":"HIGH","files":[".github/workflows/backlog-enforcement.yml:103",".husky/pre-push:38"],"why_it_matters":"Security patterns checked in pre-push hook (files being pushed) AND backlog-enforcement workflow (all files or PR files) -> Duplication and potential inconsistency","suggested_fix":"Remove security-patterns job from backlog-enforcement.yml. Keep security check in pre-push hook only. Add scheduled workflow (weekly) for full-repo security audit with GitHub Security tab integration.","acceptance_tests":["Security pattern check consolidated to pre-push hook only","Scheduled security audit workflow created for full-repo scans","backlog-enforcement workflow simplified (1 job instead of 2)","No regression in security issue detection"],"file":".github/workflows/backlog-enforcement.yml","line":103,"description":"Security patterns checked in pre-push hook (files being pushed) AND backlog-enforcement workflow (all files or PR files) -> Duplication and potential inconsistency","recommendation":"Remove security-patterns job from backlog-enforcement.yml. Keep security check in pre-push hook only. Add scheduled workflow (weekly) for full-repo security audit with GitHub Security tab integration.","id":"process::automation::consolidate-security-checks"}
{"category":"process","title":"Improve: Use lint-staged for more than just formatting","fingerprint":"process::automation::expand-lint-staged","severity":"S3","effort":"E2","confidence":"HIGH","files":["package.json:78",".husky/pre-commit:8"],"why_it_matters":"lint-staged only runs prettier (formatting) on staged files -> ESLint, pattern compliance, and other checks run on ALL files even if not staged","suggested_fix":"Expand lint-staged config: '*.{ts,tsx,js,jsx}': ['eslint --fix', 'prettier --write'], '*.md': ['markdownlint --fix', 'prettier --write']. Moves ESLint to lint-staged for automatic fixes and faster execution (staged files only).","acceptance_tests":["lint-staged config expanded to include eslint, markdownlint","Pre-commit hook updated to rely on lint-staged for more checks","Checks run only on staged files, not entire codebase","Auto-fixes applied and staged before commit"],"file":"package.json","line":78,"description":"lint-staged only runs prettier (formatting) on staged files -> ESLint, pattern compliance, and other checks run on ALL files even if not staged","recommendation":"Expand lint-staged config: '*.{ts,tsx,js,jsx}': ['eslint --fix', 'prettier --write'], '*.md': ['markdownlint --fix', 'prettier --write']. Moves ESLint to lint-staged for automatic fixes and faster execution (staged files only).","id":"process::automation::expand-lint-staged"}
{"category":"process","title":"Improve: Add commit message validation hook","fingerprint":"process::automation::add-commit-msg-hook","severity":"S3","effort":"E1","confidence":"HIGH","files":[".husky/_/commit-msg:1"],"why_it_matters":"No commit message validation enforced -> Inconsistent commit messages make changelog generation and git log navigation harder","suggested_fix":"Add .husky/commit-msg hook to validate conventional commits format (feat:, fix:, docs:, chore:, etc). Use commitlint or simple regex. Block commits with bad format or provide helpful error message.","acceptance_tests":["commit-msg hook validates conventional commit format","Invalid commit messages blocked with helpful error","Valid commit formats pass through unchanged","Override available with --no-verify for edge cases"],"file":".husky/_/commit-msg","line":1,"description":"No commit message validation enforced -> Inconsistent commit messages make changelog generation and git log navigation harder","recommendation":"Add .husky/commit-msg hook to validate conventional commits format (feat:, fix:, docs:, chore:, etc). Use commitlint or simple regex. Block commits with bad format or provide helpful error message.","id":"process::automation::add-commit-msg-hook"}
{"category":"process","title":"Improve: Add GitHub Actions workflow caching strategy","fingerprint":"process::automation::workflow-caching-strategy","severity":"S3","effort":"E2","confidence":"HIGH","files":[".github/workflows/ci.yml:22",".github/workflows/docs-lint.yml:29",".github/workflows/review-check.yml:26"],"why_it_matters":"All workflows use 'cache: npm' which only caches npm packages, not build artifacts or script outputs -> Miss opportunity for faster CI runs","suggested_fix":"Add composite caching: 1) npm packages (already cached) 2) Next.js .next/ cache 3) TypeScript build cache 4) test:build dist-tests/ output. Use cache-dependency-path for all package-lock.json files.","acceptance_tests":["Build artifact caching implemented across all workflows","Cache hit rate >60% after initial runs","CI execution time reduced by 20-30% on cache hits","Cache invalidation works correctly when dependencies change"],"file":".github/workflows/ci.yml","line":22,"description":"All workflows use 'cache: npm' which only caches npm packages, not build artifacts or script outputs -> Miss opportunity for faster CI runs","recommendation":"Add composite caching: 1) npm packages (already cached) 2) Next.js .next/ cache 3) TypeScript build cache 4) test:build dist-tests/ output. Use cache-dependency-path for all package-lock.json files.","id":"process::automation::workflow-caching-strategy"}
{"category":"process","title":"Improve: Migrate manual documentation tasks to automated triggers","fingerprint":"process::automation::automate-doc-maintenance","severity":"S3","effort":"E2","confidence":"MEDIUM","files":["package.json:14","package.json:16","package.json:18"],"why_it_matters":"3 documentation maintenance scripts require manual invocation: docs:update-readme, docs:archive, docs:index -> Leads to stale documentation","suggested_fix":"1) docs:index - already has pre-commit check, make it auto-run and stage 2) docs:update-readme - add to sync-readme.yml workflow trigger 3) docs:archive - add scheduled workflow (monthly) to identify docs that should be archived (no updates in 6+ months, marked obsolete)","acceptance_tests":["docs:index auto-runs in pre-commit when .md files staged","docs:archive scheduled workflow runs monthly with suggestions","docs:update-readme integrated into existing sync workflow","Manual script invocations reduced, documentation stays current"],"file":"package.json","line":14,"description":"3 documentation maintenance scripts require manual invocation: docs:update-readme, docs:archive, docs:index -> Leads to stale documentation","recommendation":"1) docs:index - already has pre-commit check, make it auto-run and stage 2) docs:update-readme - add to sync-readme.yml workflow trigger 3) docs:archive - add scheduled workflow (monthly) to identify docs that should be archived (no updates in 6+ months, marked obsolete)","id":"process::automation::automate-doc-maintenance"}
{"category":"process","title":"Improve: Add workflow for stale branch cleanup","fingerprint":"process::automation::stale-branch-cleanup","severity":"S3","effort":"E1","confidence":"HIGH","files":[],"why_it_matters":"No automated branch cleanup -> Old feature branches accumulate, clutter repository, cause confusion","suggested_fix":"Add scheduled workflow using actions/stale to: 1) Label branches with no commits in 30 days as 'stale' 2) Delete branches with no commits in 60 days (excluding main, develop, release/*) 3) Post comment on associated PR before deletion","acceptance_tests":["Stale branch workflow created and scheduled (weekly)","Branches inactive for 30 days labeled as stale","Branches inactive for 60 days auto-deleted with notification","Protected branches excluded from cleanup"],"file":".github/workflows/*","line":1,"description":"No automated branch cleanup -> Old feature branches accumulate, clutter repository, cause confusion","recommendation":"Add scheduled workflow using actions/stale to: 1) Label branches with no commits in 30 days as 'stale' 2) Delete branches with no commits in 60 days (excluding main, develop, release/*) 3) Post comment on associated PR before deletion","id":"process::automation::stale-branch-cleanup"}
{"category":"process","title":"Improve: Replace manual trigger checks with GitHub Actions expressions","fingerprint":"process::automation::simplify-trigger-checks","severity":"S3","effort":"E2","confidence":"HIGH","files":[".github/workflows/ci.yml:42",".github/workflows/docs-lint.yml:35"],"why_it_matters":"Workflows use tj-actions/changed-files then bash scripts to check patterns -> Extra action dependency, slower execution, more complex logic","suggested_fix":"Use GitHub Actions native path filters and expressions: 'if: contains(github.event.head_commit.modified, '.md')' or combine with paths: filter in workflow trigger. Reduces external dependencies.","acceptance_tests":["Changed files detection uses native GitHub Actions where possible","tj-actions/changed-files only used where native solution insufficient","Workflow execution time reduced by 5-10 seconds","Pattern matching still accurate and reliable"],"file":".github/workflows/ci.yml","line":42,"description":"Workflows use tj-actions/changed-files then bash scripts to check patterns -> Extra action dependency, slower execution, more complex logic","recommendation":"Use GitHub Actions native path filters and expressions: 'if: contains(github.event.head_commit.modified, '.md')' or combine with paths: filter in workflow trigger. Reduces external dependencies.","id":"process::automation::simplify-trigger-checks"}
{"category":"process","title":"Docs: Missing README in scripts/lib/","fingerprint":"process::scripts/lib::missing-readme","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/lib/:1"],"why_it_matters":"The lib/ directory contains shared utilities used across multiple scripts. Without a README, developers cannot quickly understand what utilities are available, their purpose, or usage patterns. This increases onboarding time and risk of code duplication.","suggested_fix":"Create scripts/lib/README.md documenting each utility module: ai-pattern-checks.js (AI pattern detection), sanitize-error.js (error sanitization), security-helpers.js (security utilities), validate-paths.js (path validation).","acceptance_tests":["README.md exists in scripts/lib/","README documents all utility modules","README includes usage examples"],"file":"scripts/lib/","line":1,"description":"The lib/ directory contains shared utilities used across multiple scripts. Without a README, developers cannot quickly understand what utilities are available, their purpose, or usage patterns. This increases onboarding time and risk of code duplication.","recommendation":"Create scripts/lib/README.md documenting each utility module: ai-pattern-checks.js (AI pattern detection), sanitize-error.js (error sanitization), security-helpers.js (security utilities), validate-paths.js (path validation).","id":"process::scripts/lib::missing-readme"}
{"category":"process","title":"Docs: Missing README in scripts/config/","fingerprint":"process::scripts/config::missing-readme","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/config/:1"],"why_it_matters":"The config/ directory contains JSON configuration files used by multiple scripts. Without documentation explaining the schema and purpose of each config file, developers may misuse configs or fail to update them when requirements change.","suggested_fix":"Create scripts/config/README.md documenting each config file: audit-schema.json (valid audit field values), audit-config.json (category thresholds), ai-patterns.json (AI pattern detection rules), skill-config.json (skill definitions), and load-config.js (config loader utility).","acceptance_tests":["README.md exists in scripts/config/","README documents all config files","README explains schema for each config"],"file":"scripts/config/","line":1,"description":"The config/ directory contains JSON configuration files used by multiple scripts. Without documentation explaining the schema and purpose of each config file, developers may misuse configs or fail to update them when requirements change.","recommendation":"Create scripts/config/README.md documenting each config file: audit-schema.json (valid audit field values), audit-config.json (category thresholds), ai-patterns.json (AI pattern detection rules), skill-config.json (skill definitions), and load-config.js (config loader utility).","id":"process::scripts/config::missing-readme"}
{"category":"process","title":"Docs: Missing README in scripts/debt/","fingerprint":"process::scripts/debt::missing-readme","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/debt/:1"],"why_it_matters":"The debt/ directory contains 17 technical debt management scripts. Without a README, developers cannot understand the debt workflow, which scripts to run in which order, or how to properly intake and resolve debt items.","suggested_fix":"Create scripts/debt/README.md documenting the technical debt workflow: intake scripts (intake-audit.js, intake-manual.js, intake-pr-deferred.js), processing scripts (normalize-all.js, consolidate-all.js, dedup-multi-pass.js), and resolution scripts (resolve-item.js, resolve-bulk.js). Include usage examples and workflow diagrams.","acceptance_tests":["README.md exists in scripts/debt/","README documents all debt scripts","README explains debt management workflow","README includes usage examples"],"file":"scripts/debt/","line":1,"description":"The debt/ directory contains 17 technical debt management scripts. Without a README, developers cannot understand the debt workflow, which scripts to run in which order, or how to properly intake and resolve debt items.","recommendation":"Create scripts/debt/README.md documenting the technical debt workflow: intake scripts (intake-audit.js, intake-manual.js, intake-pr-deferred.js), processing scripts (normalize-all.js, consolidate-all.js, dedup-multi-pass.js), and resolution scripts (resolve-item.js, resolve-bulk.js). Include usage examples and workflow diagrams.","id":"process::scripts/debt::missing-readme"}
{"category":"process","title":"Docs: Missing README in scripts/audit/","fingerprint":"process::scripts/audit::missing-readme","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/audit/:1"],"why_it_matters":"The audit/ directory contains audit-related scripts without documentation. Developers need to understand what these scripts do and when to use them as part of the audit process.","suggested_fix":"Create scripts/audit/README.md documenting: transform-jsonl-schema.js (schema transformation) and validate-audit-integration.js (audit validation). Include usage examples and integration points with the main audit workflow.","acceptance_tests":["README.md exists in scripts/audit/","README documents all audit scripts","README explains when to use each script"],"file":"scripts/audit/","line":1,"description":"The audit/ directory contains audit-related scripts without documentation. Developers need to understand what these scripts do and when to use them as part of the audit process.","recommendation":"Create scripts/audit/README.md documenting: transform-jsonl-schema.js (schema transformation) and validate-audit-integration.js (audit validation). Include usage examples and integration points with the main audit workflow.","id":"process::scripts/audit::missing-readme"}
{"category":"process","title":"Docs: No header comment in enrich-addresses.ts","fingerprint":"process::scripts/enrich-addresses.ts::no-header","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/enrich-addresses.ts:1"],"why_it_matters":"Script lacks header comment explaining purpose, making it difficult for developers to understand what it does without reading the implementation. Header comments serve as quick documentation.","suggested_fix":"Add JSDoc header comment explaining: Purpose (enrich meeting addresses with geocoding data from Nominatim/OSM), Usage (npx tsx scripts/enrich-addresses.ts), Prerequisites (Firebase service account, internet connection), and Rate limits (OSM Nominatim 1req/sec).","acceptance_tests":["Header comment exists","Comment explains purpose clearly","Comment includes usage instructions","Comment notes rate limits and prerequisites"],"file":"scripts/enrich-addresses.ts","line":1,"description":"Script lacks header comment explaining purpose, making it difficult for developers to understand what it does without reading the implementation. Header comments serve as quick documentation.","recommendation":"Add JSDoc header comment explaining: Purpose (enrich meeting addresses with geocoding data from Nominatim/OSM), Usage (npx tsx scripts/enrich-addresses.ts), Prerequisites (Firebase service account, internet connection), and Rate limits (OSM Nominatim 1req/sec).","id":"process::scripts/enrich-addresses.ts::no-header"}
{"category":"process","title":"Docs: No header comment in test-geocode.ts","fingerprint":"process::scripts/test-geocode.ts::no-header","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/test-geocode.ts:1"],"why_it_matters":"Test script lacks header comment explaining purpose and usage. Without documentation, developers don't know this is a test utility or what it validates.","suggested_fix":"Add JSDoc header comment explaining: Purpose (test Google Maps Geocoding API connectivity and credentials), Usage (npx tsx scripts/test-geocode.ts), Prerequisites (NEXT_PUBLIC_FIREBASE_API_KEY environment variable), and Expected output.","acceptance_tests":["Header comment exists","Comment explains testing purpose","Comment documents required environment variables","Comment describes expected results"],"file":"scripts/test-geocode.ts","line":1,"description":"Test script lacks header comment explaining purpose and usage. Without documentation, developers don't know this is a test utility or what it validates.","recommendation":"Add JSDoc header comment explaining: Purpose (test Google Maps Geocoding API connectivity and credentials), Usage (npx tsx scripts/test-geocode.ts), Prerequisites (NEXT_PUBLIC_FIREBASE_API_KEY environment variable), and Expected output.","id":"process::scripts/test-geocode.ts::no-header"}
{"category":"process","title":"Docs: No header comment in sync-geocache.ts","fingerprint":"process::scripts/sync-geocache.ts::no-header","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/sync-geocache.ts:1"],"why_it_matters":"Script lacks header comment. Developers cannot quickly understand this script syncs geocoding results from Firestore to a local cache file for performance optimization.","suggested_fix":"Add JSDoc header comment explaining: Purpose (sync meeting coordinates from Firestore to local geocoding_cache.json), Usage (npx tsx scripts/sync-geocache.ts), Prerequisites (Firebase service account, meetings collection with coordinates), and Output (geocoding_cache.json with sorted address-to-coordinate mappings).","acceptance_tests":["Header comment exists","Comment explains caching purpose","Comment documents prerequisites","Comment describes output format"],"file":"scripts/sync-geocache.ts","line":1,"description":"Script lacks header comment. Developers cannot quickly understand this script syncs geocoding results from Firestore to a local cache file for performance optimization.","recommendation":"Add JSDoc header comment explaining: Purpose (sync meeting coordinates from Firestore to local geocoding_cache.json), Usage (npx tsx scripts/sync-geocache.ts), Prerequisites (Firebase service account, meetings collection with coordinates), and Output (geocoding_cache.json with sorted address-to-coordinate mappings).","id":"process::scripts/sync-geocache.ts::no-header"}
{"category":"process","title":"Docs: No header comment in retry-failures.ts","fingerprint":"process::scripts/retry-failures.ts::no-header","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/retry-failures.ts:1"],"why_it_matters":"Script lacks header comment. Developers need to understand this retries failed geocoding operations from enrichment_failures.json.","suggested_fix":"Add JSDoc header comment explaining: Purpose (retry failed geocoding operations from enrichment_failures.json), Usage (npx tsx scripts/retry-failures.ts), Prerequisites (enrichment_failures.json, Firebase service account, OSM Nominatim access), Dependencies (requires prior run of enrich-addresses.ts), and Rate limits (2.5 second delay between requests).","acceptance_tests":["Header comment exists","Comment explains retry mechanism","Comment documents prerequisites and dependencies","Comment notes rate limiting"],"file":"scripts/retry-failures.ts","line":1,"description":"Script lacks header comment. Developers need to understand this retries failed geocoding operations from enrichment_failures.json.","recommendation":"Add JSDoc header comment explaining: Purpose (retry failed geocoding operations from enrichment_failures.json), Usage (npx tsx scripts/retry-failures.ts), Prerequisites (enrichment_failures.json, Firebase service account, OSM Nominatim access), Dependencies (requires prior run of enrich-addresses.ts), and Rate limits (2.5 second delay between requests).","id":"process::scripts/retry-failures.ts::no-header"}
{"category":"process","title":"Docs: No header comment in migrate-library-content.ts","fingerprint":"process::scripts/migrate-library-content.ts::no-header","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/migrate-library-content.ts:1"],"why_it_matters":"Migration script lacks header comment. Developers need to know this is a one-time migration from hardcoded library links to Firestore, when to run it, and side effects.","suggested_fix":"Add JSDoc header comment explaining: Purpose (one-time migration of library quick links from hardcoded array to Firestore), Usage (npx tsx scripts/migrate-library-content.ts), Prerequisites (Firebase service account), Warning (one-time use only, idempotent), and Output (populates library_content collection).","acceptance_tests":["Header comment exists","Comment explains migration purpose","Comment warns about one-time nature","Comment documents prerequisites"],"file":"scripts/migrate-library-content.ts","line":1,"description":"Migration script lacks header comment. Developers need to know this is a one-time migration from hardcoded library links to Firestore, when to run it, and side effects.","recommendation":"Add JSDoc header comment explaining: Purpose (one-time migration of library quick links from hardcoded array to Firestore), Usage (npx tsx scripts/migrate-library-content.ts), Prerequisites (Firebase service account), Warning (one-time use only, idempotent), and Output (populates library_content collection).","id":"process::scripts/migrate-library-content.ts::no-header"}
{"category":"process","title":"Docs: No header comment in seed-real-data.ts","fingerprint":"process::scripts/seed-real-data.ts::no-header","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/seed-real-data.ts:1"],"why_it_matters":"Data seeding script lacks header comment. Developers need to understand this imports meeting data from CSV with geocoding, when to use it, and potential performance implications.","suggested_fix":"Add JSDoc header comment explaining: Purpose (import meeting data from SoNash_Meetings__cleaned.csv with geocoding), Usage (npx tsx scripts/seed-real-data.ts), Prerequisites (CSV file, Firebase service account, Google Maps API key or Nominatim access), Performance notes (rate limited 1.1s per request), and Output (populates meetings collection with geocoded addresses).","acceptance_tests":["Header comment exists","Comment explains data import purpose","Comment documents prerequisites including API keys","Comment notes rate limiting and performance considerations"],"file":"scripts/seed-real-data.ts","line":1,"description":"Data seeding script lacks header comment. Developers need to understand this imports meeting data from CSV with geocoding, when to use it, and potential performance implications.","recommendation":"Add JSDoc header comment explaining: Purpose (import meeting data from SoNash_Meetings__cleaned.csv with geocoding), Usage (npx tsx scripts/seed-real-data.ts), Prerequisites (CSV file, Firebase service account, Google Maps API key or Nominatim access), Performance notes (rate limited 1.1s per request), and Output (populates meetings collection with geocoded addresses).","id":"process::scripts/seed-real-data.ts::no-header"}
{"category":"process","title":"Docs: Inadequate header in dedupe-quotes.ts","fingerprint":"process::scripts/dedupe-quotes.ts::minimal-header","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/dedupe-quotes.ts:1"],"why_it_matters":"Script has only 2-line header comment that doesn't explain purpose, usage, or expected workflow. Makes it difficult to understand when/how to use this deduplication utility.","suggested_fix":"Expand header comment to explain: Purpose (deduplication of recovery quotes from recovery_quotes_50.md), Usage, Expected input/output format, and Integration with the quote management system. Current comment 'Deduplication Script for Recovery Quotes' is too terse.","acceptance_tests":["Header comment is at least 5 lines","Comment explains deduplication algorithm","Comment includes usage instructions","Comment describes input/output"],"file":"scripts/dedupe-quotes.ts","line":1,"description":"Script has only 2-line header comment that doesn't explain purpose, usage, or expected workflow. Makes it difficult to understand when/how to use this deduplication utility.","recommendation":"Expand header comment to explain: Purpose (deduplication of recovery quotes from recovery_quotes_50.md), Usage, Expected input/output format, and Integration with the quote management system. Current comment 'Deduplication Script for Recovery Quotes' is too terse.","id":"process::scripts/dedupe-quotes.ts::minimal-header"}
{"category":"process","title":"Docs: Config file ai-patterns.json lacks inline documentation","fingerprint":"process::scripts/config/ai-patterns.json::no-schema-docs","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/config/ai-patterns.json:1"],"why_it_matters":"Configuration file defines AI pattern detection rules without schema documentation. Developers modifying patterns need to understand the structure: pattern object format, severity levels, regex descriptor format.","suggested_fix":"Add JSON comment block at top (or convert to .jsonc) documenting schema: {patterns: {[key]: {name, severity, patterns: [{source, flags}], description}}}. Explain that 'source' is regex pattern string, 'flags' are regex flags, 'severity' is S0-S3.","acceptance_tests":["Schema is documented in file or README","Pattern object structure is explained","Regex descriptor format is documented","Valid severity values are listed"],"file":"scripts/config/ai-patterns.json","line":1,"description":"Configuration file defines AI pattern detection rules without schema documentation. Developers modifying patterns need to understand the structure: pattern object format, severity levels, regex descriptor format.","recommendation":"Add JSON comment block at top (or convert to .jsonc) documenting schema: {patterns: {[key]: {name, severity, patterns: [{source, flags}], description}}}. Explain that 'source' is regex pattern string, 'flags' are regex flags, 'severity' is S0-S3.","id":"process::scripts/config/ai-patterns.json::no-schema-docs"}
{"category":"process","title":"Docs: Config file audit-config.json lacks inline documentation","fingerprint":"process::scripts/config/audit-config.json::no-schema-docs","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/config/audit-config.json:1"],"why_it_matters":"Configuration file defines audit category thresholds without documentation. Developers tuning thresholds need to understand units (commit counts vs file counts), regex descriptor format, and threshold semantics.","suggested_fix":"Add documentation explaining: categoryThresholds structure (commits=count, files=count, filePattern=regex descriptor, excludePattern=regex descriptor), multiAiThresholds (totalCommits threshold, daysSinceAudit threshold), categoryHeaders (regex to find category sections in AUDIT_TRACKER.md). Document that regex descriptors use {source, flags} format.","acceptance_tests":["Schema is documented in file or README","Threshold semantics are explained","Regex descriptor format is documented","Each field purpose is clear"],"file":"scripts/config/audit-config.json","line":1,"description":"Configuration file defines audit category thresholds without documentation. Developers tuning thresholds need to understand units (commit counts vs file counts), regex descriptor format, and threshold semantics.","recommendation":"Add documentation explaining: categoryThresholds structure (commits=count, files=count, filePattern=regex descriptor, excludePattern=regex descriptor), multiAiThresholds (totalCommits threshold, daysSinceAudit threshold), categoryHeaders (regex to find category sections in AUDIT_TRACKER.md). Document that regex descriptors use {source, flags} format.","id":"process::scripts/config/audit-config.json::no-schema-docs"}
{"category":"process","title":"Docs: Config file audit-schema.json lacks inline documentation","fingerprint":"process::scripts/config/audit-schema.json::no-schema-docs","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/config/audit-schema.json:1"],"why_it_matters":"Configuration file defines valid audit field values without explanation. Developers need to understand when to use each category, severity, type, status, and effort level.","suggested_fix":"Add documentation explaining: validCategories (what each category means: security, performance, code-quality, documentation, process, refactoring, engineering-productivity), validSeverities (S0=critical, S1=high, S2=medium, S3=low), validTypes (bug, code-smell, vulnerability, hotspot, tech-debt, process-gap), validStatuses (lifecycle), validEfforts (E0=days, E1=hours, E2=30min, E3=5min), requiredFields.","acceptance_tests":["Each category is explained","Severity levels are defined","Types are documented","Status lifecycle is explained","Effort levels are quantified"],"file":"scripts/config/audit-schema.json","line":1,"description":"Configuration file defines valid audit field values without explanation. Developers need to understand when to use each category, severity, type, status, and effort level.","recommendation":"Add documentation explaining: validCategories (what each category means: security, performance, code-quality, documentation, process, refactoring, engineering-productivity), validSeverities (S0=critical, S1=high, S2=medium, S3=low), validTypes (bug, code-smell, vulnerability, hotspot, tech-debt, process-gap), validStatuses (lifecycle), validEfforts (E0=days, E1=hours, E2=30min, E3=5min), requiredFields.","id":"process::scripts/config/audit-schema.json::no-schema-docs"}
{"category":"process","title":"Docs: Config file skill-config.json lacks inline documentation","fingerprint":"process::scripts/config/skill-config.json::no-schema-docs","severity":"S3","effort":"E1","confidence":"HIGH","files":["scripts/config/skill-config.json:1"],"why_it_matters":"Configuration file defines skill sections and patterns without documentation. Developers adding or modifying skills need to understand the schema structure and pattern format.","suggested_fix":"Add documentation explaining: requiredSections structure (audit vs session skill types), deprecatedPatterns array format ({pattern: {source, flags}, message}), topicAliases mapping (canonical topic to related terms). Explain regex descriptor format and how aliases improve search accuracy.","acceptance_tests":["Schema structure is documented","Required sections are explained","Deprecated pattern format is clear","Topic alias purpose is documented"],"file":"scripts/config/skill-config.json","line":1,"description":"Configuration file defines skill sections and patterns without documentation. Developers adding or modifying skills need to understand the schema structure and pattern format.","recommendation":"Add documentation explaining: requiredSections structure (audit vs session skill types), deprecatedPatterns array format ({pattern: {source, flags}, message}), topicAliases mapping (canonical topic to related terms). Explain regex descriptor format and how aliases improve search accuracy.","id":"process::scripts/config/skill-config.json::no-schema-docs"}
{"category":"process","title":"Docs: Excessively long file aggregate-audit-findings.js (1934 lines)","fingerprint":"process::scripts/aggregate-audit-findings.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/aggregate-audit-findings.js:1"],"why_it_matters":"File is 1934 lines (nearly 4x recommended 500-line limit). Large files are harder to understand, test, and maintain. Indicates multiple responsibilities that should be separated.","suggested_fix":"Split into modules: parsers.js (parseCanonItems, parseSingleSessionAudit, parseRoadmapItems, parseTechDebtItems, parseBacklogIssues), deduplication.js (deduplication logic, similarity scoring, shouldMerge), formatters.js (markdown formatting, JSONL output), and main orchestrator (aggregate-audit-findings.js). Move configuration constants to config files.","acceptance_tests":["No single file exceeds 500 lines","Each module has single responsibility","Modules have clear interfaces","Test coverage maintained after split"],"file":"scripts/aggregate-audit-findings.js","line":1,"description":"File is 1934 lines (nearly 4x recommended 500-line limit). Large files are harder to understand, test, and maintain. Indicates multiple responsibilities that should be separated.","recommendation":"Split into modules: parsers.js (parseCanonItems, parseSingleSessionAudit, parseRoadmapItems, parseTechDebtItems, parseBacklogIssues), deduplication.js (deduplication logic, similarity scoring, shouldMerge), formatters.js (markdown formatting, JSONL output), and main orchestrator (aggregate-audit-findings.js). Move configuration constants to config files.","id":"process::scripts/aggregate-audit-findings.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file analyze-learning-effectiveness.js (1271 lines)","fingerprint":"process::scripts/analyze-learning-effectiveness.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/analyze-learning-effectiveness.js:1"],"why_it_matters":"File is 1271 lines (2.5x recommended limit). Contains analysis, reporting, and interactive CLI logic that should be separated for maintainability and testability.","suggested_fix":"Split into modules: pattern-analyzer.js (pattern detection and recurrence analysis), metrics-calculator.js (effectiveness scoring and statistics), report-generator.js (dashboard and detailed reports), cli-interface.js (readline prompts and interactive mode), and main orchestrator (analyze-learning-effectiveness.js).","acceptance_tests":["No single file exceeds 500 lines","Analysis logic separated from presentation","CLI interface is modular","Each module is independently testable"],"file":"scripts/analyze-learning-effectiveness.js","line":1,"description":"File is 1271 lines (2.5x recommended limit). Contains analysis, reporting, and interactive CLI logic that should be separated for maintainability and testability.","recommendation":"Split into modules: pattern-analyzer.js (pattern detection and recurrence analysis), metrics-calculator.js (effectiveness scoring and statistics), report-generator.js (dashboard and detailed reports), cli-interface.js (readline prompts and interactive mode), and main orchestrator (analyze-learning-effectiveness.js).","id":"process::scripts/analyze-learning-effectiveness.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file validate-audit-integration.js (1242 lines)","fingerprint":"process::scripts/audit/validate-audit-integration.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/audit/validate-audit-integration.js:1"],"why_it_matters":"File is 1242 lines (2.5x recommended limit). Validation script contains schema validation, false positive checking, evidence validation, and reporting that should be modular.","suggested_fix":"Split into modules: schema-validator.js (JSONL schema validation), false-positive-checker.js (FP database lookup), evidence-validator.js (file:line verification), tool-integrator.js (npm audit, ESLint, patterns:check), report-generator.js (validation reports), and main orchestrator (validate-audit-integration.js).","acceptance_tests":["No single file exceeds 500 lines","Validation concerns are separated","Each validator is independently usable","Test coverage is maintained"],"file":"scripts/audit/validate-audit-integration.js","line":1,"description":"File is 1242 lines (2.5x recommended limit). Validation script contains schema validation, false positive checking, evidence validation, and reporting that should be modular.","recommendation":"Split into modules: schema-validator.js (JSONL schema validation), false-positive-checker.js (FP database lookup), evidence-validator.js (file:line verification), tool-integrator.js (npm audit, ESLint, patterns:check), report-generator.js (validation reports), and main orchestrator (validate-audit-integration.js).","id":"process::scripts/audit/validate-audit-integration.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file check-review-needed.js (1056 lines)","fingerprint":"process::scripts/check-review-needed.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/check-review-needed.js:1"],"why_it_matters":"File is 1056 lines (2x recommended limit). Contains git analysis, threshold checking, SonarCloud integration, and reporting that should be modular.","suggested_fix":"Split into modules: git-analyzer.js (commit counting, file change detection), threshold-checker.js (per-category threshold logic), sonarcloud-client.js (API integration), category-rules.js (category-specific thresholds), report-generator.js (human and JSON output), and main orchestrator (check-review-needed.js).","acceptance_tests":["No single file exceeds 500 lines","Git logic separated from threshold checking","SonarCloud client is reusable module","Each category has isolated logic"],"file":"scripts/check-review-needed.js","line":1,"description":"File is 1056 lines (2x recommended limit). Contains git analysis, threshold checking, SonarCloud integration, and reporting that should be modular.","recommendation":"Split into modules: git-analyzer.js (commit counting, file change detection), threshold-checker.js (per-category threshold logic), sonarcloud-client.js (API integration), category-rules.js (category-specific thresholds), report-generator.js (human and JSON output), and main orchestrator (check-review-needed.js).","id":"process::scripts/check-review-needed.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file multi-ai/normalize-format.js (1014 lines)","fingerprint":"process::scripts/multi-ai/normalize-format.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/multi-ai/normalize-format.js:1"],"why_it_matters":"File is 1014 lines (2x recommended limit). Format normalization script handles multiple input formats (JSONL, JSON, markdown, plain text) that should be separate parser modules.","suggested_fix":"Split into modules: jsonl-parser.js, json-parser.js, markdown-parser.js (tables, lists, headers), text-parser.js, format-detector.js (auto-detection), schema-normalizer.js (output normalization), and main orchestrator (normalize-format.js). Each parser should export parse() function.","acceptance_tests":["No single file exceeds 500 lines","Each input format has dedicated parser","Format detection is isolated","Parsers are independently testable"],"file":"scripts/multi-ai/normalize-format.js","line":1,"description":"File is 1014 lines (2x recommended limit). Format normalization script handles multiple input formats (JSONL, JSON, markdown, plain text) that should be separate parser modules.","recommendation":"Split into modules: jsonl-parser.js, json-parser.js, markdown-parser.js (tables, lists, headers), text-parser.js, format-detector.js (auto-detection), schema-normalizer.js (output normalization), and main orchestrator (normalize-format.js). Each parser should export parse() function.","id":"process::scripts/multi-ai/normalize-format.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file validate-audit.js (980 lines)","fingerprint":"process::scripts/validate-audit.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/validate-audit.js:1"],"why_it_matters":"File is 980 lines (nearly 2x recommended limit). Post-audit validation contains multiple validation types, external tool integration, and confidence scoring that should be modular.","suggested_fix":"Split into modules: false-positive-validator.js (FP database checking), evidence-validator.js (file:line verification, code snippet validation), tool-validator.js (npm audit, ESLint, patterns:check cross-reference), confidence-scorer.js (confidence level validation), duplicate-detector.js, and main orchestrator (validate-audit.js).","acceptance_tests":["No single file exceeds 500 lines","Each validation type is a module","Tool integrations are reusable","Confidence scoring is isolated"],"file":"scripts/validate-audit.js","line":1,"description":"File is 980 lines (nearly 2x recommended limit). Post-audit validation contains multiple validation types, external tool integration, and confidence scoring that should be modular.","recommendation":"Split into modules: false-positive-validator.js (FP database checking), evidence-validator.js (file:line verification, code snippet validation), tool-validator.js (npm audit, ESLint, patterns:check cross-reference), confidence-scorer.js (confidence level validation), duplicate-detector.js, and main orchestrator (validate-audit.js).","id":"process::scripts/validate-audit.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file generate-documentation-index.js (980 lines)","fingerprint":"process::scripts/generate-documentation-index.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/generate-documentation-index.js:1"],"why_it_matters":"File is 980 lines (nearly 2x recommended limit). Documentation indexer combines file traversal, markdown parsing, hierarchy building, and output formatting that should be separated.","suggested_fix":"Split into modules: file-traverser.js (recursive directory traversal with excludes), markdown-parser.js (frontmatter extraction, header parsing), hierarchy-builder.js (tree structure generation), output-formatter.js (JSON and markdown formatting), and main orchestrator (generate-documentation-index.js).","acceptance_tests":["No single file exceeds 500 lines","File system logic separated from parsing","Hierarchy building is isolated","Output formatting is modular"],"file":"scripts/generate-documentation-index.js","line":1,"description":"File is 980 lines (nearly 2x recommended limit). Documentation indexer combines file traversal, markdown parsing, hierarchy building, and output formatting that should be separated.","recommendation":"Split into modules: file-traverser.js (recursive directory traversal with excludes), markdown-parser.js (frontmatter extraction, header parsing), hierarchy-builder.js (tree structure generation), output-formatter.js (JSON and markdown formatting), and main orchestrator (generate-documentation-index.js).","id":"process::scripts/generate-documentation-index.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file check-docs-light.js (866 lines)","fingerprint":"process::scripts/check-docs-light.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/check-docs-light.js:1"],"why_it_matters":"File is 866 lines (1.7x recommended limit). Documentation checker combines multiple validation types (frontmatter, links, structure, coverage) that should be separate modules.","suggested_fix":"Split into modules: frontmatter-validator.js, link-checker.js (internal and external), structure-validator.js (heading hierarchy, required sections), coverage-analyzer.js (undocumented files), report-generator.js, and main orchestrator (check-docs-light.js).","acceptance_tests":["No single file exceeds 500 lines","Each validation type is a module","Validators are independently usable","Report generation is isolated"],"file":"scripts/check-docs-light.js","line":1,"description":"File is 866 lines (1.7x recommended limit). Documentation checker combines multiple validation types (frontmatter, links, structure, coverage) that should be separate modules.","recommendation":"Split into modules: frontmatter-validator.js, link-checker.js (internal and external), structure-validator.js (heading hierarchy, required sections), coverage-analyzer.js (undocumented files), report-generator.js, and main orchestrator (check-docs-light.js).","id":"process::scripts/check-docs-light.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file check-pattern-compliance.js (834 lines)","fingerprint":"process::scripts/check-pattern-compliance.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/check-pattern-compliance.js:1"],"why_it_matters":"File is 834 lines (1.7x recommended limit). Pattern compliance checker combines pattern loading, file scanning, pattern matching, and reporting that should be modular.","suggested_fix":"Split into modules: pattern-loader.js (load from ai-patterns.json), file-scanner.js (recursive scanning with excludes), pattern-matcher.js (regex matching with performance limits), violation-reporter.js (format violations for output), and main orchestrator (check-pattern-compliance.js).","acceptance_tests":["No single file exceeds 500 lines","Pattern matching is isolated","File scanning is reusable","Report formatting is modular"],"file":"scripts/check-pattern-compliance.js","line":1,"description":"File is 834 lines (1.7x recommended limit). Pattern compliance checker combines pattern loading, file scanning, pattern matching, and reporting that should be modular.","recommendation":"Split into modules: pattern-loader.js (load from ai-patterns.json), file-scanner.js (recursive scanning with excludes), pattern-matcher.js (regex matching with performance limits), violation-reporter.js (format violations for output), and main orchestrator (check-pattern-compliance.js).","id":"process::scripts/check-pattern-compliance.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file debt/sync-sonarcloud.js (770 lines)","fingerprint":"process::scripts/debt/sync-sonarcloud.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/debt/sync-sonarcloud.js:1"],"why_it_matters":"File is 770 lines (1.5x recommended limit). SonarCloud sync combines API client logic, data transformation, deduplication, and persistence that should be separated.","suggested_fix":"Split into modules: sonarcloud-client.js (API requests, authentication, pagination), issue-transformer.js (SonarCloud to internal schema mapping), deduplicator.js (match existing debt items), persistence.js (JSONL writing), and main orchestrator (sync-sonarcloud.js).","acceptance_tests":["No single file exceeds 500 lines","API client is reusable module","Data transformation is isolated","Deduplication logic is testable"],"file":"scripts/debt/sync-sonarcloud.js","line":1,"description":"File is 770 lines (1.5x recommended limit). SonarCloud sync combines API client logic, data transformation, deduplication, and persistence that should be separated.","recommendation":"Split into modules: sonarcloud-client.js (API requests, authentication, pagination), issue-transformer.js (SonarCloud to internal schema mapping), deduplicator.js (match existing debt items), persistence.js (JSONL writing), and main orchestrator (sync-sonarcloud.js).","id":"process::scripts/debt/sync-sonarcloud.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file audit/transform-jsonl-schema.js (761 lines)","fingerprint":"process::scripts/audit/transform-jsonl-schema.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/audit/transform-jsonl-schema.js:1"],"why_it_matters":"File is 761 lines (1.5x recommended limit). Schema transformation combines parsing, validation, field mapping, and output generation that should be modular.","suggested_fix":"Split into modules: jsonl-parser.js (parse JSONL with error handling), schema-mapper.js (field mapping rules), field-validator.js (validate transformed output), output-generator.js (write transformed JSONL), and main orchestrator (transform-jsonl-schema.js).","acceptance_tests":["No single file exceeds 500 lines","Parsing separated from transformation","Validation is independent","Field mapping is data-driven"],"file":"scripts/audit/transform-jsonl-schema.js","line":1,"description":"File is 761 lines (1.5x recommended limit). Schema transformation combines parsing, validation, field mapping, and output generation that should be modular.","recommendation":"Split into modules: jsonl-parser.js (parse JSONL with error handling), schema-mapper.js (field mapping rules), field-validator.js (validate transformed output), output-generator.js (write transformed JSONL), and main orchestrator (transform-jsonl-schema.js).","id":"process::scripts/audit/transform-jsonl-schema.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file run-consolidation.js (743 lines)","fingerprint":"process::scripts/run-consolidation.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/run-consolidation.js:1"],"why_it_matters":"File is 743 lines (1.5x recommended limit). Consolidation orchestrator combines workflow logic, file I/O, validation, and reporting that should be separated.","suggested_fix":"Split into modules: consolidation-workflow.js (step orchestration), file-manager.js (read/write JSONL files), validation-runner.js (schema validation), report-generator.js (consolidation reports), and main entry point (run-consolidation.js).","acceptance_tests":["No single file exceeds 500 lines","Workflow logic is declarative","File operations are centralized","Validation is reusable"],"file":"scripts/run-consolidation.js","line":1,"description":"File is 743 lines (1.5x recommended limit). Consolidation orchestrator combines workflow logic, file I/O, validation, and reporting that should be separated.","recommendation":"Split into modules: consolidation-workflow.js (step orchestration), file-manager.js (read/write JSONL files), validation-runner.js (schema validation), report-generator.js (consolidation reports), and main entry point (run-consolidation.js).","id":"process::scripts/run-consolidation.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file multi-ai/unify-findings.js (716 lines)","fingerprint":"process::scripts/multi-ai/unify-findings.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/multi-ai/unify-findings.js:1"],"why_it_matters":"File is 716 lines (1.4x recommended limit). Finding unification combines parsing, similarity detection, merging, and conflict resolution that should be modular.","suggested_fix":"Split into modules: finding-parser.js (parse multiple formats), similarity-detector.js (detect duplicate findings), finding-merger.js (merge logic), conflict-resolver.js (handle disagreements), and main orchestrator (unify-findings.js).","acceptance_tests":["No single file exceeds 500 lines","Similarity detection is isolated","Merging logic is testable","Conflict resolution is separate"],"file":"scripts/multi-ai/unify-findings.js","line":1,"description":"File is 716 lines (1.4x recommended limit). Finding unification combines parsing, similarity detection, merging, and conflict resolution that should be modular.","recommendation":"Split into modules: finding-parser.js (parse multiple formats), similarity-detector.js (detect duplicate findings), finding-merger.js (merge logic), conflict-resolver.js (handle disagreements), and main orchestrator (unify-findings.js).","id":"process::scripts/multi-ai/unify-findings.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file archive-doc.js (712 lines)","fingerprint":"process::scripts/archive-doc.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/archive-doc.js:1"],"why_it_matters":"File is 712 lines (1.4x recommended limit). Document archival combines file moving, content transformation, index updating, and git operations that should be separated.","suggested_fix":"Split into modules: file-archiver.js (move files to archive), content-transformer.js (add archive metadata), index-updater.js (update documentation indexes), git-operations.js (stage and commit), and main orchestrator (archive-doc.js).","acceptance_tests":["No single file exceeds 500 lines","File operations are isolated","Content transformation is modular","Git operations are reusable"],"file":"scripts/archive-doc.js","line":1,"description":"File is 712 lines (1.4x recommended limit). Document archival combines file moving, content transformation, index updating, and git operations that should be separated.","recommendation":"Split into modules: file-archiver.js (move files to archive), content-transformer.js (add archive metadata), index-updater.js (update documentation indexes), git-operations.js (stage and commit), and main orchestrator (archive-doc.js).","id":"process::scripts/archive-doc.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file check-external-links.js (701 lines)","fingerprint":"process::scripts/check-external-links.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/check-external-links.js:1"],"why_it_matters":"File is 701 lines (1.4x recommended limit). Link checker combines link extraction, HTTP requests, caching, retry logic, and reporting that should be modular.","suggested_fix":"Split into modules: link-extractor.js (parse markdown for links), http-checker.js (validate URLs with retries), cache-manager.js (link validation cache), rate-limiter.js (throttle requests), report-generator.js, and main orchestrator (check-external-links.js).","acceptance_tests":["No single file exceeds 500 lines","Link extraction is reusable","HTTP logic handles all error cases","Cache management is isolated"],"file":"scripts/check-external-links.js","line":1,"description":"File is 701 lines (1.4x recommended limit). Link checker combines link extraction, HTTP requests, caching, retry logic, and reporting that should be modular.","recommendation":"Split into modules: link-extractor.js (parse markdown for links), http-checker.js (validate URLs with retries), cache-manager.js (link validation cache), rate-limiter.js (throttle requests), report-generator.js, and main orchestrator (check-external-links.js).","id":"process::scripts/check-external-links.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file phase-complete-check.js (690 lines)","fingerprint":"process::scripts/phase-complete-check.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/phase-complete-check.js:1"],"why_it_matters":"File is 690 lines (1.4x recommended limit). Phase completion checker combines requirement parsing, status checking, dependency validation, and reporting that should be separated.","suggested_fix":"Split into modules: requirement-parser.js (parse phase requirements), status-checker.js (check completion criteria), dependency-validator.js (validate phase dependencies), blocking-analyzer.js (identify blockers), report-generator.js, and main orchestrator (phase-complete-check.js).","acceptance_tests":["No single file exceeds 500 lines","Requirement parsing is data-driven","Status checking is isolated","Dependency logic is testable"],"file":"scripts/phase-complete-check.js","line":1,"description":"File is 690 lines (1.4x recommended limit). Phase completion checker combines requirement parsing, status checking, dependency validation, and reporting that should be separated.","recommendation":"Split into modules: requirement-parser.js (parse phase requirements), status-checker.js (check completion criteria), dependency-validator.js (validate phase dependencies), blocking-analyzer.js (identify blockers), report-generator.js, and main orchestrator (phase-complete-check.js).","id":"process::scripts/phase-complete-check.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file check-doc-placement.js (616 lines)","fingerprint":"process::scripts/check-doc-placement.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/check-doc-placement.js:1"],"why_it_matters":"File is 616 lines (1.2x recommended limit). Document placement checker combines rules loading, file scanning, rule evaluation, and reporting that should be modular.","suggested_fix":"Split into modules: placement-rules.js (load and parse placement rules), file-scanner.js (find documents), rule-evaluator.js (check documents against rules), violation-detector.js (identify misplaced docs), report-generator.js, and main orchestrator (check-doc-placement.js).","acceptance_tests":["No single file exceeds 500 lines","Rules are data-driven and loadable","File scanning is reusable","Rule evaluation is testable"],"file":"scripts/check-doc-placement.js","line":1,"description":"File is 616 lines (1.2x recommended limit). Document placement checker combines rules loading, file scanning, rule evaluation, and reporting that should be modular.","recommendation":"Split into modules: placement-rules.js (load and parse placement rules), file-scanner.js (find documents), rule-evaluator.js (check documents against rules), violation-detector.js (identify misplaced docs), report-generator.js, and main orchestrator (check-doc-placement.js).","id":"process::scripts/check-doc-placement.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file multi-ai/fix-schema.js (615 lines)","fingerprint":"process::scripts/multi-ai/fix-schema.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/multi-ai/fix-schema.js:1"],"why_it_matters":"File is 615 lines (1.2x recommended limit). Schema fixing combines validation, field correction, migration, and output that should be separated.","suggested_fix":"Split into modules: schema-validator.js (detect schema issues), field-fixer.js (correction rules for each field type), migration-rules.js (schema version migrations), output-writer.js (write corrected JSONL), and main orchestrator (fix-schema.js).","acceptance_tests":["No single file exceeds 500 lines","Validation separated from correction","Correction rules are data-driven","Migration logic is versioned"],"file":"scripts/multi-ai/fix-schema.js","line":1,"description":"File is 615 lines (1.2x recommended limit). Schema fixing combines validation, field correction, migration, and output that should be separated.","recommendation":"Split into modules: schema-validator.js (detect schema issues), field-fixer.js (correction rules for each field type), migration-rules.js (schema version migrations), output-writer.js (write corrected JSONL), and main orchestrator (fix-schema.js).","id":"process::scripts/multi-ai/fix-schema.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file multi-ai/aggregate-category.js (603 lines)","fingerprint":"process::scripts/multi-ai/aggregate-category.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/multi-ai/aggregate-category.js:1"],"why_it_matters":"File is 603 lines (1.2x recommended limit). Category aggregation combines parsing, grouping, statistics, and output formatting that should be modular.","suggested_fix":"Split into modules: category-parser.js (parse findings by category), grouping-engine.js (group related findings), statistics-calculator.js (compute category metrics), output-formatter.js (generate reports), and main orchestrator (aggregate-category.js).","acceptance_tests":["No single file exceeds 500 lines","Parsing logic is isolated","Grouping algorithm is testable","Statistics calculation is reusable"],"file":"scripts/multi-ai/aggregate-category.js","line":1,"description":"File is 603 lines (1.2x recommended limit). Category aggregation combines parsing, grouping, statistics, and output formatting that should be modular.","recommendation":"Split into modules: category-parser.js (parse findings by category), grouping-engine.js (group related findings), statistics-calculator.js (compute category metrics), output-formatter.js (generate reports), and main orchestrator (aggregate-category.js).","id":"process::scripts/multi-ai/aggregate-category.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file verify-sonar-phase.js (597 lines)","fingerprint":"process::scripts/verify-sonar-phase.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/verify-sonar-phase.js:1"],"why_it_matters":"File is 597 lines (1.2x recommended limit). SonarCloud verification combines API calls, phase validation, issue tracking, and reporting that should be separated.","suggested_fix":"Split into modules: sonarcloud-client.js (API integration), phase-requirements.js (load phase criteria), issue-tracker.js (track verification issues), validation-engine.js (check requirements), report-generator.js, and main orchestrator (verify-sonar-phase.js).","acceptance_tests":["No single file exceeds 500 lines","API client is reusable","Phase requirements are data-driven","Validation logic is isolated"],"file":"scripts/verify-sonar-phase.js","line":1,"description":"File is 597 lines (1.2x recommended limit). SonarCloud verification combines API calls, phase validation, issue tracking, and reporting that should be separated.","recommendation":"Split into modules: sonarcloud-client.js (API integration), phase-requirements.js (load phase criteria), issue-tracker.js (track verification issues), validation-engine.js (check requirements), report-generator.js, and main orchestrator (verify-sonar-phase.js).","id":"process::scripts/verify-sonar-phase.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file update-readme-status.js (597 lines)","fingerprint":"process::scripts/update-readme-status.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/update-readme-status.js:1"],"why_it_matters":"File is 597 lines (1.2x recommended limit). README updater combines status collection, badge generation, markdown formatting, and file writing that should be modular.","suggested_fix":"Split into modules: status-collector.js (gather status from multiple sources), badge-generator.js (create status badges), markdown-formatter.js (format status sections), file-updater.js (in-place README updates), and main orchestrator (update-readme-status.js).","acceptance_tests":["No single file exceeds 500 lines","Status collection is centralized","Badge generation is templated","Markdown updates are atomic"],"file":"scripts/update-readme-status.js","line":1,"description":"File is 597 lines (1.2x recommended limit). README updater combines status collection, badge generation, markdown formatting, and file writing that should be modular.","recommendation":"Split into modules: status-collector.js (gather status from multiple sources), badge-generator.js (create status badges), markdown-formatter.js (format status sections), file-updater.js (in-place README updates), and main orchestrator (update-readme-status.js).","id":"process::scripts/update-readme-status.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file debt/intake-audit.js (586 lines)","fingerprint":"process::scripts/debt/intake-audit.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/debt/intake-audit.js:1"],"why_it_matters":"File is 586 lines (1.2x recommended limit). Audit intake combines parsing, validation, ID generation, deduplication, and persistence that should be separated.","suggested_fix":"Split into modules: audit-parser.js (parse audit JSONL), item-validator.js (schema validation), id-generator.js (generate debt IDs), deduplicator.js (detect duplicates), persister.js (write to MASTER_DEBT.jsonl), and main orchestrator (intake-audit.js).","acceptance_tests":["No single file exceeds 500 lines","Parsing separated from validation","ID generation is deterministic","Deduplication logic is testable"],"file":"scripts/debt/intake-audit.js","line":1,"description":"File is 586 lines (1.2x recommended limit). Audit intake combines parsing, validation, ID generation, deduplication, and persistence that should be separated.","recommendation":"Split into modules: audit-parser.js (parse audit JSONL), item-validator.js (schema validation), id-generator.js (generate debt IDs), deduplicator.js (detect duplicates), persister.js (write to MASTER_DEBT.jsonl), and main orchestrator (intake-audit.js).","id":"process::scripts/debt/intake-audit.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file generate-detailed-sonar-report.js (561 lines)","fingerprint":"process::scripts/generate-detailed-sonar-report.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/generate-detailed-sonar-report.js:1"],"why_it_matters":"File is 561 lines (1.1x recommended limit). SonarCloud report generator combines API calls, data aggregation, HTML generation, and file writing that should be modular.","suggested_fix":"Split into modules: sonarcloud-client.js (API integration), data-aggregator.js (aggregate issues by type/severity), html-generator.js (create HTML report), markdown-generator.js (create MD report), and main orchestrator (generate-detailed-sonar-report.js).","acceptance_tests":["No single file exceeds 500 lines","API client is reusable","Data aggregation is isolated","Report formats are pluggable"],"file":"scripts/generate-detailed-sonar-report.js","line":1,"description":"File is 561 lines (1.1x recommended limit). SonarCloud report generator combines API calls, data aggregation, HTML generation, and file writing that should be modular.","recommendation":"Split into modules: sonarcloud-client.js (API integration), data-aggregator.js (aggregate issues by type/severity), html-generator.js (create HTML report), markdown-generator.js (create MD report), and main orchestrator (generate-detailed-sonar-report.js).","id":"process::scripts/generate-detailed-sonar-report.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file lib/ai-pattern-checks.js (554 lines)","fingerprint":"process::scripts/lib/ai-pattern-checks.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/lib/ai-pattern-checks.js:1"],"why_it_matters":"File is 554 lines (1.1x recommended limit). AI pattern library combines pattern loading, matching, reporting, and multiple specific pattern checkers that should be separated.","suggested_fix":"Split into modules: pattern-loader.js (load ai-patterns.json), pattern-matcher.js (core matching engine), pattern-checks/ directory with specific checkers (happy-path.js, trivial-assertions.js, todo-markers.js, etc.), and main exports (ai-pattern-checks.js).","acceptance_tests":["No single file exceeds 500 lines","Each pattern check is a module","Pattern matching is reusable","New patterns can be added easily"],"file":"scripts/lib/ai-pattern-checks.js","line":1,"description":"File is 554 lines (1.1x recommended limit). AI pattern library combines pattern loading, matching, reporting, and multiple specific pattern checkers that should be separated.","recommendation":"Split into modules: pattern-loader.js (load ai-patterns.json), pattern-matcher.js (core matching engine), pattern-checks/ directory with specific checkers (happy-path.js, trivial-assertions.js, todo-markers.js, etc.), and main exports (ai-pattern-checks.js).","id":"process::scripts/lib/ai-pattern-checks.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file migrate-existing-findings.js (541 lines)","fingerprint":"process::scripts/migrate-existing-findings.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/migrate-existing-findings.js:1"],"why_it_matters":"File is 541 lines (1.1x recommended limit). Migration script combines multiple source parsing, schema transformation, deduplication, and output that should be separated.","suggested_fix":"Split into modules: source-parsers.js (parse old formats), schema-transformer.js (old to new schema), deduplicator.js (detect duplicates across sources), output-writer.js (write migrated JSONL), and main orchestrator (migrate-existing-findings.js).","acceptance_tests":["No single file exceeds 500 lines","Each source parser is isolated","Schema transformation is declarative","Migration is idempotent"],"file":"scripts/migrate-existing-findings.js","line":1,"description":"File is 541 lines (1.1x recommended limit). Migration script combines multiple source parsing, schema transformation, deduplication, and output that should be separated.","recommendation":"Split into modules: source-parsers.js (parse old formats), schema-transformer.js (old to new schema), deduplicator.js (detect duplicates across sources), output-writer.js (write migrated JSONL), and main orchestrator (migrate-existing-findings.js).","id":"process::scripts/migrate-existing-findings.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file check-content-accuracy.js (516 lines)","fingerprint":"process::scripts/check-content-accuracy.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/check-content-accuracy.js:1"],"why_it_matters":"File is 516 lines (1.0x recommended limit). Content accuracy checker combines reference validation, fact checking, staleness detection, and reporting that should be modular.","suggested_fix":"Split into modules: reference-validator.js (check cross-references), fact-checker.js (validate claims), staleness-detector.js (identify outdated content), accuracy-scorer.js (compute accuracy metrics), report-generator.js, and main orchestrator (check-content-accuracy.js).","acceptance_tests":["No single file exceeds 500 lines","Each validation type is a module","Fact checking is data-driven","Staleness rules are configurable"],"file":"scripts/check-content-accuracy.js","line":1,"description":"File is 516 lines (1.0x recommended limit). Content accuracy checker combines reference validation, fact checking, staleness detection, and reporting that should be modular.","recommendation":"Split into modules: reference-validator.js (check cross-references), fact-checker.js (validate claims), staleness-detector.js (identify outdated content), accuracy-scorer.js (compute accuracy metrics), report-generator.js, and main orchestrator (check-content-accuracy.js).","id":"process::scripts/check-content-accuracy.js::excessive-length"}
{"category":"process","title":"Docs: Excessively long file sync-claude-settings.js (501 lines)","fingerprint":"process::scripts/sync-claude-settings.js::excessive-length","severity":"S3","effort":"E2","confidence":"HIGH","files":["scripts/sync-claude-settings.js:1"],"why_it_matters":"File is 501 lines (exactly at limit). Settings sync combines parsing, validation, merging, and persistence that should be separated for better maintainability.","suggested_fix":"Split into modules: settings-parser.js (parse .claude/ settings), settings-validator.js (validate schema), settings-merger.js (merge strategies), settings-writer.js (atomic writes), and main orchestrator (sync-claude-settings.js).","acceptance_tests":["No single file exceeds 500 lines","Parsing separated from validation","Merge strategies are pluggable","Settings writes are atomic"],"file":"scripts/sync-claude-settings.js","line":1,"description":"File is 501 lines (exactly at limit). Settings sync combines parsing, validation, merging, and persistence that should be separated for better maintainability.","recommendation":"Split into modules: settings-parser.js (parse .claude/ settings), settings-validator.js (validate schema), settings-merger.js (merge strategies), settings-writer.js (atomic writes), and main orchestrator (sync-claude-settings.js).","id":"process::scripts/sync-claude-settings.js::excessive-length"}
