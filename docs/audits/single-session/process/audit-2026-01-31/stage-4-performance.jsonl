{"id": "PERF-HOOK-001", "file": ".claude/hooks/session-start.js", "issue": "Synchronous child_process.execSync calls in sequence with no parallelization", "impact": "slow session startup (10-20s). Multiple npm install, build, and test commands run serially", "severity": "S2", "recommendation": "Use Promise.all() with execAsync to parallelize independent operations like npm ci for root and functions in parallel, and pattern-check with backlog-health check in parallel"}
{"id": "PERF-HOOK-002", "file": ".claude/hooks/session-start.js", "issue": "Reads and computes hashes of lockfiles synchronously on every startup - no cache between sessions", "impact": "slow session startup. fs.readFileSync on package-lock.json and functions/package-lock.json happens every startup", "severity": "S3", "recommendation": "Cache computed hashes in .env or filesystem across sessions. Compare only if hashes differ, skip re-read if within last 24 hours"}
{"id": "PERF-HOOK-003", "file": ".claude/hooks/large-context-warning.js", "issue": "Reads entire file into memory and splits on newlines for every Read/Glob operation - O(n) memory for file content", "impact": "slow on large files. Memory usage scales with file size (5000+ line files hit memory allocation cost)", "severity": "S3", "recommendation": "Use streaming or line-counting loop instead of split(). Count newlines with charCodeAt(i) === 10 without storing lines array"}
{"id": "PERF-HOOK-004", "file": ".claude/hooks/large-context-warning.js", "issue": "Reads JSON state file on every Read/Glob operation - multiple fs.readFileSync calls per session", "impact": "slow reads. State file accessed for every file read tracked", "severity": "S3", "recommendation": "Cache state in memory with periodic flush (every 10 files), only read on session-start and write once per 30 seconds"}
{"id": "PERF-HOOK-005", "file": ".claude/hooks/pattern-check.js", "issue": "Calls external spawnSync(node scripts/check-pattern-compliance.js) for every file - spawning child process overhead", "impact": "slow edits. 30-100ms per file edit just to spawn and run pattern checker", "severity": "S2", "recommendation": "Move pattern-check logic into hook (fs.readFileSync + regex checks) to avoid subprocess overhead. Use lazy evaluation - only check if file >100 lines (already implemented but subprocess overhead remains)"}
{"id": "PERF-HOOK-006", "file": ".claude/hooks/pattern-check.js", "issue": "Calls fs.realpathSync() to resolve real paths before pattern check - filesystem traversal overhead", "impact": "slow edits. realpathSync is blocking and resolves symlinks which is expensive on some filesystems", "severity": "S3", "recommendation": "Skip realpathSync for pattern check - use relative path directly. Containment validation is already done via path.relative() earlier in code"}
{"id": "PERF-HOOK-007", "file": ".claude/hooks/agent-trigger-enforcer.js", "issue": "Reads and writes JSON state file on every PostToolUse - frequent file I/O for tracking", "impact": "slow tool use. Adds 100ms+ per file modification to read state, update, write state", "severity": "S3", "recommendation": "Batch state updates. Only write state once per 10 files modified or once per minute, not on every invocation"}
{"id": "PERF-HOOK-008", "file": ".claude/hooks/component-size-check.js", "issue": "Reads entire component file into memory and splits on newline to count lines for every .tsx write", "impact": "slow component edits. Line counting via split() creates large intermediate array", "severity": "S2", "recommendation": "Use streaming line count or charCodeAt loop instead of split(). File read is unavoidable but processing can be optimized"}
{"id": "PERF-HOOK-009", "file": ".claude/hooks/firestore-write-block.js", "issue": "Uses regex exec() with /g flag in loop without proper lastIndex reset between iterations", "impact": "potential infinite loop or incorrect behavior. Multiple FIRESTORE_WRITE_PATTERNS.forEach with pattern.exec() during global flag processing", "severity": "S1", "recommendation": "Reset pattern.lastIndex = 0 before each exec() loop, or use String.match() instead of pattern.exec()"}
{"id": "PERF-HOOK-010", "file": ".claude/hooks/repository-pattern-check.js", "issue": "Iterates through component file line-by-line checking regex patterns for every Firestore method", "impact": "slow .tsx edits. O(n * m) complexity where n=lines, m=method patterns (14 methods). No early exit per line", "severity": "S2", "recommendation": "Use single regex with alternation: /\\b(collection|doc|query|getDocs|...)/g instead of looping methods. Early exit after first match per line"}
{"id": "PERF-HOOK-011", "file": ".claude/hooks/typescript-strict-check.js", "issue": "Iterates all lines checking 5 regex patterns for any type - O(n*5) complexity per .ts file", "impact": "slow typescript edits. Each line checked against 5 ANY_PATTERNS without combining patterns", "severity": "S2", "recommendation": "Combine into single regex: /:\\s*any(?:\\s*[;,)\\]}]|\\s*$)|(\\s+as\\s+any|<any>|any\\[\\])/  to avoid repeated iteration"}
{"id": "PERF-HOOK-012", "file": ".claude/hooks/auto-save-context.js", "issue": "Uses execSync child process operations for context preservation - blocking subprocess calls", "impact": "slow reads. Blocks for git operations when saving context to MCP memory", "severity": "S3", "recommendation": "Make context save async via Promise.all() with execFile. Defer to next idle moment, don't block user operations"}
{"id": "PERF-HOOK-013", "file": ".claude/hooks/check-remote-session-context.js", "issue": "Runs multiple git commands via execFileSync sequentially - blocking subprocess calls in series", "impact": "slow session start. git fetch, git branch -r, git show all block in sequence", "severity": "S2", "recommendation": "Parallelize git commands: git fetch + git branch -r in parallel, then sequential git show only if needed. Add timeout <30s total"}
{"id": "PERF-HOOK-014", "file": ".claude/hooks/session-start.js", "issue": "execSync commands lack uniform timeout settings - some commands may hang indefinitely", "impact": "slow session startup. npm install, build, test commands can hang if subprocess has issues", "severity": "S2", "recommendation": "Add timeout parameter to all execSync calls. Default 120s for install/build, 30s for checks. Kill process if timeout exceeded"}
{"id": "PERF-HOOK-015", "file": ".claude/hooks/analyze-user-request.js", "issue": "Loops through 7 priority categories with word matching - no early return on first match", "impact": "slow on user prompts. Continues checking all patterns even after security pattern matches (should exit early)", "severity": "S3", "recommendation": "Add early return in priority order. Return immediately after finding first match in security check instead of continuing to lower priorities"}
{"id": "PERF-HOOK-016", "file": ".claude/hooks/alerts-reminder.js", "issue": "Reads 5 separate JSON files on every UserPromptSubmit (PostToolUse) - multiple synchronous fs.readFileSync calls", "impact": "slow prompt submission. Adds 50-100ms per user message just to read state files", "severity": "S2", "recommendation": "Cache alerts and context in memory. Only refresh from disk on first read and every 30 seconds, not on every prompt"}
{"id": "PERF-HOOK-017", "file": ".claude/hooks/decision-save-prompt.js", "issue": "Loops through SIGNIFICANT_KEYWORDS (18 items) for every question with .some() - no string index optimization", "impact": "moderate on decisions. O(n*m) where n=questions, m=keywords. No compiled pattern or trie structure", "severity": "S3", "recommendation": "Compile keyword set into single regex /\\b(architecture|design|...)/i for one-pass matching instead of .some() loop"}
{"id": "PERF-HOOK-018", "file": ".claude/hooks/plan-mode-suggestion.js", "issue": "Checks 7 implementation keywords and 14 complexity indicators separately - duplicated pattern compilation", "impact": "moderate on long prompts (>50 words). Two separate filter operations on keyword/pattern arrays", "severity": "S3", "recommendation": "Combine keyword checks into single compiled regex. Use && operator for short-circuit evaluation instead of separate checks"}
{"id": "PERF-HOOK-019", "file": ".claude/hooks/stop-serena-dashboard.js", "issue": "Spawns PowerShell (Windows) or execFileSync(lsof) for process lookup - heavyweight subprocess overhead", "impact": "slow session startup on Windows. PowerShell startup time ~500ms, execFileSync lsof ~100ms on Unix", "severity": "S2", "recommendation": "Use native Node.js library like pidof-node or process-list. Cache recent PIDs to avoid repeated lookups within 30s"}
{"id": "PERF-HOOK-020", "file": ".claude/hooks/stop-serena-dashboard.js", "issue": "Polls process existence in 250ms intervals with Atomics.wait - inefficient polling loop for 5s timeout", "impact": "slow process termination. 5s timeout with 250ms intervals = 20 iterations with blocking wait", "severity": "S3", "recommendation": "Use platform-specific event APIs: inotify on Linux, FSEvents on macOS, WaitForSingleObject on Windows. Or reduce poll interval to 100ms max"}
{"id": "PERF-HOOK-021", "file": ".claude/hooks/audit-s0s1-validator.js", "issue": "Parses JSONL line-by-line without streaming - loads entire file into memory via split() then parses each line", "impact": "slow on large audit files. Memory scales with file size (10MB file = significant memory overhead)", "severity": "S2", "recommendation": "Stream JSONL parsing using readline module for line-by-line reading without loading entire file into memory"}
{"id": "PERF-HOOK-022", "file": ".claude/hooks/audit-s0s1-validator.js", "issue": "Validates against 7+ Set collections and 2+ regex patterns for every S0/S1 finding - repeated validation checks", "impact": "moderate on large audits. O(n*m) validation where n=findings, m=validation types. No caching", "severity": "S3", "recommendation": "Compile validation into single pass with combined logic. Cache compiled patterns outside finding loop for reuse"}
{"id": "PERF-HOOK-023", "file": ".claude/hooks/track-agent-invocation.js", "issue": "Uses atomic write pattern with temp files (write to .tmp then rename) - doubles filesystem operations", "impact": "slow agent tracking. Adds 2 syscalls (write + rename) for every agent invocation to ensure atomicity", "severity": "S3", "recommendation": "Use writeFile with atomic flag if available. For this low-risk state, skip atomic write or cache in memory with periodic flush"}
{"id": "PERF-HOOK-024", "file": ".claude/hooks/session-start.js", "issue": "Runs console.log for every build step sequentially - unbuffered output causes synchronization overhead", "impact": "slow session startup visually. Console.log formatting and synchronization overhead compounds over many steps", "severity": "S3", "recommendation": "Buffer output and log once after all steps complete, or use single progress indicator instead of per-step console.log"}
{"id": "PERF-HOOK-025", "file": ".claude/hooks/check-mcp-servers.js", "issue": "Reads and parses JSON file on every SessionStart - no caching across sessions", "impact": "moderate on session startup. MCP server list parsed every time even if .mcp.json unchanged", "severity": "S3", "recommendation": "Cache server names in memory per session. Only refresh if .mcp.json mtime newer than cached version"}
{"id": "PERF-CI-001", "file": ".github/workflows/ci.yml", "issue": "Missing Next.js build cache in build job", "impact": "slow CI - build job recomputes from scratch every run (est. 60-90 seconds per build)", "severity": "S2", "recommendation": "Add 'actions/cache@v4' with key for .next directory. Cache key: 'nextjs-build-${{ hashFiles('**/package-lock.json') }}-${{ github.ref }}'. Restore keys: ['nextjs-build-', 'nextjs-']. Paths: '.next/cache'"}
{"id": "PERF-CI-002", "file": ".github/workflows/ci.yml", "issue": "npm install dependencies cached but not TypeScript compilation cache", "impact": "slow CI - tsc --noEmit recompiles all files (est. 30-45 seconds)", "severity": "S3", "recommendation": "Add cache for tsc output. Use actions/cache with key 'tsc-cache-${{ hashFiles('**/tsconfig.json', '**/package-lock.json') }}' and restore-keys pattern. Preserve tsconfig.tsbuildinfo if present."}
{"id": "PERF-CI-003", "file": ".github/workflows/ci.yml", "issue": "Test build and coverage recompile TypeScript each run - no artifact reuse between test:build and test:coverage", "impact": "wasted resources - tsc runs twice for test compilation (est. 30-45 seconds duplicated)", "severity": "S2", "recommendation": "Refactor test:build into separate 'Build Tests' step that caches dist-tests/ output. Share compiled artifacts between test and test:coverage steps. Use actions/cache or upload-artifact for dist-tests/."}
{"id": "PERF-CI-004", "file": ".github/workflows/ci.yml", "issue": "Build job depends on lint-typecheck-test (sequential), but could run in parallel on different runners", "impact": "slow CI - critical path extends by full lint time (est. 2-3 minutes added latency)", "severity": "S2", "recommendation": "Remove 'needs: lint-typecheck-test' from build job. Run both jobs in parallel. Add separate 'lint-gates' job that blocks on 'build' completion to ensure quality gates pass before merge."}
{"id": "PERF-CI-005", "file": ".github/workflows/ci.yml", "issue": "Coverage artifact retention set to 14 days (default), but never used in CI", "impact": "wasted resources - coverage artifacts consume storage for 2 weeks without value", "severity": "S3", "recommendation": "Reduce retention-days to 1 (or set on-demand). Or integrate into SonarCloud upload step to automatically delete local after upload. Consider generating HTML reports and uploading to GitHub Pages instead."}
{"id": "PERF-CI-006", "file": ".github/workflows/ci.yml", "issue": "Pattern compliance check runs changed-files action, but requires full git checkout - no shallow clone", "impact": "slow CI - unnecessary bandwidth for full history (est. 10-15 seconds saved)", "severity": "S3", "recommendation": "Add 'fetch-depth: 0' only for 'push to main' case. For PR, use shallow clone (fetch-depth: 1) and get changed-files from github.event.pull_request context instead of git diff."}
{"id": "PERF-CI-007", "file": ".github/workflows/deploy-firebase.yml", "issue": "Next.js build lacks persistent .next cache - rebuilds on every deployment", "impact": "slow CI - deployment adds 60-90 seconds for redundant build (est. 5-7 minutes per deploy)", "severity": "S2", "recommendation": "Add actions/cache@v4 for .next/ before 'Build Next.js app' step. Use same cache key strategy as ci.yml build job. Also enable incremental builds with 'NEXT_INCREMENTAL_BUILD=1' env var."}
{"id": "PERF-CI-008", "file": ".github/workflows/deploy-firebase.yml", "issue": "Functions build recompiles TypeScript from scratch - no tsc cache", "impact": "slow CI - functions tsc compilation (est. 15-25 seconds per deploy)", "severity": "S3", "recommendation": "Add cache for functions/lib/ (compiled output). Cache key: 'tsc-functions-${{ hashFiles('functions/tsconfig.json', 'functions/package-lock.json') }}'. Also add '--incremental' flag to functions/tsconfig.json."}
{"id": "PERF-CI-009", "file": ".github/workflows/deploy-firebase.yml", "issue": "Two separate package-lock.json caches (root + functions/) but resolved after root install completes", "impact": "wasted resources - sequential npm ci (can parallelize)", "severity": "S2", "recommendation": "Install root and functions dependencies in parallel using 'npm install' in background job or split into separate jobs with '&'. Or use monorepo tool (npm workspaces) to install both with single 'npm ci' pass."}
{"id": "PERF-CI-010", "file": ".github/workflows/docs-lint.yml", "issue": "No early exit for files with no markdown changes - still runs changed-files action", "impact": "slow CI - unnecessary git operations (est. 5-10 seconds)", "severity": "S3", "recommendation": "Add early 'Check if markdown files changed' step that exits with 0 immediately if no *.md files changed. Use 'if: steps.check.outputs.has_md_changes != 'true'' for later steps."}
{"id": "PERF-CI-011", "file": ".github/workflows/review-check.yml", "issue": "Full npm ci required just for Node.js environment to run single script - no caching for this workflow", "impact": "wasted resources - installing all deps (120+ packages) takes est. 30-40 seconds for lightweight script", "severity": "S2", "recommendation": "Add 'cache: npm' to setup-node step. Consider splitting scripts to run without full node_modules (use tsx/node directly on single file). Or cache node_modules explicitly with actions/cache."}
{"id": "PERF-CI-012", "file": ".github/workflows/auto-label-review-tier.yml", "issue": "No cache: npm in setup-node, requires full npm ci even though script is lightweight", "impact": "wasted resources - node_modules install takes est. 30-40 seconds for simple file pattern matching", "severity": "S2", "recommendation": "Add 'cache: npm' to setup-node step. Script could run without deps - consider reducing scope or using shell-native tools."}
{"id": "PERF-CI-013", "file": ".github/workflows/sonarcloud.yml", "issue": "No fail-fast or timeout for SonarCloud analysis - can hang or run indefinitely if token fails", "impact": "slow CI - SonarCloud can block PR for hours if service is slow (est. 15-30 min typical)", "severity": "S2", "recommendation": "Add 'timeout-minutes: 30' to job. Add explicit error handling and retry logic. Set SONARCLOUD_TIMEOUT env var if supported. Consider skipping for non-main branches in fork PRs (already done but no timeout)."}
{"id": "PERF-CI-014", "file": ".github/workflows/backlog-enforcement.yml", "issue": "Backlog check job runs without npm cache even though it uses npm ci and node script", "impact": "wasted resources - est. 30-40 seconds per backlog check for install + script", "severity": "S2", "recommendation": "Add 'cache: npm' to setup-node step. Security patterns job also missing cache. Both workflows should use '--prefer-offline --no-audit --no-fund' flags (already in backlog job) but need cache enabled."}
{"id": "PERF-SCRIPT-001", "file": "scripts/debt/dedup-multi-pass.js", "lines": "302-410", "issue": "O(n²) Nested Loops in Deduplication - All four deduplication passes use nested loops comparing every item with every other item", "impact": "slow execution - Each pass runs O(n²) comparisons. With 1000+ items, creates 500k+ comparisons", "severity": "S2", "recommendation": "Pre-bucket items by file/category before comparing to reduce comparison set size"}
{"id": "PERF-SCRIPT-002", "file": "scripts/debt/dedup-multi-pass.js", "lines": "91-161", "issue": "stringSimilarity() calls levenshtein() which runs O(m*n) matrix operations for every pair without memoization", "impact": "slow execution - Levenshtein distance recalculated identically for same title pairs across Pass 2, 3, and 4", "severity": "S2", "recommendation": "Cache similarity results in a Map with key '${a.title}|${b.title}'"}
{"id": "PERF-SCRIPT-003", "file": "scripts/debt/normalize-all.js", "lines": "152-170", "issue": "Synchronous file reads in loop - fs.readFileSync for each file in sequence, not parallelized", "impact": "slow execution - Waits for each file I/O to complete before starting the next, blocking event loop", "severity": "S3", "recommendation": "Use Promise.all() with fs.promises.readFile() for parallel reads"}
{"id": "PERF-SCRIPT-004", "file": "scripts/aggregate-audit-findings.js", "lines": "1489-1512", "issue": "Large array built via repeated .push() then JSON.stringify().join() - creates many intermediate strings", "impact": "memory pressure - Each JSON.stringify creates temp string, then join() creates another. With 5000+ findings doubles memory", "severity": "S3", "recommendation": "Use streaming writes or write in chunks for large datasets"}
{"id": "PERF-SCRIPT-005", "file": "scripts/aggregate-audit-findings.js", "lines": "202", "issue": ".includes() in loop for array search creates O(n²) complexity with roadmapByDescription.get().some()", "impact": "slow execution - For 200+ roadmap items with synonym expansion, each word check scans the full list", "severity": "S3", "recommendation": "Use Set instead of array: const roadmapSet = new Set(roadmapByDescription.get(word).map(i => i.id)) then check with .has()"}
{"id": "PERF-SCRIPT-006", "file": "scripts/check-pattern-compliance.js", "lines": "713-742", "issue": "Global regex with /g flag in findPatternMatches() function - exec() in loop has stateful lastIndex", "impact": "slow execution|potential bugs - /g flag maintains state, fragile when pattern reused across files", "severity": "S2", "recommendation": "Clone regex on each file check or use String.match() instead of pattern.exec()"}
{"id": "PERF-SCRIPT-007", "file": "scripts/debt/generate-views.js", "lines": "214-223", "issue": "Repeated field filtering without caching - builds bySeverity, byCategory, byStatus by iterating items 3 times separately", "impact": "slow execution - O(3n) instead of O(n). With 5000 items, unnecessary 10k additional iterations", "severity": "S3", "recommendation": "Combine into single loop that populates all three maps in one pass"}
{"id": "PERF-SCRIPT-008", "file": "scripts/check-pattern-compliance.js", "lines": "587-639", "issue": "Glob pattern too broad - walk() traverses entire repo including node_modules/.next before hitting ignore checks", "impact": "slow execution - Unnecessary directory traversals before ignore checks at line 615", "severity": "S3", "recommendation": "Check for ignore directories BEFORE recursing: if (ignoreDirs.includes(entry)) continue before lstat.isDirectory()"}
{"id": "PERF-SCRIPT-009", "file": "scripts/aggregate-audit-findings.js", "lines": "1141", "issue": "Array Index Lookup in Loop - .filter((v, i, a) => a.indexOf(v) === i) classic O(n²) dedup pattern", "impact": "slow execution - indexOf() scans from start each time for evidence array deduplication", "severity": "S3", "recommendation": "Use Set: [...new Set(evidence)] instead of .filter((v, i, a) => a.indexOf(v) === i)"}
{"id": "PERF-SCRIPT-010", "file": "scripts/debt/dedup-multi-pass.js", "lines": "219-231", "issue": "Synchronous JSON Parse without streaming - reads entire file then splits by newlines and parses each", "impact": "slow execution|memory pressure - Full file in memory, all lines parsed, invalid ones discarded. No streaming or early exit", "severity": "S2", "recommendation": "For large files (>50MB), use streaming parser or chunk-based processing"}
