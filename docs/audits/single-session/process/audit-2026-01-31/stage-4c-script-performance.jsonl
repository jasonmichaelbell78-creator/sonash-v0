{"id": "PERF-SCRIPT-001", "file": "scripts/debt/dedup-multi-pass.js", "lines": "302-410", "issue": "O(n²) Nested Loops in Deduplication - All four deduplication passes use nested loops comparing every item with every other item", "impact": "slow execution - Each pass runs O(n²) comparisons. With 1000+ items, creates 500k+ comparisons", "severity": "S2", "recommendation": "Pre-bucket items by file/category before comparing to reduce comparison set size"}
{"id": "PERF-SCRIPT-002", "file": "scripts/debt/dedup-multi-pass.js", "lines": "91-161", "issue": "stringSimilarity() calls levenshtein() which runs O(m*n) matrix operations for every pair without memoization", "impact": "slow execution - Levenshtein distance recalculated identically for same title pairs across Pass 2, 3, and 4", "severity": "S2", "recommendation": "Cache similarity results in a Map with key '${a.title}|${b.title}'"}
{"id": "PERF-SCRIPT-003", "file": "scripts/debt/normalize-all.js", "lines": "152-170", "issue": "Synchronous file reads in loop - fs.readFileSync for each file in sequence, not parallelized", "impact": "slow execution - Waits for each file I/O to complete before starting the next, blocking event loop", "severity": "S3", "recommendation": "Use Promise.all() with fs.promises.readFile() for parallel reads"}
{"id": "PERF-SCRIPT-004", "file": "scripts/aggregate-audit-findings.js", "lines": "1489-1512", "issue": "Large array built via repeated .push() then JSON.stringify().join() - creates many intermediate strings", "impact": "memory pressure - Each JSON.stringify creates temp string, then join() creates another. With 5000+ findings doubles memory", "severity": "S3", "recommendation": "Use streaming writes or write in chunks for large datasets"}
{"id": "PERF-SCRIPT-005", "file": "scripts/aggregate-audit-findings.js", "lines": "202", "issue": ".includes() in loop for array search creates O(n²) complexity with roadmapByDescription.get().some()", "impact": "slow execution - For 200+ roadmap items with synonym expansion, each word check scans the full list", "severity": "S3", "recommendation": "Use Set instead of array: const roadmapSet = new Set(roadmapByDescription.get(word).map(i => i.id)) then check with .has()"}
{"id": "PERF-SCRIPT-006", "file": "scripts/check-pattern-compliance.js", "lines": "713-742", "issue": "Global regex with /g flag in findPatternMatches() function - exec() in loop has stateful lastIndex", "impact": "slow execution|potential bugs - /g flag maintains state, fragile when pattern reused across files", "severity": "S2", "recommendation": "Clone regex on each file check or use String.match() instead of pattern.exec()"}
{"id": "PERF-SCRIPT-007", "file": "scripts/debt/generate-views.js", "lines": "214-223", "issue": "Repeated field filtering without caching - builds bySeverity, byCategory, byStatus by iterating items 3 times separately", "impact": "slow execution - O(3n) instead of O(n). With 5000 items, unnecessary 10k additional iterations", "severity": "S3", "recommendation": "Combine into single loop that populates all three maps in one pass"}
{"id": "PERF-SCRIPT-008", "file": "scripts/check-pattern-compliance.js", "lines": "587-639", "issue": "Glob pattern too broad - walk() traverses entire repo including node_modules/.next before hitting ignore checks", "impact": "slow execution - Unnecessary directory traversals before ignore checks at line 615", "severity": "S3", "recommendation": "Check for ignore directories BEFORE recursing: if (ignoreDirs.includes(entry)) continue before lstat.isDirectory()"}
{"id": "PERF-SCRIPT-009", "file": "scripts/aggregate-audit-findings.js", "lines": "1141", "issue": "Array Index Lookup in Loop - .filter((v, i, a) => a.indexOf(v) === i) classic O(n²) dedup pattern", "impact": "slow execution - indexOf() scans from start each time for evidence array deduplication", "severity": "S3", "recommendation": "Use Set: [...new Set(evidence)] instead of .filter((v, i, a) => a.indexOf(v) === i)"}
{"id": "PERF-SCRIPT-010", "file": "scripts/debt/dedup-multi-pass.js", "lines": "219-231", "issue": "Synchronous JSON Parse without streaming - reads entire file then splits by newlines and parses each", "impact": "slow execution|memory pressure - Full file in memory, all lines parsed, invalid ones discarded. No streaming or early exit", "severity": "S2", "recommendation": "For large files (>50MB), use streaming parser or chunk-based processing"}
