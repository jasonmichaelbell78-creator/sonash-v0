{"id": "SCRIPT-001", "category": "Scripts", "severity": "S1", "effort": "E2", "file": "scripts/aggregate-audit-findings.js", "line": 1040, "title": "Unsafe String.prototype.replaceAll() with regex containing regex source", "description": "Lines 1041-1042 use String.prototype.replaceAll() with a regex constructed from a pattern's .source property, but the patterns are not escaped. If a pattern name contains regex metacharacters (e.g., '[test]'), this could create unintended matching behavior. While current patterns in the file are safe, this is a code smell that violates principle of least surprise.", "recommendation": "Either: (1) Use a pre-escaped replacement function instead of regex-based replaceAll, or (2) Escape pattern.source before constructing the regex, or (3) Document why specific pattern names are guaranteed to be regex-safe."}
{"id": "SCRIPT-002", "category": "Scripts", "severity": "S2", "effort": "E1", "file": "scripts/aggregate-audit-findings.js", "line": 1342, "title": "Deduplication fixpoint loop lacks progress documentation", "description": "The deduplication process (lines 1342-1400) uses a multi-pass iteration with MAX_PASSES = 10 safety limit. However, there's no instrumentation or logging to help operators understand: (1) Why a particular input hits MAX_PASSES, (2) How many passes were actually needed, or (3) Whether the algorithm converged to fixpoint. If an operator sees 'hit max passes' warning, they have no way to understand what's happening.", "recommendation": "Add optional logging with --verbose flag that outputs: (1) Number of merges per pass, (2) Bucket sizes that were skipped, (3) Final pass count and whether fixpoint was reached. This helps with debugging and understanding dedup performance."}
{"id": "SCRIPT-003", "category": "Scripts", "severity": "S2", "effort": "E2", "file": "scripts/check-pattern-compliance.js", "line": 712, "title": "Regex pattern object mutation without state reset in loop", "description": "Line 713 creates a new RegExp using pattern.pattern.source and pattern.pattern.flags, but if pattern.pattern contains the 'g' (global) flag, calling exec() in a loop can cause lastIndex pollution if not properly reset. Although line 724 attempts to reset, the pattern is created fresh each time through shouldSkipPattern (line 788), so this is likely safe in practice. However, this is a code smell that makes the code fragile to refactoring.", "recommendation": "Explicitly document why pattern mutation is safe here, or refactor to create regex patterns once at module load time (like SYNONYM_LOOKUP does). Consider: const createPatternRegex = (p) => new RegExp(p.source, p.flags); // Documented: fresh pattern each iteration"}
{"id": "SCRIPT-004", "category": "Scripts", "severity": "S1", "effort": "E1", "file": "scripts/check-pattern-compliance.js", "line": 45, "title": "Global exclusion list is order-dependent and hard to maintain", "description": "Lines 45-80 define GLOBAL_EXCLUDE with 30+ regex patterns. The list is difficult to maintain because: (1) Multiple developers might add overlapping patterns, (2) Order matters if patterns are checked sequentially, (3) No documentation of why each file is excluded. Maintenance burden grows with each exclusion, and it's easy to add dead entries that are never cleaned up.", "recommendation": "Refactor GLOBAL_EXCLUDE into a structured format with comments explaining each exclusion reason. Example: {\"scripts/ai-review.js\": {reason: \"Development utility (Review #136)\", reviewLink: \"https://...\"}. Add a pre-commit check that warns when GLOBAL_EXCLUDE entries haven't been accessed in 30+ days."}
{"id": "SCRIPT-005", "category": "Scripts", "severity": "S2", "effort": "E2", "file": "scripts/security-check.js", "line": 155, "title": "Pattern violation detection is line-number dependent with inconsistent CRLF handling", "description": "Lines 155-195 normalize CRLF to LF (line 160), then calculate line numbers from normalized content (line 175). However, the input `lines` parameter (from line 382) is split from raw content without normalization. This creates a mismatch: if a file has mixed line endings, the line number reported might be off by 1. The code calls this 'Review #190: Use normalizedLines derived from normalizedContent', but normalizedLines is computed but `_lines` parameter is never used, suggesting incomplete refactoring.", "recommendation": "Remove the unused `_lines` parameter or use it consistently. Ensure all line number calculations derive from normalizedContent. Add a test case with mixed CRLF/LF line endings to verify line numbers are always accurate."}
{"id": "SCRIPT-006", "category": "Scripts", "severity": "S2", "effort": "E3", "file": "scripts/debt/consolidate-all.js", "line": 51, "title": "Sequential script execution with weak error recovery", "description": "The consolidation pipeline (lines 18-48, executed at lines 95-101) runs 6 scripts sequentially. If step 3 fails, steps 4-6 are still attempted, which could produce corrupted output. For example, if dedup fails partway through, then 'generate-views' could operate on incomplete data. The script marks steps as 'required: true' but doesn't validate input prerequisites between steps (e.g., 'does step 3 produce output expected by step 4?').", "recommendation": "Add validation between steps: (1) Check that each step produces expected output files before proceeding, (2) Implement rollback logic for required steps (keep checkpoint files), (3) Add --dry-run mode that validates all steps without modifying files. Consider using a database cursor/transaction pattern for audit trail."}
{"id": "SCRIPT-007", "category": "Scripts", "severity": "S1", "effort": "E1", "file": "scripts/lib/security-helpers.js", "line": 334, "description": "Cognitive complexity and circular dependency risk", "title": "safeReadFile helper uses require() inside function body instead of module load", "description": "Line 334 calls require('node:fs').readFileSync inside the function, which is inefficient. This function is imported at module load time but re-requires fs on every call. While this works, it's slower and obscures that fs is a runtime dependency. More critically, if security-helpers.js is ever imported from a test that mocks the fs module, the require() inside the function will get the original (unmocked) fs.", "recommendation": "Move `const { readFileSync } = require('node:fs');` to the top of the file (next to line 12). Change line 334 to just call readFileSync(). This improves performance, fixes potential test mocking issues, and makes dependencies explicit."}
{"id": "SCRIPT-008", "category": "Scripts", "severity": "S2", "effort": "E2", "file": "scripts/run-consolidation.js", "line": 51, "title": "Hard-coded consolidation threshold with no runtime override", "description": "Line 51 sets CONSOLIDATION_THRESHOLD = 10 as a constant. This threshold controls when consolidation is triggered. However, there's no CLI flag to override it (--threshold, --count, etc.). If a team wants to consolidate more frequently (e.g., every 5 reviews) or less frequently (e.g., every 20), they must edit the file. This is poor UX for DevOps teams managing this process.", "recommendation": "Add optional --threshold N flag to override CONSOLIDATION_THRESHOLD. Parse it using the existing parseCliArgs helper (line 257-322 in security-helpers.js). Example: `npm run consolidation:run -- --threshold 5 --apply`. Document the flag in the file header."}
{"id": "SCRIPT-009", "category": "Scripts", "severity": "S1", "effort": "E2", "file": "scripts/run-consolidation.js", "line": 160, "title": "Cross-reference list parsing is fragile to markdown format changes", "description": "Lines 118-145 parse 'Last Consolidation' section using string position indexOf() and split(). If the markdown structure changes (e.g., section is renamed, header level changes from ## to ###), the parsing silently fails and getLastConsolidatedReview returns 0. This causes the consolidation threshold check to reset, potentially re-processing already-consolidated reviews. The fallback chain (lines 118-145) suggests this has been fragile in the past.", "recommendation": "Use a markdown parser library or at minimum, add explicit validation: (1) Check that expected section headers exist before parsing, (2) Throw an error if parser falls through all fallbacks (don't silently return 0), (3) Log which fallback was used. Add a --validate-structure flag that just parses and reports the detected structure without consolidating."}
{"id": "SCRIPT-010", "category": "Scripts", "severity": "S0", "effort": "E1", "file": "scripts/run-consolidation.js", "line": 295, "title": "Pattern keyword list has incomplete regex coverage", "description": "Lines 239-289 define patternKeywords as a hardcoded array of regex patterns. This list is manually curated and will miss emerging patterns that aren't in the list. For example, if a new 'TOCTOU' or 'race condition' pattern emerges in reviews, extractPatterns() won't detect it unless the keyword is manually added. This is a maintenance burden and a source of missing patterns.", "recommendation": "Change extractPatterns to use a hybrid approach: (1) Keep the hardcoded keywords as 'known patterns', (2) Also extract any word that appears 3+ times across review descriptions, (3) Add these discovered patterns to a supplementary list for manual review. This way, new patterns surface automatically and only need manual confirmation to be added to the keyword list."}
